{
  "best_metric": 1.0509576797485352,
  "best_model_checkpoint": "../Modelos/HelBERT-uncased-fs/checkpoint-455958",
  "epoch": 6.0,
  "eval_steps": 500,
  "global_step": 455958,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006579553379916571,
      "grad_norm": 3.139854907989502,
      "learning_rate": 5e-06,
      "loss": 9.084,
      "step": 500
    },
    {
      "epoch": 0.013159106759833143,
      "grad_norm": 2.2373688220977783,
      "learning_rate": 1e-05,
      "loss": 7.1712,
      "step": 1000
    },
    {
      "epoch": 0.019738660139749713,
      "grad_norm": 1.789867877960205,
      "learning_rate": 1.5e-05,
      "loss": 6.3245,
      "step": 1500
    },
    {
      "epoch": 0.026318213519666286,
      "grad_norm": 2.5151095390319824,
      "learning_rate": 2e-05,
      "loss": 6.1247,
      "step": 2000
    },
    {
      "epoch": 0.032897766899582855,
      "grad_norm": 2.3780860900878906,
      "learning_rate": 2.5e-05,
      "loss": 5.9748,
      "step": 2500
    },
    {
      "epoch": 0.03947732027949943,
      "grad_norm": 2.5433366298675537,
      "learning_rate": 3e-05,
      "loss": 5.8166,
      "step": 3000
    },
    {
      "epoch": 0.046056873659416,
      "grad_norm": 3.067661762237549,
      "learning_rate": 3.5e-05,
      "loss": 5.6112,
      "step": 3500
    },
    {
      "epoch": 0.05263642703933257,
      "grad_norm": 3.3288516998291016,
      "learning_rate": 4e-05,
      "loss": 5.3119,
      "step": 4000
    },
    {
      "epoch": 0.059215980419249144,
      "grad_norm": 3.9524641036987305,
      "learning_rate": 4.5e-05,
      "loss": 4.9226,
      "step": 4500
    },
    {
      "epoch": 0.06579553379916571,
      "grad_norm": 3.847578525543213,
      "learning_rate": 5e-05,
      "loss": 4.5269,
      "step": 5000
    },
    {
      "epoch": 0.07237508717908228,
      "grad_norm": 4.049356460571289,
      "learning_rate": 5.500000000000001e-05,
      "loss": 4.1464,
      "step": 5500
    },
    {
      "epoch": 0.07895464055899885,
      "grad_norm": 3.4120535850524902,
      "learning_rate": 6e-05,
      "loss": 3.832,
      "step": 6000
    },
    {
      "epoch": 0.08553419393891543,
      "grad_norm": 3.045287609100342,
      "learning_rate": 6.500000000000001e-05,
      "loss": 3.5526,
      "step": 6500
    },
    {
      "epoch": 0.092113747318832,
      "grad_norm": 2.9033312797546387,
      "learning_rate": 7e-05,
      "loss": 3.3681,
      "step": 7000
    },
    {
      "epoch": 0.09869330069874857,
      "grad_norm": 3.1457715034484863,
      "learning_rate": 7.500000000000001e-05,
      "loss": 3.202,
      "step": 7500
    },
    {
      "epoch": 0.10527285407866514,
      "grad_norm": 3.3716514110565186,
      "learning_rate": 8e-05,
      "loss": 3.0826,
      "step": 8000
    },
    {
      "epoch": 0.11185240745858172,
      "grad_norm": 3.0209219455718994,
      "learning_rate": 8.499e-05,
      "loss": 2.9587,
      "step": 8500
    },
    {
      "epoch": 0.11843196083849829,
      "grad_norm": 3.3128323554992676,
      "learning_rate": 8.999000000000001e-05,
      "loss": 2.8665,
      "step": 9000
    },
    {
      "epoch": 0.12501151421841486,
      "grad_norm": 2.910661458969116,
      "learning_rate": 9.499e-05,
      "loss": 2.7729,
      "step": 9500
    },
    {
      "epoch": 0.13159106759833142,
      "grad_norm": 2.710343360900879,
      "learning_rate": 9.999000000000001e-05,
      "loss": 2.7214,
      "step": 10000
    },
    {
      "epoch": 0.138170620978248,
      "grad_norm": 2.936994791030884,
      "learning_rate": 9.996701680950552e-05,
      "loss": 2.6441,
      "step": 10500
    },
    {
      "epoch": 0.14475017435816456,
      "grad_norm": 2.7759897708892822,
      "learning_rate": 9.993390115639861e-05,
      "loss": 2.5765,
      "step": 11000
    },
    {
      "epoch": 0.15132972773808115,
      "grad_norm": 2.923658847808838,
      "learning_rate": 9.99007855032917e-05,
      "loss": 2.5162,
      "step": 11500
    },
    {
      "epoch": 0.1579092811179977,
      "grad_norm": 2.9793031215667725,
      "learning_rate": 9.986766985018479e-05,
      "loss": 2.463,
      "step": 12000
    },
    {
      "epoch": 0.1644888344979143,
      "grad_norm": 2.766552686691284,
      "learning_rate": 9.983462042838409e-05,
      "loss": 2.4181,
      "step": 12500
    },
    {
      "epoch": 0.17106838787783085,
      "grad_norm": 2.8277833461761475,
      "learning_rate": 9.980150477527719e-05,
      "loss": 2.3695,
      "step": 13000
    },
    {
      "epoch": 0.1776479412577474,
      "grad_norm": 3.004333019256592,
      "learning_rate": 9.976838912217027e-05,
      "loss": 2.3361,
      "step": 13500
    },
    {
      "epoch": 0.184227494637664,
      "grad_norm": 2.8478944301605225,
      "learning_rate": 9.973527346906336e-05,
      "loss": 2.302,
      "step": 14000
    },
    {
      "epoch": 0.19080704801758055,
      "grad_norm": 2.788205146789551,
      "learning_rate": 9.970215781595646e-05,
      "loss": 2.2567,
      "step": 14500
    },
    {
      "epoch": 0.19738660139749714,
      "grad_norm": 2.9470596313476562,
      "learning_rate": 9.966904216284953e-05,
      "loss": 2.234,
      "step": 15000
    },
    {
      "epoch": 0.2039661547774137,
      "grad_norm": 2.5772016048431396,
      "learning_rate": 9.963599274104885e-05,
      "loss": 2.1989,
      "step": 15500
    },
    {
      "epoch": 0.21054570815733029,
      "grad_norm": 2.542638063430786,
      "learning_rate": 9.960287708794193e-05,
      "loss": 2.1804,
      "step": 16000
    },
    {
      "epoch": 0.21712526153724684,
      "grad_norm": 2.7925097942352295,
      "learning_rate": 9.956976143483502e-05,
      "loss": 2.163,
      "step": 16500
    },
    {
      "epoch": 0.22370481491716343,
      "grad_norm": 2.9346425533294678,
      "learning_rate": 9.95366457817281e-05,
      "loss": 2.132,
      "step": 17000
    },
    {
      "epoch": 0.23028436829708,
      "grad_norm": 2.594303607940674,
      "learning_rate": 9.95035301286212e-05,
      "loss": 2.1195,
      "step": 17500
    },
    {
      "epoch": 0.23686392167699657,
      "grad_norm": 2.908281087875366,
      "learning_rate": 9.94704807068205e-05,
      "loss": 2.102,
      "step": 18000
    },
    {
      "epoch": 0.24344347505691313,
      "grad_norm": 2.6272025108337402,
      "learning_rate": 9.94373650537136e-05,
      "loss": 2.0817,
      "step": 18500
    },
    {
      "epoch": 0.2500230284368297,
      "grad_norm": 2.709420919418335,
      "learning_rate": 9.940424940060669e-05,
      "loss": 2.0516,
      "step": 19000
    },
    {
      "epoch": 0.2566025818167463,
      "grad_norm": 2.9833767414093018,
      "learning_rate": 9.937113374749978e-05,
      "loss": 2.0376,
      "step": 19500
    },
    {
      "epoch": 0.26318213519666284,
      "grad_norm": 2.6850810050964355,
      "learning_rate": 9.933808432569908e-05,
      "loss": 2.012,
      "step": 20000
    },
    {
      "epoch": 0.2697616885765794,
      "grad_norm": 2.8599801063537598,
      "learning_rate": 9.930496867259217e-05,
      "loss": 2.005,
      "step": 20500
    },
    {
      "epoch": 0.276341241956496,
      "grad_norm": 2.734727621078491,
      "learning_rate": 9.927185301948525e-05,
      "loss": 1.9816,
      "step": 21000
    },
    {
      "epoch": 0.28292079533641257,
      "grad_norm": 2.8560431003570557,
      "learning_rate": 9.923873736637834e-05,
      "loss": 1.9766,
      "step": 21500
    },
    {
      "epoch": 0.2895003487163291,
      "grad_norm": 2.7166988849639893,
      "learning_rate": 9.920562171327144e-05,
      "loss": 1.9639,
      "step": 22000
    },
    {
      "epoch": 0.2960799020962457,
      "grad_norm": 2.569251775741577,
      "learning_rate": 9.917250606016452e-05,
      "loss": 1.9496,
      "step": 22500
    },
    {
      "epoch": 0.3026594554761623,
      "grad_norm": 2.682354211807251,
      "learning_rate": 9.913945663836383e-05,
      "loss": 1.9314,
      "step": 23000
    },
    {
      "epoch": 0.30923900885607886,
      "grad_norm": 2.4927756786346436,
      "learning_rate": 9.910634098525691e-05,
      "loss": 1.9201,
      "step": 23500
    },
    {
      "epoch": 0.3158185622359954,
      "grad_norm": 2.51750111579895,
      "learning_rate": 9.907322533215001e-05,
      "loss": 1.9136,
      "step": 24000
    },
    {
      "epoch": 0.322398115615912,
      "grad_norm": 2.5202322006225586,
      "learning_rate": 9.90401096790431e-05,
      "loss": 1.8938,
      "step": 24500
    },
    {
      "epoch": 0.3289776689958286,
      "grad_norm": 2.753662347793579,
      "learning_rate": 9.900699402593618e-05,
      "loss": 1.8911,
      "step": 25000
    },
    {
      "epoch": 0.33555722237574515,
      "grad_norm": 2.8912603855133057,
      "learning_rate": 9.897394460413549e-05,
      "loss": 1.8904,
      "step": 25500
    },
    {
      "epoch": 0.3421367757556617,
      "grad_norm": 2.6139957904815674,
      "learning_rate": 9.894082895102857e-05,
      "loss": 1.8718,
      "step": 26000
    },
    {
      "epoch": 0.34871632913557826,
      "grad_norm": 2.565263032913208,
      "learning_rate": 9.890771329792167e-05,
      "loss": 1.8551,
      "step": 26500
    },
    {
      "epoch": 0.3552958825154948,
      "grad_norm": 2.3705434799194336,
      "learning_rate": 9.887459764481476e-05,
      "loss": 1.8422,
      "step": 27000
    },
    {
      "epoch": 0.36187543589541143,
      "grad_norm": 2.4775021076202393,
      "learning_rate": 9.884148199170784e-05,
      "loss": 1.84,
      "step": 27500
    },
    {
      "epoch": 0.368454989275328,
      "grad_norm": 2.8447675704956055,
      "learning_rate": 9.880843256990715e-05,
      "loss": 1.8369,
      "step": 28000
    },
    {
      "epoch": 0.37503454265524455,
      "grad_norm": 2.6742067337036133,
      "learning_rate": 9.877531691680025e-05,
      "loss": 1.835,
      "step": 28500
    },
    {
      "epoch": 0.3816140960351611,
      "grad_norm": 2.7340962886810303,
      "learning_rate": 9.874220126369332e-05,
      "loss": 1.8231,
      "step": 29000
    },
    {
      "epoch": 0.3881936494150777,
      "grad_norm": 2.4240806102752686,
      "learning_rate": 9.870908561058642e-05,
      "loss": 1.8178,
      "step": 29500
    },
    {
      "epoch": 0.3947732027949943,
      "grad_norm": 2.677680015563965,
      "learning_rate": 9.867603618878572e-05,
      "loss": 1.8057,
      "step": 30000
    },
    {
      "epoch": 0.40135275617491084,
      "grad_norm": 2.5074996948242188,
      "learning_rate": 9.86429205356788e-05,
      "loss": 1.804,
      "step": 30500
    },
    {
      "epoch": 0.4079323095548274,
      "grad_norm": 2.8356595039367676,
      "learning_rate": 9.86098048825719e-05,
      "loss": 1.7902,
      "step": 31000
    },
    {
      "epoch": 0.414511862934744,
      "grad_norm": 2.581878662109375,
      "learning_rate": 9.857668922946499e-05,
      "loss": 1.7897,
      "step": 31500
    },
    {
      "epoch": 0.42109141631466057,
      "grad_norm": 2.630223274230957,
      "learning_rate": 9.85436398076643e-05,
      "loss": 1.7788,
      "step": 32000
    },
    {
      "epoch": 0.42767096969457713,
      "grad_norm": 2.614901542663574,
      "learning_rate": 9.851052415455738e-05,
      "loss": 1.7886,
      "step": 32500
    },
    {
      "epoch": 0.4342505230744937,
      "grad_norm": 2.818523406982422,
      "learning_rate": 9.847740850145048e-05,
      "loss": 1.7667,
      "step": 33000
    },
    {
      "epoch": 0.4408300764544103,
      "grad_norm": 2.6949453353881836,
      "learning_rate": 9.844429284834355e-05,
      "loss": 1.7614,
      "step": 33500
    },
    {
      "epoch": 0.44740962983432686,
      "grad_norm": 2.7326786518096924,
      "learning_rate": 9.841124342654287e-05,
      "loss": 1.7541,
      "step": 34000
    },
    {
      "epoch": 0.4539891832142434,
      "grad_norm": 2.591294288635254,
      "learning_rate": 9.837812777343596e-05,
      "loss": 1.7591,
      "step": 34500
    },
    {
      "epoch": 0.46056873659416,
      "grad_norm": 2.5655293464660645,
      "learning_rate": 9.834501212032904e-05,
      "loss": 1.7418,
      "step": 35000
    },
    {
      "epoch": 0.46714828997407654,
      "grad_norm": 2.489513635635376,
      "learning_rate": 9.831189646722213e-05,
      "loss": 1.7324,
      "step": 35500
    },
    {
      "epoch": 0.47372784335399315,
      "grad_norm": 2.758307933807373,
      "learning_rate": 9.827884704542143e-05,
      "loss": 1.7222,
      "step": 36000
    },
    {
      "epoch": 0.4803073967339097,
      "grad_norm": 2.526388645172119,
      "learning_rate": 9.824573139231452e-05,
      "loss": 1.7251,
      "step": 36500
    },
    {
      "epoch": 0.48688695011382627,
      "grad_norm": 2.575160503387451,
      "learning_rate": 9.821261573920761e-05,
      "loss": 1.7306,
      "step": 37000
    },
    {
      "epoch": 0.4934665034937428,
      "grad_norm": 2.6585066318511963,
      "learning_rate": 9.81795000861007e-05,
      "loss": 1.7211,
      "step": 37500
    },
    {
      "epoch": 0.5000460568736594,
      "grad_norm": 2.514080047607422,
      "learning_rate": 9.81464506643e-05,
      "loss": 1.7054,
      "step": 38000
    },
    {
      "epoch": 0.506625610253576,
      "grad_norm": 2.4176418781280518,
      "learning_rate": 9.81133350111931e-05,
      "loss": 1.7052,
      "step": 38500
    },
    {
      "epoch": 0.5132051636334926,
      "grad_norm": 2.7782368659973145,
      "learning_rate": 9.808021935808619e-05,
      "loss": 1.6999,
      "step": 39000
    },
    {
      "epoch": 0.5197847170134091,
      "grad_norm": 2.5960333347320557,
      "learning_rate": 9.804710370497927e-05,
      "loss": 1.6846,
      "step": 39500
    },
    {
      "epoch": 0.5263642703933257,
      "grad_norm": 2.558537721633911,
      "learning_rate": 9.801405428317858e-05,
      "loss": 1.6846,
      "step": 40000
    },
    {
      "epoch": 0.5329438237732422,
      "grad_norm": 2.9697511196136475,
      "learning_rate": 9.798093863007168e-05,
      "loss": 1.6739,
      "step": 40500
    },
    {
      "epoch": 0.5395233771531588,
      "grad_norm": 2.5659332275390625,
      "learning_rate": 9.794782297696475e-05,
      "loss": 1.6779,
      "step": 41000
    },
    {
      "epoch": 0.5461029305330755,
      "grad_norm": 2.670562982559204,
      "learning_rate": 9.791470732385785e-05,
      "loss": 1.669,
      "step": 41500
    },
    {
      "epoch": 0.552682483912992,
      "grad_norm": 2.676208972930908,
      "learning_rate": 9.788165790205714e-05,
      "loss": 1.6672,
      "step": 42000
    },
    {
      "epoch": 0.5592620372929086,
      "grad_norm": 2.7387876510620117,
      "learning_rate": 9.784854224895024e-05,
      "loss": 1.6684,
      "step": 42500
    },
    {
      "epoch": 0.5658415906728251,
      "grad_norm": 2.6045238971710205,
      "learning_rate": 9.781542659584332e-05,
      "loss": 1.6672,
      "step": 43000
    },
    {
      "epoch": 0.5724211440527417,
      "grad_norm": 2.3740663528442383,
      "learning_rate": 9.778231094273642e-05,
      "loss": 1.6556,
      "step": 43500
    },
    {
      "epoch": 0.5790006974326583,
      "grad_norm": 2.443227529525757,
      "learning_rate": 9.774926152093571e-05,
      "loss": 1.6512,
      "step": 44000
    },
    {
      "epoch": 0.5855802508125748,
      "grad_norm": 2.7779996395111084,
      "learning_rate": 9.771614586782881e-05,
      "loss": 1.6438,
      "step": 44500
    },
    {
      "epoch": 0.5921598041924914,
      "grad_norm": 2.660731315612793,
      "learning_rate": 9.76830302147219e-05,
      "loss": 1.6457,
      "step": 45000
    },
    {
      "epoch": 0.598739357572408,
      "grad_norm": 2.543558359146118,
      "learning_rate": 9.764991456161498e-05,
      "loss": 1.6449,
      "step": 45500
    },
    {
      "epoch": 0.6053189109523246,
      "grad_norm": 3.085667848587036,
      "learning_rate": 9.761686513981429e-05,
      "loss": 1.6394,
      "step": 46000
    },
    {
      "epoch": 0.6118984643322412,
      "grad_norm": 2.5786473751068115,
      "learning_rate": 9.758374948670737e-05,
      "loss": 1.6342,
      "step": 46500
    },
    {
      "epoch": 0.6184780177121577,
      "grad_norm": 2.6644365787506104,
      "learning_rate": 9.755063383360047e-05,
      "loss": 1.643,
      "step": 47000
    },
    {
      "epoch": 0.6250575710920743,
      "grad_norm": 2.7152504920959473,
      "learning_rate": 9.751751818049356e-05,
      "loss": 1.622,
      "step": 47500
    },
    {
      "epoch": 0.6316371244719908,
      "grad_norm": 2.3618733882904053,
      "learning_rate": 9.748440252738666e-05,
      "loss": 1.6172,
      "step": 48000
    },
    {
      "epoch": 0.6382166778519074,
      "grad_norm": 2.5592072010040283,
      "learning_rate": 9.745128687427973e-05,
      "loss": 1.6303,
      "step": 48500
    },
    {
      "epoch": 0.644796231231824,
      "grad_norm": 2.577571153640747,
      "learning_rate": 9.741823745247905e-05,
      "loss": 1.6111,
      "step": 49000
    },
    {
      "epoch": 0.6513757846117405,
      "grad_norm": 2.6548919677734375,
      "learning_rate": 9.738512179937213e-05,
      "loss": 1.6163,
      "step": 49500
    },
    {
      "epoch": 0.6579553379916572,
      "grad_norm": 2.448660135269165,
      "learning_rate": 9.735200614626522e-05,
      "loss": 1.6023,
      "step": 50000
    },
    {
      "epoch": 0.6645348913715737,
      "grad_norm": 2.6322059631347656,
      "learning_rate": 9.731889049315832e-05,
      "loss": 1.6112,
      "step": 50500
    },
    {
      "epoch": 0.6711144447514903,
      "grad_norm": 2.568981885910034,
      "learning_rate": 9.728584107135761e-05,
      "loss": 1.6083,
      "step": 51000
    },
    {
      "epoch": 0.6776939981314068,
      "grad_norm": 2.624885082244873,
      "learning_rate": 9.725272541825071e-05,
      "loss": 1.6092,
      "step": 51500
    },
    {
      "epoch": 0.6842735515113234,
      "grad_norm": 2.4277708530426025,
      "learning_rate": 9.721960976514379e-05,
      "loss": 1.5993,
      "step": 52000
    },
    {
      "epoch": 0.69085310489124,
      "grad_norm": 2.6746268272399902,
      "learning_rate": 9.718649411203689e-05,
      "loss": 1.5834,
      "step": 52500
    },
    {
      "epoch": 0.6974326582711565,
      "grad_norm": 2.537621259689331,
      "learning_rate": 9.715344469023618e-05,
      "loss": 1.5853,
      "step": 53000
    },
    {
      "epoch": 0.7040122116510731,
      "grad_norm": 2.833770275115967,
      "learning_rate": 9.712032903712928e-05,
      "loss": 1.5861,
      "step": 53500
    },
    {
      "epoch": 0.7105917650309896,
      "grad_norm": 2.5488831996917725,
      "learning_rate": 9.708721338402237e-05,
      "loss": 1.5782,
      "step": 54000
    },
    {
      "epoch": 0.7171713184109063,
      "grad_norm": 2.37758207321167,
      "learning_rate": 9.705409773091545e-05,
      "loss": 1.5952,
      "step": 54500
    },
    {
      "epoch": 0.7237508717908229,
      "grad_norm": 2.6457512378692627,
      "learning_rate": 9.702104830911476e-05,
      "loss": 1.5899,
      "step": 55000
    },
    {
      "epoch": 0.7303304251707394,
      "grad_norm": 2.543125867843628,
      "learning_rate": 9.698793265600786e-05,
      "loss": 1.5731,
      "step": 55500
    },
    {
      "epoch": 0.736909978550656,
      "grad_norm": 2.7919082641601562,
      "learning_rate": 9.695481700290093e-05,
      "loss": 1.569,
      "step": 56000
    },
    {
      "epoch": 0.7434895319305725,
      "grad_norm": 2.2876229286193848,
      "learning_rate": 9.692170134979403e-05,
      "loss": 1.5806,
      "step": 56500
    },
    {
      "epoch": 0.7500690853104891,
      "grad_norm": 2.528977632522583,
      "learning_rate": 9.688858569668711e-05,
      "loss": 1.5768,
      "step": 57000
    },
    {
      "epoch": 0.7566486386904057,
      "grad_norm": 2.6582412719726562,
      "learning_rate": 9.685553627488642e-05,
      "loss": 1.5711,
      "step": 57500
    },
    {
      "epoch": 0.7632281920703222,
      "grad_norm": 2.4359960556030273,
      "learning_rate": 9.682248685308572e-05,
      "loss": 1.5612,
      "step": 58000
    },
    {
      "epoch": 0.7698077454502389,
      "grad_norm": 2.546227216720581,
      "learning_rate": 9.67893711999788e-05,
      "loss": 1.5625,
      "step": 58500
    },
    {
      "epoch": 0.7763872988301554,
      "grad_norm": 2.848630428314209,
      "learning_rate": 9.67562555468719e-05,
      "loss": 1.5617,
      "step": 59000
    },
    {
      "epoch": 0.782966852210072,
      "grad_norm": 2.527367353439331,
      "learning_rate": 9.672313989376499e-05,
      "loss": 1.5537,
      "step": 59500
    },
    {
      "epoch": 0.7895464055899886,
      "grad_norm": 2.4462690353393555,
      "learning_rate": 9.669002424065809e-05,
      "loss": 1.5477,
      "step": 60000
    },
    {
      "epoch": 0.7961259589699051,
      "grad_norm": 2.6439120769500732,
      "learning_rate": 9.665690858755116e-05,
      "loss": 1.5507,
      "step": 60500
    },
    {
      "epoch": 0.8027055123498217,
      "grad_norm": 2.599815845489502,
      "learning_rate": 9.662379293444426e-05,
      "loss": 1.5577,
      "step": 61000
    },
    {
      "epoch": 0.8092850657297382,
      "grad_norm": 4.1163506507873535,
      "learning_rate": 9.659067728133735e-05,
      "loss": 1.5402,
      "step": 61500
    },
    {
      "epoch": 0.8158646191096548,
      "grad_norm": 2.3943114280700684,
      "learning_rate": 9.655762785953665e-05,
      "loss": 1.5343,
      "step": 62000
    },
    {
      "epoch": 0.8224441724895714,
      "grad_norm": 2.731081247329712,
      "learning_rate": 9.652451220642974e-05,
      "loss": 1.543,
      "step": 62500
    },
    {
      "epoch": 0.829023725869488,
      "grad_norm": 2.45835280418396,
      "learning_rate": 9.649139655332283e-05,
      "loss": 1.5422,
      "step": 63000
    },
    {
      "epoch": 0.8356032792494046,
      "grad_norm": 2.6275525093078613,
      "learning_rate": 9.645828090021592e-05,
      "loss": 1.5392,
      "step": 63500
    },
    {
      "epoch": 0.8421828326293211,
      "grad_norm": 2.5304453372955322,
      "learning_rate": 9.642523147841522e-05,
      "loss": 1.5361,
      "step": 64000
    },
    {
      "epoch": 0.8487623860092377,
      "grad_norm": 2.66290020942688,
      "learning_rate": 9.639211582530831e-05,
      "loss": 1.5291,
      "step": 64500
    },
    {
      "epoch": 0.8553419393891543,
      "grad_norm": 2.6809182167053223,
      "learning_rate": 9.63590001722014e-05,
      "loss": 1.5327,
      "step": 65000
    },
    {
      "epoch": 0.8619214927690708,
      "grad_norm": 2.6487650871276855,
      "learning_rate": 9.63258845190945e-05,
      "loss": 1.5336,
      "step": 65500
    },
    {
      "epoch": 0.8685010461489874,
      "grad_norm": 2.648463487625122,
      "learning_rate": 9.62928350972938e-05,
      "loss": 1.5269,
      "step": 66000
    },
    {
      "epoch": 0.8750805995289039,
      "grad_norm": 2.3581769466400146,
      "learning_rate": 9.625971944418688e-05,
      "loss": 1.5167,
      "step": 66500
    },
    {
      "epoch": 0.8816601529088206,
      "grad_norm": 2.7865593433380127,
      "learning_rate": 9.622660379107997e-05,
      "loss": 1.5239,
      "step": 67000
    },
    {
      "epoch": 0.8882397062887372,
      "grad_norm": 2.4453885555267334,
      "learning_rate": 9.619348813797307e-05,
      "loss": 1.524,
      "step": 67500
    },
    {
      "epoch": 0.8948192596686537,
      "grad_norm": 2.416538953781128,
      "learning_rate": 9.616043871617236e-05,
      "loss": 1.5155,
      "step": 68000
    },
    {
      "epoch": 0.9013988130485703,
      "grad_norm": 2.344197988510132,
      "learning_rate": 9.612732306306546e-05,
      "loss": 1.5205,
      "step": 68500
    },
    {
      "epoch": 0.9079783664284868,
      "grad_norm": 2.5111892223358154,
      "learning_rate": 9.609420740995854e-05,
      "loss": 1.5039,
      "step": 69000
    },
    {
      "epoch": 0.9145579198084034,
      "grad_norm": 2.490450143814087,
      "learning_rate": 9.606109175685163e-05,
      "loss": 1.505,
      "step": 69500
    },
    {
      "epoch": 0.92113747318832,
      "grad_norm": 2.741502046585083,
      "learning_rate": 9.602804233505093e-05,
      "loss": 1.5032,
      "step": 70000
    },
    {
      "epoch": 0.9277170265682365,
      "grad_norm": 2.4418699741363525,
      "learning_rate": 9.599492668194403e-05,
      "loss": 1.4999,
      "step": 70500
    },
    {
      "epoch": 0.9342965799481531,
      "grad_norm": 2.313133716583252,
      "learning_rate": 9.596181102883712e-05,
      "loss": 1.5031,
      "step": 71000
    },
    {
      "epoch": 0.9408761333280697,
      "grad_norm": 2.509791851043701,
      "learning_rate": 9.59286953757302e-05,
      "loss": 1.5154,
      "step": 71500
    },
    {
      "epoch": 0.9474556867079863,
      "grad_norm": 2.6315622329711914,
      "learning_rate": 9.589564595392951e-05,
      "loss": 1.5047,
      "step": 72000
    },
    {
      "epoch": 0.9540352400879029,
      "grad_norm": 2.643143653869629,
      "learning_rate": 9.58625303008226e-05,
      "loss": 1.502,
      "step": 72500
    },
    {
      "epoch": 0.9606147934678194,
      "grad_norm": 2.3634567260742188,
      "learning_rate": 9.582941464771569e-05,
      "loss": 1.4961,
      "step": 73000
    },
    {
      "epoch": 0.967194346847736,
      "grad_norm": 2.4076874256134033,
      "learning_rate": 9.579629899460878e-05,
      "loss": 1.4888,
      "step": 73500
    },
    {
      "epoch": 0.9737739002276525,
      "grad_norm": 2.5316977500915527,
      "learning_rate": 9.576318334150186e-05,
      "loss": 1.5042,
      "step": 74000
    },
    {
      "epoch": 0.9803534536075691,
      "grad_norm": 2.587129831314087,
      "learning_rate": 9.573006768839495e-05,
      "loss": 1.4837,
      "step": 74500
    },
    {
      "epoch": 0.9869330069874857,
      "grad_norm": 2.542154550552368,
      "learning_rate": 9.569701826659427e-05,
      "loss": 1.4896,
      "step": 75000
    },
    {
      "epoch": 0.9935125603674022,
      "grad_norm": 2.4580047130584717,
      "learning_rate": 9.566390261348734e-05,
      "loss": 1.4869,
      "step": 75500
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.414989709854126,
      "eval_runtime": 49.5503,
      "eval_samples_per_second": 2018.152,
      "eval_steps_per_second": 15.782,
      "step": 75993
    },
    {
      "epoch": 1.0000921137473189,
      "grad_norm": 2.6691248416900635,
      "learning_rate": 9.563078696038044e-05,
      "loss": 1.4799,
      "step": 76000
    },
    {
      "epoch": 1.0066716671272353,
      "grad_norm": 2.6922378540039062,
      "learning_rate": 9.559767130727352e-05,
      "loss": 1.4742,
      "step": 76500
    },
    {
      "epoch": 1.013251220507152,
      "grad_norm": 3.2475178241729736,
      "learning_rate": 9.556462188547283e-05,
      "loss": 1.4883,
      "step": 77000
    },
    {
      "epoch": 1.0198307738870684,
      "grad_norm": 2.691763162612915,
      "learning_rate": 9.553150623236591e-05,
      "loss": 1.4806,
      "step": 77500
    },
    {
      "epoch": 1.0264103272669851,
      "grad_norm": 2.492419958114624,
      "learning_rate": 9.549839057925901e-05,
      "loss": 1.4778,
      "step": 78000
    },
    {
      "epoch": 1.0329898806469018,
      "grad_norm": 2.347482442855835,
      "learning_rate": 9.54652749261521e-05,
      "loss": 1.4696,
      "step": 78500
    },
    {
      "epoch": 1.0395694340268182,
      "grad_norm": 2.419389486312866,
      "learning_rate": 9.54322255043514e-05,
      "loss": 1.4754,
      "step": 79000
    },
    {
      "epoch": 1.046148987406735,
      "grad_norm": 2.3591506481170654,
      "learning_rate": 9.53991098512445e-05,
      "loss": 1.479,
      "step": 79500
    },
    {
      "epoch": 1.0527285407866513,
      "grad_norm": 2.399709463119507,
      "learning_rate": 9.536599419813757e-05,
      "loss": 1.4686,
      "step": 80000
    },
    {
      "epoch": 1.059308094166568,
      "grad_norm": 2.5940134525299072,
      "learning_rate": 9.533287854503067e-05,
      "loss": 1.4637,
      "step": 80500
    },
    {
      "epoch": 1.0658876475464845,
      "grad_norm": 2.4470818042755127,
      "learning_rate": 9.529982912322998e-05,
      "loss": 1.4619,
      "step": 81000
    },
    {
      "epoch": 1.0724672009264011,
      "grad_norm": 2.541896343231201,
      "learning_rate": 9.526671347012306e-05,
      "loss": 1.4597,
      "step": 81500
    },
    {
      "epoch": 1.0790467543063178,
      "grad_norm": 2.530148983001709,
      "learning_rate": 9.523359781701615e-05,
      "loss": 1.461,
      "step": 82000
    },
    {
      "epoch": 1.0856263076862342,
      "grad_norm": 2.2642643451690674,
      "learning_rate": 9.520048216390925e-05,
      "loss": 1.4595,
      "step": 82500
    },
    {
      "epoch": 1.092205861066151,
      "grad_norm": 2.565683603286743,
      "learning_rate": 9.516743274210854e-05,
      "loss": 1.4696,
      "step": 83000
    },
    {
      "epoch": 1.0987854144460674,
      "grad_norm": 2.6461660861968994,
      "learning_rate": 9.513431708900164e-05,
      "loss": 1.453,
      "step": 83500
    },
    {
      "epoch": 1.105364967825984,
      "grad_norm": 2.382054567337036,
      "learning_rate": 9.510120143589472e-05,
      "loss": 1.4561,
      "step": 84000
    },
    {
      "epoch": 1.1119445212059005,
      "grad_norm": 2.384202003479004,
      "learning_rate": 9.50680857827878e-05,
      "loss": 1.4631,
      "step": 84500
    },
    {
      "epoch": 1.1185240745858172,
      "grad_norm": 2.4374611377716064,
      "learning_rate": 9.50349701296809e-05,
      "loss": 1.4521,
      "step": 85000
    },
    {
      "epoch": 1.1251036279657336,
      "grad_norm": 2.6281235218048096,
      "learning_rate": 9.500192070788021e-05,
      "loss": 1.4396,
      "step": 85500
    },
    {
      "epoch": 1.1316831813456503,
      "grad_norm": 2.457064151763916,
      "learning_rate": 9.49688050547733e-05,
      "loss": 1.4548,
      "step": 86000
    },
    {
      "epoch": 1.1382627347255667,
      "grad_norm": 2.361156463623047,
      "learning_rate": 9.493568940166638e-05,
      "loss": 1.4533,
      "step": 86500
    },
    {
      "epoch": 1.1448422881054834,
      "grad_norm": 2.2817726135253906,
      "learning_rate": 9.490257374855948e-05,
      "loss": 1.4446,
      "step": 87000
    },
    {
      "epoch": 1.1514218414854,
      "grad_norm": 2.3340413570404053,
      "learning_rate": 9.486952432675877e-05,
      "loss": 1.4459,
      "step": 87500
    },
    {
      "epoch": 1.1580013948653165,
      "grad_norm": 2.4980716705322266,
      "learning_rate": 9.483640867365187e-05,
      "loss": 1.4439,
      "step": 88000
    },
    {
      "epoch": 1.1645809482452332,
      "grad_norm": 2.4675135612487793,
      "learning_rate": 9.480329302054496e-05,
      "loss": 1.4497,
      "step": 88500
    },
    {
      "epoch": 1.1711605016251496,
      "grad_norm": 2.5558524131774902,
      "learning_rate": 9.477017736743804e-05,
      "loss": 1.4461,
      "step": 89000
    },
    {
      "epoch": 1.1777400550050663,
      "grad_norm": 2.472127914428711,
      "learning_rate": 9.473706171433113e-05,
      "loss": 1.4448,
      "step": 89500
    },
    {
      "epoch": 1.1843196083849827,
      "grad_norm": 2.3717501163482666,
      "learning_rate": 9.470401229253044e-05,
      "loss": 1.4324,
      "step": 90000
    },
    {
      "epoch": 1.1908991617648994,
      "grad_norm": 2.469369888305664,
      "learning_rate": 9.467089663942353e-05,
      "loss": 1.4448,
      "step": 90500
    },
    {
      "epoch": 1.197478715144816,
      "grad_norm": 2.335817813873291,
      "learning_rate": 9.463778098631661e-05,
      "loss": 1.4411,
      "step": 91000
    },
    {
      "epoch": 1.2040582685247325,
      "grad_norm": 2.6975557804107666,
      "learning_rate": 9.460466533320971e-05,
      "loss": 1.436,
      "step": 91500
    },
    {
      "epoch": 1.2106378219046492,
      "grad_norm": 2.4308981895446777,
      "learning_rate": 9.45715496801028e-05,
      "loss": 1.4245,
      "step": 92000
    },
    {
      "epoch": 1.2172173752845656,
      "grad_norm": 2.6690714359283447,
      "learning_rate": 9.45385002583021e-05,
      "loss": 1.4353,
      "step": 92500
    },
    {
      "epoch": 1.2237969286644823,
      "grad_norm": 2.493798017501831,
      "learning_rate": 9.450538460519519e-05,
      "loss": 1.4292,
      "step": 93000
    },
    {
      "epoch": 1.2303764820443988,
      "grad_norm": 2.5376861095428467,
      "learning_rate": 9.447226895208829e-05,
      "loss": 1.4307,
      "step": 93500
    },
    {
      "epoch": 1.2369560354243154,
      "grad_norm": 2.642374277114868,
      "learning_rate": 9.443915329898136e-05,
      "loss": 1.4255,
      "step": 94000
    },
    {
      "epoch": 1.2435355888042319,
      "grad_norm": 2.7117414474487305,
      "learning_rate": 9.440603764587446e-05,
      "loss": 1.4233,
      "step": 94500
    },
    {
      "epoch": 1.2501151421841485,
      "grad_norm": 2.6943211555480957,
      "learning_rate": 9.437298822407375e-05,
      "loss": 1.4314,
      "step": 95000
    },
    {
      "epoch": 1.256694695564065,
      "grad_norm": 2.3873891830444336,
      "learning_rate": 9.433987257096685e-05,
      "loss": 1.4344,
      "step": 95500
    },
    {
      "epoch": 1.2632742489439817,
      "grad_norm": 2.4404735565185547,
      "learning_rate": 9.430675691785993e-05,
      "loss": 1.4246,
      "step": 96000
    },
    {
      "epoch": 1.2698538023238983,
      "grad_norm": 2.4862170219421387,
      "learning_rate": 9.427364126475303e-05,
      "loss": 1.4229,
      "step": 96500
    },
    {
      "epoch": 1.2764333557038148,
      "grad_norm": 2.6861891746520996,
      "learning_rate": 9.424059184295232e-05,
      "loss": 1.418,
      "step": 97000
    },
    {
      "epoch": 1.2830129090837314,
      "grad_norm": 2.414257287979126,
      "learning_rate": 9.420747618984542e-05,
      "loss": 1.4135,
      "step": 97500
    },
    {
      "epoch": 1.289592462463648,
      "grad_norm": 2.4285991191864014,
      "learning_rate": 9.417436053673851e-05,
      "loss": 1.4149,
      "step": 98000
    },
    {
      "epoch": 1.2961720158435646,
      "grad_norm": 2.1458792686462402,
      "learning_rate": 9.41412448836316e-05,
      "loss": 1.4246,
      "step": 98500
    },
    {
      "epoch": 1.302751569223481,
      "grad_norm": 2.3326683044433594,
      "learning_rate": 9.410812923052469e-05,
      "loss": 1.4209,
      "step": 99000
    },
    {
      "epoch": 1.3093311226033977,
      "grad_norm": 2.713840961456299,
      "learning_rate": 9.407507980872398e-05,
      "loss": 1.4124,
      "step": 99500
    },
    {
      "epoch": 1.3159106759833143,
      "grad_norm": 2.3583970069885254,
      "learning_rate": 9.404196415561708e-05,
      "loss": 1.4085,
      "step": 100000
    },
    {
      "epoch": 1.3224902293632308,
      "grad_norm": 2.4967503547668457,
      "learning_rate": 9.400891473381639e-05,
      "loss": 1.414,
      "step": 100500
    },
    {
      "epoch": 1.3290697827431475,
      "grad_norm": 2.6298279762268066,
      "learning_rate": 9.397579908070947e-05,
      "loss": 1.4117,
      "step": 101000
    },
    {
      "epoch": 1.335649336123064,
      "grad_norm": 2.4570376873016357,
      "learning_rate": 9.394268342760256e-05,
      "loss": 1.4216,
      "step": 101500
    },
    {
      "epoch": 1.3422288895029806,
      "grad_norm": 2.7450883388519287,
      "learning_rate": 9.390956777449566e-05,
      "loss": 1.4163,
      "step": 102000
    },
    {
      "epoch": 1.348808442882897,
      "grad_norm": 2.127713441848755,
      "learning_rate": 9.387645212138874e-05,
      "loss": 1.4099,
      "step": 102500
    },
    {
      "epoch": 1.3553879962628137,
      "grad_norm": 2.3652710914611816,
      "learning_rate": 9.384333646828183e-05,
      "loss": 1.3933,
      "step": 103000
    },
    {
      "epoch": 1.3619675496427304,
      "grad_norm": 2.6360085010528564,
      "learning_rate": 9.381022081517493e-05,
      "loss": 1.4026,
      "step": 103500
    },
    {
      "epoch": 1.3685471030226468,
      "grad_norm": 2.5368905067443848,
      "learning_rate": 9.377710516206801e-05,
      "loss": 1.4199,
      "step": 104000
    },
    {
      "epoch": 1.3751266564025633,
      "grad_norm": 2.4265260696411133,
      "learning_rate": 9.374398950896111e-05,
      "loss": 1.3974,
      "step": 104500
    },
    {
      "epoch": 1.38170620978248,
      "grad_norm": 2.455392837524414,
      "learning_rate": 9.371087385585418e-05,
      "loss": 1.4085,
      "step": 105000
    },
    {
      "epoch": 1.3882857631623966,
      "grad_norm": 2.3982129096984863,
      "learning_rate": 9.367775820274728e-05,
      "loss": 1.3964,
      "step": 105500
    },
    {
      "epoch": 1.394865316542313,
      "grad_norm": 2.4132423400878906,
      "learning_rate": 9.364470878094657e-05,
      "loss": 1.4078,
      "step": 106000
    },
    {
      "epoch": 1.4014448699222297,
      "grad_norm": 2.558497190475464,
      "learning_rate": 9.361159312783967e-05,
      "loss": 1.3943,
      "step": 106500
    },
    {
      "epoch": 1.4080244233021464,
      "grad_norm": 2.499019145965576,
      "learning_rate": 9.357847747473276e-05,
      "loss": 1.4048,
      "step": 107000
    },
    {
      "epoch": 1.4146039766820628,
      "grad_norm": 2.501918315887451,
      "learning_rate": 9.354536182162586e-05,
      "loss": 1.3873,
      "step": 107500
    },
    {
      "epoch": 1.4211835300619793,
      "grad_norm": 2.671891927719116,
      "learning_rate": 9.351224616851894e-05,
      "loss": 1.4003,
      "step": 108000
    },
    {
      "epoch": 1.427763083441896,
      "grad_norm": 2.769195795059204,
      "learning_rate": 9.347926297802447e-05,
      "loss": 1.39,
      "step": 108500
    },
    {
      "epoch": 1.4343426368218126,
      "grad_norm": 2.4446723461151123,
      "learning_rate": 9.344614732491754e-05,
      "loss": 1.3967,
      "step": 109000
    },
    {
      "epoch": 1.440922190201729,
      "grad_norm": 2.5340118408203125,
      "learning_rate": 9.341303167181064e-05,
      "loss": 1.3942,
      "step": 109500
    },
    {
      "epoch": 1.4475017435816457,
      "grad_norm": 2.54831600189209,
      "learning_rate": 9.337998225000993e-05,
      "loss": 1.3952,
      "step": 110000
    },
    {
      "epoch": 1.4540812969615622,
      "grad_norm": 2.5750696659088135,
      "learning_rate": 9.334686659690303e-05,
      "loss": 1.3842,
      "step": 110500
    },
    {
      "epoch": 1.4606608503414789,
      "grad_norm": 2.3987033367156982,
      "learning_rate": 9.331375094379613e-05,
      "loss": 1.3843,
      "step": 111000
    },
    {
      "epoch": 1.4672404037213953,
      "grad_norm": 2.4515678882598877,
      "learning_rate": 9.328063529068921e-05,
      "loss": 1.3899,
      "step": 111500
    },
    {
      "epoch": 1.473819957101312,
      "grad_norm": 2.5773353576660156,
      "learning_rate": 9.32475196375823e-05,
      "loss": 1.3954,
      "step": 112000
    },
    {
      "epoch": 1.4803995104812286,
      "grad_norm": 2.357548236846924,
      "learning_rate": 9.321440398447538e-05,
      "loss": 1.3905,
      "step": 112500
    },
    {
      "epoch": 1.486979063861145,
      "grad_norm": 2.602388381958008,
      "learning_rate": 9.318128833136848e-05,
      "loss": 1.3886,
      "step": 113000
    },
    {
      "epoch": 1.4935586172410618,
      "grad_norm": 3.421190023422241,
      "learning_rate": 9.314817267826157e-05,
      "loss": 1.3724,
      "step": 113500
    },
    {
      "epoch": 1.5001381706209782,
      "grad_norm": 2.5649163722991943,
      "learning_rate": 9.311505702515465e-05,
      "loss": 1.3914,
      "step": 114000
    },
    {
      "epoch": 1.5067177240008949,
      "grad_norm": 2.367245674133301,
      "learning_rate": 9.308194137204775e-05,
      "loss": 1.3795,
      "step": 114500
    },
    {
      "epoch": 1.5132972773808113,
      "grad_norm": 2.7248499393463135,
      "learning_rate": 9.304882571894083e-05,
      "loss": 1.3793,
      "step": 115000
    },
    {
      "epoch": 1.519876830760728,
      "grad_norm": 2.872533082962036,
      "learning_rate": 9.301571006583393e-05,
      "loss": 1.3819,
      "step": 115500
    },
    {
      "epoch": 1.5264563841406447,
      "grad_norm": 2.173344135284424,
      "learning_rate": 9.298266064403322e-05,
      "loss": 1.3885,
      "step": 116000
    },
    {
      "epoch": 1.533035937520561,
      "grad_norm": 2.8060801029205322,
      "learning_rate": 9.294954499092632e-05,
      "loss": 1.3837,
      "step": 116500
    },
    {
      "epoch": 1.5396154909004776,
      "grad_norm": 2.6260898113250732,
      "learning_rate": 9.29164293378194e-05,
      "loss": 1.3752,
      "step": 117000
    },
    {
      "epoch": 1.5461950442803942,
      "grad_norm": 2.7163803577423096,
      "learning_rate": 9.28833136847125e-05,
      "loss": 1.3765,
      "step": 117500
    },
    {
      "epoch": 1.552774597660311,
      "grad_norm": 2.704744815826416,
      "learning_rate": 9.285019803160558e-05,
      "loss": 1.3823,
      "step": 118000
    },
    {
      "epoch": 1.5593541510402273,
      "grad_norm": 2.4068174362182617,
      "learning_rate": 9.281708237849868e-05,
      "loss": 1.3749,
      "step": 118500
    },
    {
      "epoch": 1.565933704420144,
      "grad_norm": 2.439023733139038,
      "learning_rate": 9.278396672539176e-05,
      "loss": 1.389,
      "step": 119000
    },
    {
      "epoch": 1.5725132578000607,
      "grad_norm": 2.718893527984619,
      "learning_rate": 9.275085107228485e-05,
      "loss": 1.3703,
      "step": 119500
    },
    {
      "epoch": 1.5790928111799771,
      "grad_norm": 2.5925958156585693,
      "learning_rate": 9.271773541917795e-05,
      "loss": 1.3796,
      "step": 120000
    },
    {
      "epoch": 1.5856723645598936,
      "grad_norm": 2.2720694541931152,
      "learning_rate": 9.268468599737724e-05,
      "loss": 1.3623,
      "step": 120500
    },
    {
      "epoch": 1.5922519179398102,
      "grad_norm": 2.560147285461426,
      "learning_rate": 9.265157034427034e-05,
      "loss": 1.3837,
      "step": 121000
    },
    {
      "epoch": 1.598831471319727,
      "grad_norm": 2.442469596862793,
      "learning_rate": 9.261845469116342e-05,
      "loss": 1.3777,
      "step": 121500
    },
    {
      "epoch": 1.6054110246996434,
      "grad_norm": 2.4116127490997314,
      "learning_rate": 9.258533903805652e-05,
      "loss": 1.3575,
      "step": 122000
    },
    {
      "epoch": 1.6119905780795598,
      "grad_norm": 2.5328073501586914,
      "learning_rate": 9.25522233849496e-05,
      "loss": 1.3751,
      "step": 122500
    },
    {
      "epoch": 1.6185701314594765,
      "grad_norm": 2.2568647861480713,
      "learning_rate": 9.251917396314891e-05,
      "loss": 1.3754,
      "step": 123000
    },
    {
      "epoch": 1.6251496848393931,
      "grad_norm": 2.4399757385253906,
      "learning_rate": 9.248605831004198e-05,
      "loss": 1.363,
      "step": 123500
    },
    {
      "epoch": 1.6317292382193096,
      "grad_norm": 2.5836904048919678,
      "learning_rate": 9.245294265693508e-05,
      "loss": 1.3692,
      "step": 124000
    },
    {
      "epoch": 1.6383087915992263,
      "grad_norm": 2.796750545501709,
      "learning_rate": 9.241982700382817e-05,
      "loss": 1.3646,
      "step": 124500
    },
    {
      "epoch": 1.644888344979143,
      "grad_norm": 2.5795059204101562,
      "learning_rate": 9.238671135072127e-05,
      "loss": 1.3626,
      "step": 125000
    },
    {
      "epoch": 1.6514678983590594,
      "grad_norm": 2.656078338623047,
      "learning_rate": 9.235372816022678e-05,
      "loss": 1.3666,
      "step": 125500
    },
    {
      "epoch": 1.6580474517389758,
      "grad_norm": 2.371997833251953,
      "learning_rate": 9.232061250711988e-05,
      "loss": 1.3632,
      "step": 126000
    },
    {
      "epoch": 1.6646270051188925,
      "grad_norm": 2.466104745864868,
      "learning_rate": 9.228749685401296e-05,
      "loss": 1.3583,
      "step": 126500
    },
    {
      "epoch": 1.6712065584988092,
      "grad_norm": 2.350619316101074,
      "learning_rate": 9.225438120090605e-05,
      "loss": 1.3555,
      "step": 127000
    },
    {
      "epoch": 1.6777861118787256,
      "grad_norm": 2.272038459777832,
      "learning_rate": 9.222126554779915e-05,
      "loss": 1.3632,
      "step": 127500
    },
    {
      "epoch": 1.6843656652586423,
      "grad_norm": 2.3408687114715576,
      "learning_rate": 9.218821612599844e-05,
      "loss": 1.352,
      "step": 128000
    },
    {
      "epoch": 1.690945218638559,
      "grad_norm": 2.232896327972412,
      "learning_rate": 9.215510047289154e-05,
      "loss": 1.3441,
      "step": 128500
    },
    {
      "epoch": 1.6975247720184754,
      "grad_norm": 2.5284006595611572,
      "learning_rate": 9.212198481978462e-05,
      "loss": 1.3546,
      "step": 129000
    },
    {
      "epoch": 1.7041043253983919,
      "grad_norm": 2.483107805252075,
      "learning_rate": 9.208886916667771e-05,
      "loss": 1.36,
      "step": 129500
    },
    {
      "epoch": 1.7106838787783085,
      "grad_norm": 2.5915122032165527,
      "learning_rate": 9.205575351357079e-05,
      "loss": 1.3534,
      "step": 130000
    },
    {
      "epoch": 1.7172634321582252,
      "grad_norm": 2.455352783203125,
      "learning_rate": 9.202263786046389e-05,
      "loss": 1.3536,
      "step": 130500
    },
    {
      "epoch": 1.7238429855381416,
      "grad_norm": 2.3403701782226562,
      "learning_rate": 9.198952220735698e-05,
      "loss": 1.3573,
      "step": 131000
    },
    {
      "epoch": 1.730422538918058,
      "grad_norm": 2.424572467803955,
      "learning_rate": 9.195640655425006e-05,
      "loss": 1.3618,
      "step": 131500
    },
    {
      "epoch": 1.737002092297975,
      "grad_norm": 2.435964345932007,
      "learning_rate": 9.192335713244937e-05,
      "loss": 1.3539,
      "step": 132000
    },
    {
      "epoch": 1.7435816456778914,
      "grad_norm": 2.3409171104431152,
      "learning_rate": 9.189024147934247e-05,
      "loss": 1.3519,
      "step": 132500
    },
    {
      "epoch": 1.7501611990578079,
      "grad_norm": 2.6631362438201904,
      "learning_rate": 9.185719205754176e-05,
      "loss": 1.3545,
      "step": 133000
    },
    {
      "epoch": 1.7567407524377245,
      "grad_norm": 2.6280205249786377,
      "learning_rate": 9.182407640443486e-05,
      "loss": 1.3391,
      "step": 133500
    },
    {
      "epoch": 1.7633203058176412,
      "grad_norm": 2.430891275405884,
      "learning_rate": 9.179096075132794e-05,
      "loss": 1.3559,
      "step": 134000
    },
    {
      "epoch": 1.7698998591975577,
      "grad_norm": 2.2667598724365234,
      "learning_rate": 9.175784509822103e-05,
      "loss": 1.3465,
      "step": 134500
    },
    {
      "epoch": 1.776479412577474,
      "grad_norm": 2.2896816730499268,
      "learning_rate": 9.172472944511413e-05,
      "loss": 1.3452,
      "step": 135000
    },
    {
      "epoch": 1.7830589659573908,
      "grad_norm": 2.7149407863616943,
      "learning_rate": 9.169161379200721e-05,
      "loss": 1.3374,
      "step": 135500
    },
    {
      "epoch": 1.7896385193373074,
      "grad_norm": 2.6321165561676025,
      "learning_rate": 9.16584981389003e-05,
      "loss": 1.3552,
      "step": 136000
    },
    {
      "epoch": 1.796218072717224,
      "grad_norm": 2.486288547515869,
      "learning_rate": 9.162538248579338e-05,
      "loss": 1.3626,
      "step": 136500
    },
    {
      "epoch": 1.8027976260971406,
      "grad_norm": 2.6107242107391357,
      "learning_rate": 9.159226683268648e-05,
      "loss": 1.346,
      "step": 137000
    },
    {
      "epoch": 1.8093771794770572,
      "grad_norm": 2.3349876403808594,
      "learning_rate": 9.155928364219199e-05,
      "loss": 1.3436,
      "step": 137500
    },
    {
      "epoch": 1.8159567328569737,
      "grad_norm": 2.6851255893707275,
      "learning_rate": 9.152616798908509e-05,
      "loss": 1.3486,
      "step": 138000
    },
    {
      "epoch": 1.8225362862368901,
      "grad_norm": 2.611515998840332,
      "learning_rate": 9.149305233597818e-05,
      "loss": 1.3345,
      "step": 138500
    },
    {
      "epoch": 1.8291158396168068,
      "grad_norm": 2.5223236083984375,
      "learning_rate": 9.145993668287126e-05,
      "loss": 1.3443,
      "step": 139000
    },
    {
      "epoch": 1.8356953929967235,
      "grad_norm": 2.417102098464966,
      "learning_rate": 9.142682102976436e-05,
      "loss": 1.3348,
      "step": 139500
    },
    {
      "epoch": 1.84227494637664,
      "grad_norm": 2.637437582015991,
      "learning_rate": 9.139370537665744e-05,
      "loss": 1.3352,
      "step": 140000
    },
    {
      "epoch": 1.8488544997565566,
      "grad_norm": 2.6592462062835693,
      "learning_rate": 9.136058972355053e-05,
      "loss": 1.3435,
      "step": 140500
    },
    {
      "epoch": 1.8554340531364732,
      "grad_norm": 2.4774460792541504,
      "learning_rate": 9.132754030174983e-05,
      "loss": 1.3312,
      "step": 141000
    },
    {
      "epoch": 1.8620136065163897,
      "grad_norm": 2.2149710655212402,
      "learning_rate": 9.129442464864293e-05,
      "loss": 1.3358,
      "step": 141500
    },
    {
      "epoch": 1.8685931598963061,
      "grad_norm": 2.4962191581726074,
      "learning_rate": 9.1261308995536e-05,
      "loss": 1.3296,
      "step": 142000
    },
    {
      "epoch": 1.8751727132762228,
      "grad_norm": 2.394467830657959,
      "learning_rate": 9.12281933424291e-05,
      "loss": 1.3275,
      "step": 142500
    },
    {
      "epoch": 1.8817522666561395,
      "grad_norm": 2.577522039413452,
      "learning_rate": 9.119514392062841e-05,
      "loss": 1.3444,
      "step": 143000
    },
    {
      "epoch": 1.888331820036056,
      "grad_norm": 2.5096583366394043,
      "learning_rate": 9.11620282675215e-05,
      "loss": 1.33,
      "step": 143500
    },
    {
      "epoch": 1.8949113734159724,
      "grad_norm": 2.4326233863830566,
      "learning_rate": 9.112891261441458e-05,
      "loss": 1.3246,
      "step": 144000
    },
    {
      "epoch": 1.901490926795889,
      "grad_norm": 2.4166879653930664,
      "learning_rate": 9.109579696130768e-05,
      "loss": 1.3361,
      "step": 144500
    },
    {
      "epoch": 1.9080704801758057,
      "grad_norm": 2.3146395683288574,
      "learning_rate": 9.106274753950697e-05,
      "loss": 1.3308,
      "step": 145000
    },
    {
      "epoch": 1.9146500335557222,
      "grad_norm": 2.507652997970581,
      "learning_rate": 9.102963188640007e-05,
      "loss": 1.3248,
      "step": 145500
    },
    {
      "epoch": 1.9212295869356388,
      "grad_norm": 2.722282886505127,
      "learning_rate": 9.099651623329317e-05,
      "loss": 1.3205,
      "step": 146000
    },
    {
      "epoch": 1.9278091403155555,
      "grad_norm": 2.4875195026397705,
      "learning_rate": 9.096340058018624e-05,
      "loss": 1.3337,
      "step": 146500
    },
    {
      "epoch": 1.934388693695472,
      "grad_norm": 2.3863484859466553,
      "learning_rate": 9.093028492707934e-05,
      "loss": 1.321,
      "step": 147000
    },
    {
      "epoch": 1.9409682470753884,
      "grad_norm": 2.5089006423950195,
      "learning_rate": 9.089716927397242e-05,
      "loss": 1.3325,
      "step": 147500
    },
    {
      "epoch": 1.947547800455305,
      "grad_norm": 2.549931287765503,
      "learning_rate": 9.086411985217173e-05,
      "loss": 1.3254,
      "step": 148000
    },
    {
      "epoch": 1.9541273538352217,
      "grad_norm": 2.538729190826416,
      "learning_rate": 9.083100419906481e-05,
      "loss": 1.3368,
      "step": 148500
    },
    {
      "epoch": 1.9607069072151382,
      "grad_norm": 2.547438383102417,
      "learning_rate": 9.079788854595791e-05,
      "loss": 1.3241,
      "step": 149000
    },
    {
      "epoch": 1.9672864605950549,
      "grad_norm": 2.2756104469299316,
      "learning_rate": 9.0764772892851e-05,
      "loss": 1.3267,
      "step": 149500
    },
    {
      "epoch": 1.9738660139749715,
      "grad_norm": 2.2641665935516357,
      "learning_rate": 9.073165723974408e-05,
      "loss": 1.3293,
      "step": 150000
    },
    {
      "epoch": 1.980445567354888,
      "grad_norm": 2.3214304447174072,
      "learning_rate": 9.069854158663718e-05,
      "loss": 1.3287,
      "step": 150500
    },
    {
      "epoch": 1.9870251207348044,
      "grad_norm": 2.400092840194702,
      "learning_rate": 9.066549216483647e-05,
      "loss": 1.3191,
      "step": 151000
    },
    {
      "epoch": 1.993604674114721,
      "grad_norm": 3.025855302810669,
      "learning_rate": 9.063237651172957e-05,
      "loss": 1.3328,
      "step": 151500
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.2601280212402344,
      "eval_runtime": 49.446,
      "eval_samples_per_second": 2022.407,
      "eval_steps_per_second": 15.815,
      "step": 151986
    },
    {
      "epoch": 2.0001842274946378,
      "grad_norm": 2.720566749572754,
      "learning_rate": 9.059926085862266e-05,
      "loss": 1.3229,
      "step": 152000
    },
    {
      "epoch": 2.006763780874554,
      "grad_norm": 2.248600721359253,
      "learning_rate": 9.056614520551576e-05,
      "loss": 1.3134,
      "step": 152500
    },
    {
      "epoch": 2.0133433342544707,
      "grad_norm": 2.4969754219055176,
      "learning_rate": 9.053302955240883e-05,
      "loss": 1.3148,
      "step": 153000
    },
    {
      "epoch": 2.0199228876343875,
      "grad_norm": 2.642563819885254,
      "learning_rate": 9.049991389930193e-05,
      "loss": 1.3158,
      "step": 153500
    },
    {
      "epoch": 2.026502441014304,
      "grad_norm": 2.6636054515838623,
      "learning_rate": 9.046679824619501e-05,
      "loss": 1.3301,
      "step": 154000
    },
    {
      "epoch": 2.0330819943942204,
      "grad_norm": 2.3123459815979004,
      "learning_rate": 9.043368259308811e-05,
      "loss": 1.321,
      "step": 154500
    },
    {
      "epoch": 2.039661547774137,
      "grad_norm": 2.5888679027557373,
      "learning_rate": 9.04005669399812e-05,
      "loss": 1.3085,
      "step": 155000
    },
    {
      "epoch": 2.0462411011540538,
      "grad_norm": 2.6440255641937256,
      "learning_rate": 9.03675175181805e-05,
      "loss": 1.3075,
      "step": 155500
    },
    {
      "epoch": 2.0528206545339702,
      "grad_norm": 2.5016331672668457,
      "learning_rate": 9.033440186507359e-05,
      "loss": 1.3046,
      "step": 156000
    },
    {
      "epoch": 2.0594002079138867,
      "grad_norm": 2.566767930984497,
      "learning_rate": 9.030128621196667e-05,
      "loss": 1.315,
      "step": 156500
    },
    {
      "epoch": 2.0659797612938036,
      "grad_norm": 2.6419522762298584,
      "learning_rate": 9.026817055885977e-05,
      "loss": 1.3126,
      "step": 157000
    },
    {
      "epoch": 2.07255931467372,
      "grad_norm": 2.6065409183502197,
      "learning_rate": 9.023512113705906e-05,
      "loss": 1.3098,
      "step": 157500
    },
    {
      "epoch": 2.0791388680536365,
      "grad_norm": 2.776097297668457,
      "learning_rate": 9.020200548395216e-05,
      "loss": 1.3149,
      "step": 158000
    },
    {
      "epoch": 2.085718421433553,
      "grad_norm": 2.5781149864196777,
      "learning_rate": 9.016895606215147e-05,
      "loss": 1.3122,
      "step": 158500
    },
    {
      "epoch": 2.09229797481347,
      "grad_norm": 2.402381420135498,
      "learning_rate": 9.013584040904455e-05,
      "loss": 1.3149,
      "step": 159000
    },
    {
      "epoch": 2.0988775281933862,
      "grad_norm": 2.06514048576355,
      "learning_rate": 9.010272475593764e-05,
      "loss": 1.3067,
      "step": 159500
    },
    {
      "epoch": 2.1054570815733027,
      "grad_norm": 2.4490790367126465,
      "learning_rate": 9.006967533413695e-05,
      "loss": 1.3139,
      "step": 160000
    },
    {
      "epoch": 2.1120366349532196,
      "grad_norm": 2.627497911453247,
      "learning_rate": 9.003655968103003e-05,
      "loss": 1.3081,
      "step": 160500
    },
    {
      "epoch": 2.118616188333136,
      "grad_norm": 2.440034866333008,
      "learning_rate": 9.000344402792313e-05,
      "loss": 1.2914,
      "step": 161000
    },
    {
      "epoch": 2.1251957417130525,
      "grad_norm": 2.4191529750823975,
      "learning_rate": 8.997032837481621e-05,
      "loss": 1.3127,
      "step": 161500
    },
    {
      "epoch": 2.131775295092969,
      "grad_norm": 2.5034234523773193,
      "learning_rate": 8.99372127217093e-05,
      "loss": 1.3081,
      "step": 162000
    },
    {
      "epoch": 2.138354848472886,
      "grad_norm": 2.5589759349823,
      "learning_rate": 8.99040970686024e-05,
      "loss": 1.3056,
      "step": 162500
    },
    {
      "epoch": 2.1449344018528023,
      "grad_norm": 2.4576187133789062,
      "learning_rate": 8.98710476468017e-05,
      "loss": 1.3078,
      "step": 163000
    },
    {
      "epoch": 2.1515139552327187,
      "grad_norm": 2.546689748764038,
      "learning_rate": 8.983793199369479e-05,
      "loss": 1.3046,
      "step": 163500
    },
    {
      "epoch": 2.1580935086126356,
      "grad_norm": 2.8249905109405518,
      "learning_rate": 8.980481634058787e-05,
      "loss": 1.2987,
      "step": 164000
    },
    {
      "epoch": 2.164673061992552,
      "grad_norm": 2.437106132507324,
      "learning_rate": 8.977170068748097e-05,
      "loss": 1.3101,
      "step": 164500
    },
    {
      "epoch": 2.1712526153724685,
      "grad_norm": 2.4512147903442383,
      "learning_rate": 8.973858503437405e-05,
      "loss": 1.298,
      "step": 165000
    },
    {
      "epoch": 2.177832168752385,
      "grad_norm": 2.930427074432373,
      "learning_rate": 8.970546938126714e-05,
      "loss": 1.3008,
      "step": 165500
    },
    {
      "epoch": 2.184411722132302,
      "grad_norm": 2.6481809616088867,
      "learning_rate": 8.967235372816023e-05,
      "loss": 1.3025,
      "step": 166000
    },
    {
      "epoch": 2.1909912755122183,
      "grad_norm": 2.6559364795684814,
      "learning_rate": 8.963923807505332e-05,
      "loss": 1.2997,
      "step": 166500
    },
    {
      "epoch": 2.1975708288921347,
      "grad_norm": 2.5795600414276123,
      "learning_rate": 8.960618865325262e-05,
      "loss": 1.2982,
      "step": 167000
    },
    {
      "epoch": 2.204150382272051,
      "grad_norm": 2.590757131576538,
      "learning_rate": 8.957307300014571e-05,
      "loss": 1.2925,
      "step": 167500
    },
    {
      "epoch": 2.210729935651968,
      "grad_norm": 2.305295705795288,
      "learning_rate": 8.95399573470388e-05,
      "loss": 1.2907,
      "step": 168000
    },
    {
      "epoch": 2.2173094890318845,
      "grad_norm": 2.5161664485931396,
      "learning_rate": 8.950684169393188e-05,
      "loss": 1.3037,
      "step": 168500
    },
    {
      "epoch": 2.223889042411801,
      "grad_norm": 2.3446054458618164,
      "learning_rate": 8.947372604082498e-05,
      "loss": 1.2979,
      "step": 169000
    },
    {
      "epoch": 2.230468595791718,
      "grad_norm": 2.581738233566284,
      "learning_rate": 8.944067661902429e-05,
      "loss": 1.3042,
      "step": 169500
    },
    {
      "epoch": 2.2370481491716343,
      "grad_norm": 2.339862823486328,
      "learning_rate": 8.940756096591737e-05,
      "loss": 1.2999,
      "step": 170000
    },
    {
      "epoch": 2.2436277025515508,
      "grad_norm": 2.405687093734741,
      "learning_rate": 8.937444531281046e-05,
      "loss": 1.2982,
      "step": 170500
    },
    {
      "epoch": 2.250207255931467,
      "grad_norm": 2.3780815601348877,
      "learning_rate": 8.934132965970356e-05,
      "loss": 1.2848,
      "step": 171000
    },
    {
      "epoch": 2.256786809311384,
      "grad_norm": 2.6442179679870605,
      "learning_rate": 8.930821400659664e-05,
      "loss": 1.2997,
      "step": 171500
    },
    {
      "epoch": 2.2633663626913005,
      "grad_norm": 2.678795337677002,
      "learning_rate": 8.927509835348973e-05,
      "loss": 1.2964,
      "step": 172000
    },
    {
      "epoch": 2.269945916071217,
      "grad_norm": 2.5761940479278564,
      "learning_rate": 8.924198270038281e-05,
      "loss": 1.2999,
      "step": 172500
    },
    {
      "epoch": 2.2765254694511334,
      "grad_norm": 2.4614694118499756,
      "learning_rate": 8.920886704727591e-05,
      "loss": 1.298,
      "step": 173000
    },
    {
      "epoch": 2.2831050228310503,
      "grad_norm": 2.1049790382385254,
      "learning_rate": 8.9175751394169e-05,
      "loss": 1.2811,
      "step": 173500
    },
    {
      "epoch": 2.2896845762109668,
      "grad_norm": 2.507854461669922,
      "learning_rate": 8.914263574106208e-05,
      "loss": 1.2881,
      "step": 174000
    },
    {
      "epoch": 2.296264129590883,
      "grad_norm": 2.407914638519287,
      "learning_rate": 8.910952008795518e-05,
      "loss": 1.2841,
      "step": 174500
    },
    {
      "epoch": 2.3028436829708,
      "grad_norm": 2.7808644771575928,
      "learning_rate": 8.907640443484827e-05,
      "loss": 1.2973,
      "step": 175000
    },
    {
      "epoch": 2.3094232363507166,
      "grad_norm": 2.5101382732391357,
      "learning_rate": 8.904335501304757e-05,
      "loss": 1.2861,
      "step": 175500
    },
    {
      "epoch": 2.316002789730633,
      "grad_norm": 2.5759520530700684,
      "learning_rate": 8.901023935994066e-05,
      "loss": 1.2958,
      "step": 176000
    },
    {
      "epoch": 2.3225823431105495,
      "grad_norm": 2.5106101036071777,
      "learning_rate": 8.897712370683376e-05,
      "loss": 1.2854,
      "step": 176500
    },
    {
      "epoch": 2.3291618964904663,
      "grad_norm": 2.3938381671905518,
      "learning_rate": 8.894407428503305e-05,
      "loss": 1.2848,
      "step": 177000
    },
    {
      "epoch": 2.335741449870383,
      "grad_norm": 2.5092720985412598,
      "learning_rate": 8.891095863192615e-05,
      "loss": 1.2829,
      "step": 177500
    },
    {
      "epoch": 2.3423210032502992,
      "grad_norm": 2.4083592891693115,
      "learning_rate": 8.887784297881923e-05,
      "loss": 1.2859,
      "step": 178000
    },
    {
      "epoch": 2.348900556630216,
      "grad_norm": 2.545307159423828,
      "learning_rate": 8.884472732571232e-05,
      "loss": 1.2838,
      "step": 178500
    },
    {
      "epoch": 2.3554801100101326,
      "grad_norm": 2.631819725036621,
      "learning_rate": 8.881161167260542e-05,
      "loss": 1.2904,
      "step": 179000
    },
    {
      "epoch": 2.362059663390049,
      "grad_norm": 2.4089365005493164,
      "learning_rate": 8.87784960194985e-05,
      "loss": 1.2904,
      "step": 179500
    },
    {
      "epoch": 2.3686392167699655,
      "grad_norm": 2.461505889892578,
      "learning_rate": 8.87453803663916e-05,
      "loss": 1.2857,
      "step": 180000
    },
    {
      "epoch": 2.3752187701498824,
      "grad_norm": 2.631765604019165,
      "learning_rate": 8.871226471328467e-05,
      "loss": 1.2868,
      "step": 180500
    },
    {
      "epoch": 2.381798323529799,
      "grad_norm": 2.537003517150879,
      "learning_rate": 8.86792815227902e-05,
      "loss": 1.2929,
      "step": 181000
    },
    {
      "epoch": 2.3883778769097153,
      "grad_norm": 2.326262950897217,
      "learning_rate": 8.864629833229572e-05,
      "loss": 1.2728,
      "step": 181500
    },
    {
      "epoch": 2.394957430289632,
      "grad_norm": 2.5656440258026123,
      "learning_rate": 8.86131826791888e-05,
      "loss": 1.283,
      "step": 182000
    },
    {
      "epoch": 2.4015369836695486,
      "grad_norm": 2.5484371185302734,
      "learning_rate": 8.858006702608189e-05,
      "loss": 1.2815,
      "step": 182500
    },
    {
      "epoch": 2.408116537049465,
      "grad_norm": 2.4767537117004395,
      "learning_rate": 8.854695137297499e-05,
      "loss": 1.2786,
      "step": 183000
    },
    {
      "epoch": 2.4146960904293815,
      "grad_norm": 2.1993043422698975,
      "learning_rate": 8.851383571986808e-05,
      "loss": 1.2836,
      "step": 183500
    },
    {
      "epoch": 2.4212756438092984,
      "grad_norm": 2.5507354736328125,
      "learning_rate": 8.848072006676116e-05,
      "loss": 1.2893,
      "step": 184000
    },
    {
      "epoch": 2.427855197189215,
      "grad_norm": 2.4121358394622803,
      "learning_rate": 8.844760441365425e-05,
      "loss": 1.2882,
      "step": 184500
    },
    {
      "epoch": 2.4344347505691313,
      "grad_norm": 2.496734142303467,
      "learning_rate": 8.841448876054735e-05,
      "loss": 1.2827,
      "step": 185000
    },
    {
      "epoch": 2.441014303949048,
      "grad_norm": 2.454406976699829,
      "learning_rate": 8.838137310744043e-05,
      "loss": 1.2819,
      "step": 185500
    },
    {
      "epoch": 2.4475938573289646,
      "grad_norm": 2.441211223602295,
      "learning_rate": 8.834825745433352e-05,
      "loss": 1.2745,
      "step": 186000
    },
    {
      "epoch": 2.454173410708881,
      "grad_norm": 2.4780280590057373,
      "learning_rate": 8.831514180122661e-05,
      "loss": 1.2814,
      "step": 186500
    },
    {
      "epoch": 2.4607529640887975,
      "grad_norm": 2.9312641620635986,
      "learning_rate": 8.82820261481197e-05,
      "loss": 1.2794,
      "step": 187000
    },
    {
      "epoch": 2.4673325174687144,
      "grad_norm": 2.6647372245788574,
      "learning_rate": 8.824891049501279e-05,
      "loss": 1.2772,
      "step": 187500
    },
    {
      "epoch": 2.473912070848631,
      "grad_norm": 2.72605562210083,
      "learning_rate": 8.821592730451831e-05,
      "loss": 1.2788,
      "step": 188000
    },
    {
      "epoch": 2.4804916242285473,
      "grad_norm": 2.202538013458252,
      "learning_rate": 8.81828778827176e-05,
      "loss": 1.2706,
      "step": 188500
    },
    {
      "epoch": 2.4870711776084637,
      "grad_norm": 2.4035844802856445,
      "learning_rate": 8.81498284609169e-05,
      "loss": 1.2725,
      "step": 189000
    },
    {
      "epoch": 2.4936507309883806,
      "grad_norm": 2.473832845687866,
      "learning_rate": 8.811671280780999e-05,
      "loss": 1.2826,
      "step": 189500
    },
    {
      "epoch": 2.500230284368297,
      "grad_norm": 2.388871431350708,
      "learning_rate": 8.808359715470309e-05,
      "loss": 1.2738,
      "step": 190000
    },
    {
      "epoch": 2.5068098377482135,
      "grad_norm": 2.4969701766967773,
      "learning_rate": 8.805048150159619e-05,
      "loss": 1.2775,
      "step": 190500
    },
    {
      "epoch": 2.51338939112813,
      "grad_norm": 2.4958319664001465,
      "learning_rate": 8.801736584848926e-05,
      "loss": 1.2802,
      "step": 191000
    },
    {
      "epoch": 2.519968944508047,
      "grad_norm": 2.2347912788391113,
      "learning_rate": 8.798425019538236e-05,
      "loss": 1.2795,
      "step": 191500
    },
    {
      "epoch": 2.5265484978879633,
      "grad_norm": 2.488762855529785,
      "learning_rate": 8.795113454227544e-05,
      "loss": 1.2659,
      "step": 192000
    },
    {
      "epoch": 2.53312805126788,
      "grad_norm": 2.5741772651672363,
      "learning_rate": 8.791801888916854e-05,
      "loss": 1.277,
      "step": 192500
    },
    {
      "epoch": 2.5397076046477967,
      "grad_norm": 2.448676824569702,
      "learning_rate": 8.788490323606162e-05,
      "loss": 1.2737,
      "step": 193000
    },
    {
      "epoch": 2.546287158027713,
      "grad_norm": 2.4032952785491943,
      "learning_rate": 8.785178758295471e-05,
      "loss": 1.2846,
      "step": 193500
    },
    {
      "epoch": 2.5528667114076296,
      "grad_norm": 2.4913930892944336,
      "learning_rate": 8.781887062376644e-05,
      "loss": 1.2816,
      "step": 194000
    },
    {
      "epoch": 2.559446264787546,
      "grad_norm": 2.64023494720459,
      "learning_rate": 8.778575497065954e-05,
      "loss": 1.2725,
      "step": 194500
    },
    {
      "epoch": 2.566025818167463,
      "grad_norm": 2.646008014678955,
      "learning_rate": 8.775263931755262e-05,
      "loss": 1.2796,
      "step": 195000
    },
    {
      "epoch": 2.5726053715473793,
      "grad_norm": 2.5842981338500977,
      "learning_rate": 8.771952366444571e-05,
      "loss": 1.266,
      "step": 195500
    },
    {
      "epoch": 2.579184924927296,
      "grad_norm": 2.2472314834594727,
      "learning_rate": 8.76864080113388e-05,
      "loss": 1.269,
      "step": 196000
    },
    {
      "epoch": 2.5857644783072127,
      "grad_norm": 2.7989861965179443,
      "learning_rate": 8.76532923582319e-05,
      "loss": 1.2683,
      "step": 196500
    },
    {
      "epoch": 2.592344031687129,
      "grad_norm": 2.3489952087402344,
      "learning_rate": 8.762017670512498e-05,
      "loss": 1.2758,
      "step": 197000
    },
    {
      "epoch": 2.5989235850670456,
      "grad_norm": 2.576188325881958,
      "learning_rate": 8.758706105201807e-05,
      "loss": 1.2785,
      "step": 197500
    },
    {
      "epoch": 2.605503138446962,
      "grad_norm": 2.411442995071411,
      "learning_rate": 8.755394539891117e-05,
      "loss": 1.2701,
      "step": 198000
    },
    {
      "epoch": 2.612082691826879,
      "grad_norm": 2.650944471359253,
      "learning_rate": 8.752082974580425e-05,
      "loss": 1.2634,
      "step": 198500
    },
    {
      "epoch": 2.6186622452067954,
      "grad_norm": 2.2421460151672363,
      "learning_rate": 8.748771409269734e-05,
      "loss": 1.2728,
      "step": 199000
    },
    {
      "epoch": 2.625241798586712,
      "grad_norm": 2.236410617828369,
      "learning_rate": 8.745459843959042e-05,
      "loss": 1.2659,
      "step": 199500
    },
    {
      "epoch": 2.6318213519666287,
      "grad_norm": 2.567091941833496,
      "learning_rate": 8.742148278648352e-05,
      "loss": 1.2633,
      "step": 200000
    },
    {
      "epoch": 2.638400905346545,
      "grad_norm": 2.7983930110931396,
      "learning_rate": 8.738836713337661e-05,
      "loss": 1.264,
      "step": 200500
    },
    {
      "epoch": 2.6449804587264616,
      "grad_norm": 2.439687490463257,
      "learning_rate": 8.73552514802697e-05,
      "loss": 1.2585,
      "step": 201000
    },
    {
      "epoch": 2.651560012106378,
      "grad_norm": 2.2603721618652344,
      "learning_rate": 8.732213582716279e-05,
      "loss": 1.2514,
      "step": 201500
    },
    {
      "epoch": 2.658139565486295,
      "grad_norm": 2.5642313957214355,
      "learning_rate": 8.728902017405588e-05,
      "loss": 1.273,
      "step": 202000
    },
    {
      "epoch": 2.6647191188662114,
      "grad_norm": 2.3625152111053467,
      "learning_rate": 8.725590452094896e-05,
      "loss": 1.2633,
      "step": 202500
    },
    {
      "epoch": 2.671298672246128,
      "grad_norm": 2.5109050273895264,
      "learning_rate": 8.722278886784205e-05,
      "loss": 1.2615,
      "step": 203000
    },
    {
      "epoch": 2.6778782256260447,
      "grad_norm": 2.3490400314331055,
      "learning_rate": 8.718967321473515e-05,
      "loss": 1.2652,
      "step": 203500
    },
    {
      "epoch": 2.684457779005961,
      "grad_norm": 2.8492820262908936,
      "learning_rate": 8.715655756162823e-05,
      "loss": 1.2574,
      "step": 204000
    },
    {
      "epoch": 2.6910373323858776,
      "grad_norm": 2.6140074729919434,
      "learning_rate": 8.712344190852132e-05,
      "loss": 1.2606,
      "step": 204500
    },
    {
      "epoch": 2.697616885765794,
      "grad_norm": 2.262080192565918,
      "learning_rate": 8.709039248672062e-05,
      "loss": 1.2579,
      "step": 205000
    },
    {
      "epoch": 2.704196439145711,
      "grad_norm": 2.4303770065307617,
      "learning_rate": 8.705734306491993e-05,
      "loss": 1.2575,
      "step": 205500
    },
    {
      "epoch": 2.7107759925256274,
      "grad_norm": 2.1607022285461426,
      "learning_rate": 8.702422741181303e-05,
      "loss": 1.2606,
      "step": 206000
    },
    {
      "epoch": 2.717355545905544,
      "grad_norm": 2.2316386699676514,
      "learning_rate": 8.699111175870611e-05,
      "loss": 1.2691,
      "step": 206500
    },
    {
      "epoch": 2.7239350992854607,
      "grad_norm": 2.5012354850769043,
      "learning_rate": 8.695799610559921e-05,
      "loss": 1.2582,
      "step": 207000
    },
    {
      "epoch": 2.730514652665377,
      "grad_norm": 2.263758897781372,
      "learning_rate": 8.692488045249228e-05,
      "loss": 1.2555,
      "step": 207500
    },
    {
      "epoch": 2.7370942060452936,
      "grad_norm": 2.4431962966918945,
      "learning_rate": 8.689176479938538e-05,
      "loss": 1.2525,
      "step": 208000
    },
    {
      "epoch": 2.74367375942521,
      "grad_norm": 2.39505672454834,
      "learning_rate": 8.685864914627847e-05,
      "loss": 1.2563,
      "step": 208500
    },
    {
      "epoch": 2.7502533128051265,
      "grad_norm": 2.411895275115967,
      "learning_rate": 8.682553349317155e-05,
      "loss": 1.2533,
      "step": 209000
    },
    {
      "epoch": 2.7568328661850434,
      "grad_norm": 2.288642644882202,
      "learning_rate": 8.679248407137086e-05,
      "loss": 1.2607,
      "step": 209500
    },
    {
      "epoch": 2.76341241956496,
      "grad_norm": 2.840708017349243,
      "learning_rate": 8.675936841826396e-05,
      "loss": 1.2547,
      "step": 210000
    },
    {
      "epoch": 2.7699919729448768,
      "grad_norm": 2.5602076053619385,
      "learning_rate": 8.672625276515704e-05,
      "loss": 1.2628,
      "step": 210500
    },
    {
      "epoch": 2.776571526324793,
      "grad_norm": 2.3073248863220215,
      "learning_rate": 8.669333580596877e-05,
      "loss": 1.266,
      "step": 211000
    },
    {
      "epoch": 2.7831510797047097,
      "grad_norm": 2.391941785812378,
      "learning_rate": 8.666022015286186e-05,
      "loss": 1.2589,
      "step": 211500
    },
    {
      "epoch": 2.789730633084626,
      "grad_norm": 2.5731241703033447,
      "learning_rate": 8.662710449975496e-05,
      "loss": 1.2611,
      "step": 212000
    },
    {
      "epoch": 2.7963101864645425,
      "grad_norm": 2.4400107860565186,
      "learning_rate": 8.659405507795425e-05,
      "loss": 1.2597,
      "step": 212500
    },
    {
      "epoch": 2.8028897398444594,
      "grad_norm": 2.309630870819092,
      "learning_rate": 8.656093942484735e-05,
      "loss": 1.2642,
      "step": 213000
    },
    {
      "epoch": 2.809469293224376,
      "grad_norm": 2.8115949630737305,
      "learning_rate": 8.652782377174043e-05,
      "loss": 1.26,
      "step": 213500
    },
    {
      "epoch": 2.8160488466042928,
      "grad_norm": 2.5148255825042725,
      "learning_rate": 8.649470811863352e-05,
      "loss": 1.2419,
      "step": 214000
    },
    {
      "epoch": 2.8226283999842092,
      "grad_norm": 2.396822690963745,
      "learning_rate": 8.646159246552661e-05,
      "loss": 1.2485,
      "step": 214500
    },
    {
      "epoch": 2.8292079533641257,
      "grad_norm": 2.434008836746216,
      "learning_rate": 8.64284768124197e-05,
      "loss": 1.245,
      "step": 215000
    },
    {
      "epoch": 2.835787506744042,
      "grad_norm": 2.4131901264190674,
      "learning_rate": 8.63953611593128e-05,
      "loss": 1.2507,
      "step": 215500
    },
    {
      "epoch": 2.8423670601239586,
      "grad_norm": 2.10672664642334,
      "learning_rate": 8.636224550620587e-05,
      "loss": 1.2522,
      "step": 216000
    },
    {
      "epoch": 2.8489466135038755,
      "grad_norm": 2.5935862064361572,
      "learning_rate": 8.632912985309897e-05,
      "loss": 1.2482,
      "step": 216500
    },
    {
      "epoch": 2.855526166883792,
      "grad_norm": 2.3243041038513184,
      "learning_rate": 8.629601419999205e-05,
      "loss": 1.2382,
      "step": 217000
    },
    {
      "epoch": 2.8621057202637084,
      "grad_norm": 2.2856605052948,
      "learning_rate": 8.626289854688514e-05,
      "loss": 1.2613,
      "step": 217500
    },
    {
      "epoch": 2.8686852736436252,
      "grad_norm": 2.3677661418914795,
      "learning_rate": 8.622978289377824e-05,
      "loss": 1.253,
      "step": 218000
    },
    {
      "epoch": 2.8752648270235417,
      "grad_norm": 2.136608600616455,
      "learning_rate": 8.619666724067132e-05,
      "loss": 1.2493,
      "step": 218500
    },
    {
      "epoch": 2.881844380403458,
      "grad_norm": 2.427839517593384,
      "learning_rate": 8.616355158756442e-05,
      "loss": 1.249,
      "step": 219000
    },
    {
      "epoch": 2.8884239337833746,
      "grad_norm": 2.340036630630493,
      "learning_rate": 8.61304359344575e-05,
      "loss": 1.2497,
      "step": 219500
    },
    {
      "epoch": 2.8950034871632915,
      "grad_norm": 2.4987716674804688,
      "learning_rate": 8.60973202813506e-05,
      "loss": 1.2413,
      "step": 220000
    },
    {
      "epoch": 2.901583040543208,
      "grad_norm": 2.4221484661102295,
      "learning_rate": 8.606420462824368e-05,
      "loss": 1.2488,
      "step": 220500
    },
    {
      "epoch": 2.9081625939231244,
      "grad_norm": 2.8365936279296875,
      "learning_rate": 8.603115520644298e-05,
      "loss": 1.2441,
      "step": 221000
    },
    {
      "epoch": 2.9147421473030413,
      "grad_norm": 2.5157461166381836,
      "learning_rate": 8.599803955333607e-05,
      "loss": 1.2334,
      "step": 221500
    },
    {
      "epoch": 2.9213217006829577,
      "grad_norm": 2.4614131450653076,
      "learning_rate": 8.596492390022917e-05,
      "loss": 1.248,
      "step": 222000
    },
    {
      "epoch": 2.927901254062874,
      "grad_norm": 2.308462381362915,
      "learning_rate": 8.593180824712225e-05,
      "loss": 1.2449,
      "step": 222500
    },
    {
      "epoch": 2.9344808074427906,
      "grad_norm": 2.560129165649414,
      "learning_rate": 8.589869259401534e-05,
      "loss": 1.2431,
      "step": 223000
    },
    {
      "epoch": 2.9410603608227075,
      "grad_norm": 2.417508125305176,
      "learning_rate": 8.586557694090844e-05,
      "loss": 1.2415,
      "step": 223500
    },
    {
      "epoch": 2.947639914202624,
      "grad_norm": 2.7131266593933105,
      "learning_rate": 8.583246128780152e-05,
      "loss": 1.247,
      "step": 224000
    },
    {
      "epoch": 2.9542194675825404,
      "grad_norm": 2.2705585956573486,
      "learning_rate": 8.579934563469462e-05,
      "loss": 1.2488,
      "step": 224500
    },
    {
      "epoch": 2.9607990209624573,
      "grad_norm": 2.5807881355285645,
      "learning_rate": 8.57662299815877e-05,
      "loss": 1.2505,
      "step": 225000
    },
    {
      "epoch": 2.9673785743423737,
      "grad_norm": 2.4092020988464355,
      "learning_rate": 8.573318055978701e-05,
      "loss": 1.2446,
      "step": 225500
    },
    {
      "epoch": 2.97395812772229,
      "grad_norm": 2.4314024448394775,
      "learning_rate": 8.570006490668008e-05,
      "loss": 1.2405,
      "step": 226000
    },
    {
      "epoch": 2.9805376811022066,
      "grad_norm": 2.2734267711639404,
      "learning_rate": 8.566694925357318e-05,
      "loss": 1.2386,
      "step": 226500
    },
    {
      "epoch": 2.9871172344821235,
      "grad_norm": 2.3924615383148193,
      "learning_rate": 8.563383360046627e-05,
      "loss": 1.24,
      "step": 227000
    },
    {
      "epoch": 2.99369678786204,
      "grad_norm": 2.283802032470703,
      "learning_rate": 8.560071794735937e-05,
      "loss": 1.2477,
      "step": 227500
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.182247519493103,
      "eval_runtime": 49.7994,
      "eval_samples_per_second": 2008.055,
      "eval_steps_per_second": 15.703,
      "step": 227979
    },
    {
      "epoch": 3.0002763412419564,
      "grad_norm": 2.3742706775665283,
      "learning_rate": 8.556760229425245e-05,
      "loss": 1.2414,
      "step": 228000
    },
    {
      "epoch": 3.006855894621873,
      "grad_norm": 2.449753522872925,
      "learning_rate": 8.553448664114554e-05,
      "loss": 1.2381,
      "step": 228500
    },
    {
      "epoch": 3.0134354480017898,
      "grad_norm": 2.3953804969787598,
      "learning_rate": 8.550143721934484e-05,
      "loss": 1.2312,
      "step": 229000
    },
    {
      "epoch": 3.020015001381706,
      "grad_norm": 2.3364617824554443,
      "learning_rate": 8.546832156623793e-05,
      "loss": 1.2254,
      "step": 229500
    },
    {
      "epoch": 3.0265945547616226,
      "grad_norm": 2.451772689819336,
      "learning_rate": 8.543520591313103e-05,
      "loss": 1.2338,
      "step": 230000
    },
    {
      "epoch": 3.0331741081415395,
      "grad_norm": 2.623823404312134,
      "learning_rate": 8.540215649133032e-05,
      "loss": 1.2344,
      "step": 230500
    },
    {
      "epoch": 3.039753661521456,
      "grad_norm": 2.9754786491394043,
      "learning_rate": 8.536904083822342e-05,
      "loss": 1.2417,
      "step": 231000
    },
    {
      "epoch": 3.0463332149013724,
      "grad_norm": 2.5363271236419678,
      "learning_rate": 8.533599141642272e-05,
      "loss": 1.2325,
      "step": 231500
    },
    {
      "epoch": 3.052912768281289,
      "grad_norm": 2.4842469692230225,
      "learning_rate": 8.53028757633158e-05,
      "loss": 1.2284,
      "step": 232000
    },
    {
      "epoch": 3.0594923216612058,
      "grad_norm": 2.316167116165161,
      "learning_rate": 8.526976011020889e-05,
      "loss": 1.2333,
      "step": 232500
    },
    {
      "epoch": 3.066071875041122,
      "grad_norm": 2.545714855194092,
      "learning_rate": 8.523671068840821e-05,
      "loss": 1.2239,
      "step": 233000
    },
    {
      "epoch": 3.0726514284210387,
      "grad_norm": 2.6290321350097656,
      "learning_rate": 8.520359503530128e-05,
      "loss": 1.2292,
      "step": 233500
    },
    {
      "epoch": 3.0792309818009556,
      "grad_norm": 2.2982194423675537,
      "learning_rate": 8.517047938219438e-05,
      "loss": 1.2332,
      "step": 234000
    },
    {
      "epoch": 3.085810535180872,
      "grad_norm": 2.3809595108032227,
      "learning_rate": 8.513736372908747e-05,
      "loss": 1.2397,
      "step": 234500
    },
    {
      "epoch": 3.0923900885607885,
      "grad_norm": 2.4155828952789307,
      "learning_rate": 8.510424807598057e-05,
      "loss": 1.2284,
      "step": 235000
    },
    {
      "epoch": 3.098969641940705,
      "grad_norm": 2.403456211090088,
      "learning_rate": 8.507113242287365e-05,
      "loss": 1.2298,
      "step": 235500
    },
    {
      "epoch": 3.105549195320622,
      "grad_norm": 2.720700979232788,
      "learning_rate": 8.503801676976674e-05,
      "loss": 1.234,
      "step": 236000
    },
    {
      "epoch": 3.1121287487005382,
      "grad_norm": 2.6121647357940674,
      "learning_rate": 8.500490111665983e-05,
      "loss": 1.2305,
      "step": 236500
    },
    {
      "epoch": 3.1187083020804547,
      "grad_norm": 2.329378128051758,
      "learning_rate": 8.49717854635529e-05,
      "loss": 1.2348,
      "step": 237000
    },
    {
      "epoch": 3.125287855460371,
      "grad_norm": 2.54818058013916,
      "learning_rate": 8.4938669810446e-05,
      "loss": 1.2306,
      "step": 237500
    },
    {
      "epoch": 3.131867408840288,
      "grad_norm": 2.3453893661499023,
      "learning_rate": 8.490555415733909e-05,
      "loss": 1.223,
      "step": 238000
    },
    {
      "epoch": 3.1384469622202045,
      "grad_norm": 2.404945135116577,
      "learning_rate": 8.48725047355384e-05,
      "loss": 1.2257,
      "step": 238500
    },
    {
      "epoch": 3.145026515600121,
      "grad_norm": 2.419325351715088,
      "learning_rate": 8.483938908243148e-05,
      "loss": 1.221,
      "step": 239000
    },
    {
      "epoch": 3.151606068980038,
      "grad_norm": 2.4190547466278076,
      "learning_rate": 8.480627342932458e-05,
      "loss": 1.226,
      "step": 239500
    },
    {
      "epoch": 3.1581856223599543,
      "grad_norm": 2.6312813758850098,
      "learning_rate": 8.477315777621766e-05,
      "loss": 1.2222,
      "step": 240000
    },
    {
      "epoch": 3.1647651757398707,
      "grad_norm": 2.3267881870269775,
      "learning_rate": 8.474004212311075e-05,
      "loss": 1.2218,
      "step": 240500
    },
    {
      "epoch": 3.171344729119787,
      "grad_norm": 2.540269136428833,
      "learning_rate": 8.470692647000385e-05,
      "loss": 1.2288,
      "step": 241000
    },
    {
      "epoch": 3.177924282499704,
      "grad_norm": 2.430459976196289,
      "learning_rate": 8.467381081689693e-05,
      "loss": 1.2244,
      "step": 241500
    },
    {
      "epoch": 3.1845038358796205,
      "grad_norm": 2.6790285110473633,
      "learning_rate": 8.464069516379003e-05,
      "loss": 1.227,
      "step": 242000
    },
    {
      "epoch": 3.191083389259537,
      "grad_norm": 2.3202064037323,
      "learning_rate": 8.46075795106831e-05,
      "loss": 1.2222,
      "step": 242500
    },
    {
      "epoch": 3.197662942639454,
      "grad_norm": 2.5177204608917236,
      "learning_rate": 8.45744638575762e-05,
      "loss": 1.2132,
      "step": 243000
    },
    {
      "epoch": 3.2042424960193703,
      "grad_norm": 2.3209238052368164,
      "learning_rate": 8.454134820446929e-05,
      "loss": 1.2201,
      "step": 243500
    },
    {
      "epoch": 3.2108220493992867,
      "grad_norm": 2.6639628410339355,
      "learning_rate": 8.450823255136239e-05,
      "loss": 1.2274,
      "step": 244000
    },
    {
      "epoch": 3.217401602779203,
      "grad_norm": 5.090656757354736,
      "learning_rate": 8.447518312956168e-05,
      "loss": 1.2252,
      "step": 244500
    },
    {
      "epoch": 3.22398115615912,
      "grad_norm": 2.4853808879852295,
      "learning_rate": 8.444206747645478e-05,
      "loss": 1.2212,
      "step": 245000
    },
    {
      "epoch": 3.2305607095390365,
      "grad_norm": 2.6249215602874756,
      "learning_rate": 8.440901805465408e-05,
      "loss": 1.2224,
      "step": 245500
    },
    {
      "epoch": 3.237140262918953,
      "grad_norm": 2.1645920276641846,
      "learning_rate": 8.437590240154717e-05,
      "loss": 1.2327,
      "step": 246000
    },
    {
      "epoch": 3.24371981629887,
      "grad_norm": 2.532456874847412,
      "learning_rate": 8.434278674844027e-05,
      "loss": 1.2177,
      "step": 246500
    },
    {
      "epoch": 3.2502993696787863,
      "grad_norm": 2.3240582942962646,
      "learning_rate": 8.430967109533334e-05,
      "loss": 1.2158,
      "step": 247000
    },
    {
      "epoch": 3.2568789230587027,
      "grad_norm": 2.5990540981292725,
      "learning_rate": 8.427655544222644e-05,
      "loss": 1.2079,
      "step": 247500
    },
    {
      "epoch": 3.263458476438619,
      "grad_norm": 2.312697172164917,
      "learning_rate": 8.424343978911952e-05,
      "loss": 1.2216,
      "step": 248000
    },
    {
      "epoch": 3.270038029818536,
      "grad_norm": 2.027768611907959,
      "learning_rate": 8.421032413601262e-05,
      "loss": 1.2247,
      "step": 248500
    },
    {
      "epoch": 3.2766175831984525,
      "grad_norm": 2.6787660121917725,
      "learning_rate": 8.417727471421191e-05,
      "loss": 1.2207,
      "step": 249000
    },
    {
      "epoch": 3.283197136578369,
      "grad_norm": 2.3107993602752686,
      "learning_rate": 8.414415906110501e-05,
      "loss": 1.2103,
      "step": 249500
    },
    {
      "epoch": 3.289776689958286,
      "grad_norm": 2.234889030456543,
      "learning_rate": 8.41110434079981e-05,
      "loss": 1.2178,
      "step": 250000
    },
    {
      "epoch": 3.2963562433382023,
      "grad_norm": 2.669450044631958,
      "learning_rate": 8.407792775489118e-05,
      "loss": 1.2137,
      "step": 250500
    },
    {
      "epoch": 3.3029357967181188,
      "grad_norm": 2.428875207901001,
      "learning_rate": 8.404481210178428e-05,
      "loss": 1.2217,
      "step": 251000
    },
    {
      "epoch": 3.309515350098035,
      "grad_norm": 2.3577802181243896,
      "learning_rate": 8.401169644867737e-05,
      "loss": 1.2246,
      "step": 251500
    },
    {
      "epoch": 3.316094903477952,
      "grad_norm": 2.5248327255249023,
      "learning_rate": 8.397858079557045e-05,
      "loss": 1.2211,
      "step": 252000
    },
    {
      "epoch": 3.3226744568578686,
      "grad_norm": 2.4467995166778564,
      "learning_rate": 8.394546514246354e-05,
      "loss": 1.2161,
      "step": 252500
    },
    {
      "epoch": 3.329254010237785,
      "grad_norm": 3.061495780944824,
      "learning_rate": 8.391241572066286e-05,
      "loss": 1.2212,
      "step": 253000
    },
    {
      "epoch": 3.3358335636177014,
      "grad_norm": 2.6141347885131836,
      "learning_rate": 8.387930006755593e-05,
      "loss": 1.2181,
      "step": 253500
    },
    {
      "epoch": 3.3424131169976183,
      "grad_norm": 2.581589937210083,
      "learning_rate": 8.384618441444903e-05,
      "loss": 1.2125,
      "step": 254000
    },
    {
      "epoch": 3.348992670377535,
      "grad_norm": 2.457829236984253,
      "learning_rate": 8.381306876134211e-05,
      "loss": 1.2148,
      "step": 254500
    },
    {
      "epoch": 3.3555722237574512,
      "grad_norm": 3.014432430267334,
      "learning_rate": 8.377995310823521e-05,
      "loss": 1.2211,
      "step": 255000
    },
    {
      "epoch": 3.3621517771373677,
      "grad_norm": 2.3410773277282715,
      "learning_rate": 8.37469036864345e-05,
      "loss": 1.2143,
      "step": 255500
    },
    {
      "epoch": 3.3687313305172846,
      "grad_norm": 2.7952077388763428,
      "learning_rate": 8.37137880333276e-05,
      "loss": 1.2082,
      "step": 256000
    },
    {
      "epoch": 3.375310883897201,
      "grad_norm": 2.5039193630218506,
      "learning_rate": 8.368067238022069e-05,
      "loss": 1.2155,
      "step": 256500
    },
    {
      "epoch": 3.3818904372771175,
      "grad_norm": 2.4991977214813232,
      "learning_rate": 8.364755672711377e-05,
      "loss": 1.2134,
      "step": 257000
    },
    {
      "epoch": 3.3884699906570344,
      "grad_norm": 2.2844314575195312,
      "learning_rate": 8.361444107400687e-05,
      "loss": 1.2146,
      "step": 257500
    },
    {
      "epoch": 3.395049544036951,
      "grad_norm": 2.3899526596069336,
      "learning_rate": 8.358145788351238e-05,
      "loss": 1.2242,
      "step": 258000
    },
    {
      "epoch": 3.4016290974168673,
      "grad_norm": 2.465038537979126,
      "learning_rate": 8.354834223040548e-05,
      "loss": 1.229,
      "step": 258500
    },
    {
      "epoch": 3.4082086507967837,
      "grad_norm": 2.5574257373809814,
      "learning_rate": 8.351522657729857e-05,
      "loss": 1.2155,
      "step": 259000
    },
    {
      "epoch": 3.4147882041767006,
      "grad_norm": 2.416874885559082,
      "learning_rate": 8.348211092419165e-05,
      "loss": 1.2145,
      "step": 259500
    },
    {
      "epoch": 3.421367757556617,
      "grad_norm": 2.22422194480896,
      "learning_rate": 8.344899527108474e-05,
      "loss": 1.2097,
      "step": 260000
    },
    {
      "epoch": 3.4279473109365335,
      "grad_norm": 2.8909623622894287,
      "learning_rate": 8.341587961797783e-05,
      "loss": 1.2121,
      "step": 260500
    },
    {
      "epoch": 3.4345268643164504,
      "grad_norm": 2.3491532802581787,
      "learning_rate": 8.338276396487092e-05,
      "loss": 1.218,
      "step": 261000
    },
    {
      "epoch": 3.441106417696367,
      "grad_norm": 2.7020418643951416,
      "learning_rate": 8.3349648311764e-05,
      "loss": 1.2108,
      "step": 261500
    },
    {
      "epoch": 3.4476859710762833,
      "grad_norm": 2.5041608810424805,
      "learning_rate": 8.331666512126952e-05,
      "loss": 1.2119,
      "step": 262000
    },
    {
      "epoch": 3.4542655244561997,
      "grad_norm": 2.4656827449798584,
      "learning_rate": 8.328354946816262e-05,
      "loss": 1.2092,
      "step": 262500
    },
    {
      "epoch": 3.4608450778361166,
      "grad_norm": 2.527804136276245,
      "learning_rate": 8.32504338150557e-05,
      "loss": 1.2032,
      "step": 263000
    },
    {
      "epoch": 3.467424631216033,
      "grad_norm": 2.539989471435547,
      "learning_rate": 8.32173181619488e-05,
      "loss": 1.2042,
      "step": 263500
    },
    {
      "epoch": 3.4740041845959495,
      "grad_norm": 2.7995474338531494,
      "learning_rate": 8.318420250884188e-05,
      "loss": 1.2084,
      "step": 264000
    },
    {
      "epoch": 3.4805837379758664,
      "grad_norm": 2.281306743621826,
      "learning_rate": 8.315108685573497e-05,
      "loss": 1.208,
      "step": 264500
    },
    {
      "epoch": 3.487163291355783,
      "grad_norm": 2.591644525527954,
      "learning_rate": 8.311797120262807e-05,
      "loss": 1.2113,
      "step": 265000
    },
    {
      "epoch": 3.4937428447356993,
      "grad_norm": 2.5105793476104736,
      "learning_rate": 8.308485554952115e-05,
      "loss": 1.2128,
      "step": 265500
    },
    {
      "epoch": 3.5003223981156157,
      "grad_norm": 2.264354705810547,
      "learning_rate": 8.305173989641424e-05,
      "loss": 1.2105,
      "step": 266000
    },
    {
      "epoch": 3.5069019514955326,
      "grad_norm": 2.388298988342285,
      "learning_rate": 8.301862424330732e-05,
      "loss": 1.2143,
      "step": 266500
    },
    {
      "epoch": 3.513481504875449,
      "grad_norm": 2.0847041606903076,
      "learning_rate": 8.298550859020042e-05,
      "loss": 1.2049,
      "step": 267000
    },
    {
      "epoch": 3.5200610582553655,
      "grad_norm": 2.3262248039245605,
      "learning_rate": 8.295239293709351e-05,
      "loss": 1.2098,
      "step": 267500
    },
    {
      "epoch": 3.5266406116352824,
      "grad_norm": 2.560687303543091,
      "learning_rate": 8.291934351529281e-05,
      "loss": 1.2037,
      "step": 268000
    },
    {
      "epoch": 3.533220165015199,
      "grad_norm": 2.1923577785491943,
      "learning_rate": 8.28862278621859e-05,
      "loss": 1.2125,
      "step": 268500
    },
    {
      "epoch": 3.5397997183951153,
      "grad_norm": 2.6301488876342773,
      "learning_rate": 8.285311220907898e-05,
      "loss": 1.2074,
      "step": 269000
    },
    {
      "epoch": 3.5463792717750318,
      "grad_norm": 2.38020920753479,
      "learning_rate": 8.281999655597208e-05,
      "loss": 1.21,
      "step": 269500
    },
    {
      "epoch": 3.552958825154948,
      "grad_norm": 2.5946176052093506,
      "learning_rate": 8.278688090286517e-05,
      "loss": 1.1989,
      "step": 270000
    },
    {
      "epoch": 3.559538378534865,
      "grad_norm": 2.3871052265167236,
      "learning_rate": 8.275376524975827e-05,
      "loss": 1.2068,
      "step": 270500
    },
    {
      "epoch": 3.5661179319147815,
      "grad_norm": 2.292628526687622,
      "learning_rate": 8.272064959665134e-05,
      "loss": 1.2184,
      "step": 271000
    },
    {
      "epoch": 3.5726974852946984,
      "grad_norm": 2.379471778869629,
      "learning_rate": 8.268753394354444e-05,
      "loss": 1.2026,
      "step": 271500
    },
    {
      "epoch": 3.579277038674615,
      "grad_norm": 2.5694143772125244,
      "learning_rate": 8.265448452174374e-05,
      "loss": 1.213,
      "step": 272000
    },
    {
      "epoch": 3.5858565920545313,
      "grad_norm": 2.2886180877685547,
      "learning_rate": 8.262136886863683e-05,
      "loss": 1.2013,
      "step": 272500
    },
    {
      "epoch": 3.592436145434448,
      "grad_norm": 2.4485023021698,
      "learning_rate": 8.258825321552993e-05,
      "loss": 1.2018,
      "step": 273000
    },
    {
      "epoch": 3.5990156988143642,
      "grad_norm": 2.425260066986084,
      "learning_rate": 8.255513756242301e-05,
      "loss": 1.2005,
      "step": 273500
    },
    {
      "epoch": 3.605595252194281,
      "grad_norm": 2.371230125427246,
      "learning_rate": 8.25220219093161e-05,
      "loss": 1.2075,
      "step": 274000
    },
    {
      "epoch": 3.6121748055741976,
      "grad_norm": 2.9194254875183105,
      "learning_rate": 8.24889724875154e-05,
      "loss": 1.2086,
      "step": 274500
    },
    {
      "epoch": 3.6187543589541145,
      "grad_norm": 2.2789318561553955,
      "learning_rate": 8.24558568344085e-05,
      "loss": 1.1994,
      "step": 275000
    },
    {
      "epoch": 3.625333912334031,
      "grad_norm": 2.322998046875,
      "learning_rate": 8.242274118130157e-05,
      "loss": 1.2022,
      "step": 275500
    },
    {
      "epoch": 3.6319134657139474,
      "grad_norm": 2.412785053253174,
      "learning_rate": 8.238969175950089e-05,
      "loss": 1.1973,
      "step": 276000
    },
    {
      "epoch": 3.638493019093864,
      "grad_norm": 2.2022902965545654,
      "learning_rate": 8.235657610639398e-05,
      "loss": 1.2031,
      "step": 276500
    },
    {
      "epoch": 3.6450725724737802,
      "grad_norm": 2.585684299468994,
      "learning_rate": 8.232346045328706e-05,
      "loss": 1.1952,
      "step": 277000
    },
    {
      "epoch": 3.651652125853697,
      "grad_norm": 2.411311388015747,
      "learning_rate": 8.229034480018015e-05,
      "loss": 1.2083,
      "step": 277500
    },
    {
      "epoch": 3.6582316792336136,
      "grad_norm": 2.386382579803467,
      "learning_rate": 8.225722914707325e-05,
      "loss": 1.1992,
      "step": 278000
    },
    {
      "epoch": 3.66481123261353,
      "grad_norm": 2.320072650909424,
      "learning_rate": 8.222411349396633e-05,
      "loss": 1.1975,
      "step": 278500
    },
    {
      "epoch": 3.671390785993447,
      "grad_norm": 2.480111598968506,
      "learning_rate": 8.219099784085942e-05,
      "loss": 1.2002,
      "step": 279000
    },
    {
      "epoch": 3.6779703393733634,
      "grad_norm": 2.214592933654785,
      "learning_rate": 8.215788218775252e-05,
      "loss": 1.1916,
      "step": 279500
    },
    {
      "epoch": 3.68454989275328,
      "grad_norm": 2.2579143047332764,
      "learning_rate": 8.21247665346456e-05,
      "loss": 1.1977,
      "step": 280000
    },
    {
      "epoch": 3.6911294461331963,
      "grad_norm": 2.2399911880493164,
      "learning_rate": 8.209165088153869e-05,
      "loss": 1.1943,
      "step": 280500
    },
    {
      "epoch": 3.697708999513113,
      "grad_norm": 2.624103546142578,
      "learning_rate": 8.205853522843177e-05,
      "loss": 1.1924,
      "step": 281000
    },
    {
      "epoch": 3.7042885528930296,
      "grad_norm": 2.2250113487243652,
      "learning_rate": 8.202548580663109e-05,
      "loss": 1.1995,
      "step": 281500
    },
    {
      "epoch": 3.710868106272946,
      "grad_norm": 2.2924795150756836,
      "learning_rate": 8.199237015352416e-05,
      "loss": 1.1927,
      "step": 282000
    },
    {
      "epoch": 3.717447659652863,
      "grad_norm": 2.4318103790283203,
      "learning_rate": 8.195925450041726e-05,
      "loss": 1.1956,
      "step": 282500
    },
    {
      "epoch": 3.7240272130327794,
      "grad_norm": 2.467191219329834,
      "learning_rate": 8.192613884731035e-05,
      "loss": 1.1982,
      "step": 283000
    },
    {
      "epoch": 3.730606766412696,
      "grad_norm": 2.3096370697021484,
      "learning_rate": 8.189302319420344e-05,
      "loss": 1.2038,
      "step": 283500
    },
    {
      "epoch": 3.7371863197926123,
      "grad_norm": 2.2882919311523438,
      "learning_rate": 8.185997377240274e-05,
      "loss": 1.1951,
      "step": 284000
    },
    {
      "epoch": 3.743765873172529,
      "grad_norm": 2.4755373001098633,
      "learning_rate": 8.182692435060204e-05,
      "loss": 1.2023,
      "step": 284500
    },
    {
      "epoch": 3.7503454265524456,
      "grad_norm": 2.4108283519744873,
      "learning_rate": 8.179380869749514e-05,
      "loss": 1.194,
      "step": 285000
    },
    {
      "epoch": 3.756924979932362,
      "grad_norm": 2.6548471450805664,
      "learning_rate": 8.176069304438823e-05,
      "loss": 1.2015,
      "step": 285500
    },
    {
      "epoch": 3.763504533312279,
      "grad_norm": 2.4423532485961914,
      "learning_rate": 8.172757739128132e-05,
      "loss": 1.2008,
      "step": 286000
    },
    {
      "epoch": 3.7700840866921954,
      "grad_norm": 2.4098737239837646,
      "learning_rate": 8.16944617381744e-05,
      "loss": 1.1995,
      "step": 286500
    },
    {
      "epoch": 3.776663640072112,
      "grad_norm": 2.4816067218780518,
      "learning_rate": 8.16613460850675e-05,
      "loss": 1.1999,
      "step": 287000
    },
    {
      "epoch": 3.7832431934520283,
      "grad_norm": 2.144608736038208,
      "learning_rate": 8.162823043196058e-05,
      "loss": 1.2114,
      "step": 287500
    },
    {
      "epoch": 3.789822746831945,
      "grad_norm": 2.31813383102417,
      "learning_rate": 8.159511477885368e-05,
      "loss": 1.193,
      "step": 288000
    },
    {
      "epoch": 3.7964023002118616,
      "grad_norm": 2.3753108978271484,
      "learning_rate": 8.156206535705297e-05,
      "loss": 1.1973,
      "step": 288500
    },
    {
      "epoch": 3.802981853591778,
      "grad_norm": 2.5623178482055664,
      "learning_rate": 8.152894970394607e-05,
      "loss": 1.1897,
      "step": 289000
    },
    {
      "epoch": 3.809561406971695,
      "grad_norm": 2.2152416706085205,
      "learning_rate": 8.149583405083915e-05,
      "loss": 1.1963,
      "step": 289500
    },
    {
      "epoch": 3.8161409603516114,
      "grad_norm": 2.8247406482696533,
      "learning_rate": 8.146271839773224e-05,
      "loss": 1.193,
      "step": 290000
    },
    {
      "epoch": 3.822720513731528,
      "grad_norm": 2.4626312255859375,
      "learning_rate": 8.142966897593154e-05,
      "loss": 1.1948,
      "step": 290500
    },
    {
      "epoch": 3.8293000671114443,
      "grad_norm": 2.404778242111206,
      "learning_rate": 8.139655332282463e-05,
      "loss": 1.1994,
      "step": 291000
    },
    {
      "epoch": 3.8358796204913608,
      "grad_norm": 2.276329517364502,
      "learning_rate": 8.136343766971773e-05,
      "loss": 1.1974,
      "step": 291500
    },
    {
      "epoch": 3.8424591738712777,
      "grad_norm": 2.12136173248291,
      "learning_rate": 8.133032201661081e-05,
      "loss": 1.1931,
      "step": 292000
    },
    {
      "epoch": 3.849038727251194,
      "grad_norm": 2.3764119148254395,
      "learning_rate": 8.129720636350391e-05,
      "loss": 1.1982,
      "step": 292500
    },
    {
      "epoch": 3.855618280631111,
      "grad_norm": 2.6676480770111084,
      "learning_rate": 8.126409071039698e-05,
      "loss": 1.1977,
      "step": 293000
    },
    {
      "epoch": 3.8621978340110275,
      "grad_norm": 2.5244405269622803,
      "learning_rate": 8.123097505729008e-05,
      "loss": 1.1892,
      "step": 293500
    },
    {
      "epoch": 3.868777387390944,
      "grad_norm": 2.238534450531006,
      "learning_rate": 8.11979918667956e-05,
      "loss": 1.1886,
      "step": 294000
    },
    {
      "epoch": 3.8753569407708603,
      "grad_norm": 2.3235809803009033,
      "learning_rate": 8.116487621368869e-05,
      "loss": 1.1962,
      "step": 294500
    },
    {
      "epoch": 3.881936494150777,
      "grad_norm": 2.485750913619995,
      "learning_rate": 8.113182679188798e-05,
      "loss": 1.1927,
      "step": 295000
    },
    {
      "epoch": 3.8885160475306937,
      "grad_norm": 2.5291354656219482,
      "learning_rate": 8.109871113878108e-05,
      "loss": 1.1989,
      "step": 295500
    },
    {
      "epoch": 3.89509560091061,
      "grad_norm": 2.2003393173217773,
      "learning_rate": 8.106559548567417e-05,
      "loss": 1.1833,
      "step": 296000
    },
    {
      "epoch": 3.901675154290527,
      "grad_norm": 2.4012579917907715,
      "learning_rate": 8.103247983256727e-05,
      "loss": 1.1872,
      "step": 296500
    },
    {
      "epoch": 3.9082547076704435,
      "grad_norm": 2.4072556495666504,
      "learning_rate": 8.099949664207278e-05,
      "loss": 1.1877,
      "step": 297000
    },
    {
      "epoch": 3.91483426105036,
      "grad_norm": 2.439957618713379,
      "learning_rate": 8.096638098896588e-05,
      "loss": 1.1952,
      "step": 297500
    },
    {
      "epoch": 3.9214138144302764,
      "grad_norm": 2.240027666091919,
      "learning_rate": 8.093326533585895e-05,
      "loss": 1.1881,
      "step": 298000
    },
    {
      "epoch": 3.927993367810193,
      "grad_norm": 2.5832433700561523,
      "learning_rate": 8.090014968275205e-05,
      "loss": 1.1874,
      "step": 298500
    },
    {
      "epoch": 3.9345729211901097,
      "grad_norm": 2.325484037399292,
      "learning_rate": 8.086703402964513e-05,
      "loss": 1.1819,
      "step": 299000
    },
    {
      "epoch": 3.941152474570026,
      "grad_norm": 2.3594062328338623,
      "learning_rate": 8.083391837653823e-05,
      "loss": 1.1888,
      "step": 299500
    },
    {
      "epoch": 3.9477320279499426,
      "grad_norm": 2.6077771186828613,
      "learning_rate": 8.080080272343132e-05,
      "loss": 1.1827,
      "step": 300000
    },
    {
      "epoch": 3.9543115813298595,
      "grad_norm": 2.5689258575439453,
      "learning_rate": 8.07676870703244e-05,
      "loss": 1.1847,
      "step": 300500
    },
    {
      "epoch": 3.960891134709776,
      "grad_norm": 2.5444767475128174,
      "learning_rate": 8.07345714172175e-05,
      "loss": 1.1855,
      "step": 301000
    },
    {
      "epoch": 3.9674706880896924,
      "grad_norm": 2.4563231468200684,
      "learning_rate": 8.070145576411057e-05,
      "loss": 1.1757,
      "step": 301500
    },
    {
      "epoch": 3.974050241469609,
      "grad_norm": 2.902449131011963,
      "learning_rate": 8.066834011100367e-05,
      "loss": 1.1848,
      "step": 302000
    },
    {
      "epoch": 3.9806297948495257,
      "grad_norm": 2.378872871398926,
      "learning_rate": 8.063522445789676e-05,
      "loss": 1.1884,
      "step": 302500
    },
    {
      "epoch": 3.987209348229442,
      "grad_norm": 2.431849241256714,
      "learning_rate": 8.060210880478986e-05,
      "loss": 1.1791,
      "step": 303000
    },
    {
      "epoch": 3.9937889016093586,
      "grad_norm": 2.4756691455841064,
      "learning_rate": 8.056899315168294e-05,
      "loss": 1.1799,
      "step": 303500
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.1265121698379517,
      "eval_runtime": 49.5306,
      "eval_samples_per_second": 2018.955,
      "eval_steps_per_second": 15.788,
      "step": 303972
    },
    {
      "epoch": 4.0003684549892755,
      "grad_norm": 2.562776803970337,
      "learning_rate": 8.053587749857603e-05,
      "loss": 1.1874,
      "step": 304000
    },
    {
      "epoch": 4.006948008369192,
      "grad_norm": 2.405233383178711,
      "learning_rate": 8.050276184546913e-05,
      "loss": 1.1773,
      "step": 304500
    },
    {
      "epoch": 4.013527561749108,
      "grad_norm": 2.424314498901367,
      "learning_rate": 8.046971242366842e-05,
      "loss": 1.1743,
      "step": 305000
    },
    {
      "epoch": 4.020107115129025,
      "grad_norm": 2.4356303215026855,
      "learning_rate": 8.043659677056152e-05,
      "loss": 1.1772,
      "step": 305500
    },
    {
      "epoch": 4.026686668508941,
      "grad_norm": 2.3696489334106445,
      "learning_rate": 8.04034811174546e-05,
      "loss": 1.1762,
      "step": 306000
    },
    {
      "epoch": 4.033266221888858,
      "grad_norm": 2.347644329071045,
      "learning_rate": 8.03703654643477e-05,
      "loss": 1.1781,
      "step": 306500
    },
    {
      "epoch": 4.039845775268775,
      "grad_norm": 2.496910572052002,
      "learning_rate": 8.033724981124077e-05,
      "loss": 1.178,
      "step": 307000
    },
    {
      "epoch": 4.0464253286486915,
      "grad_norm": 2.5871057510375977,
      "learning_rate": 8.030413415813387e-05,
      "loss": 1.1809,
      "step": 307500
    },
    {
      "epoch": 4.053004882028608,
      "grad_norm": 2.7480053901672363,
      "learning_rate": 8.027101850502696e-05,
      "loss": 1.1775,
      "step": 308000
    },
    {
      "epoch": 4.059584435408524,
      "grad_norm": 2.3386242389678955,
      "learning_rate": 8.023796908322626e-05,
      "loss": 1.1872,
      "step": 308500
    },
    {
      "epoch": 4.066163988788441,
      "grad_norm": 2.3449814319610596,
      "learning_rate": 8.020485343011936e-05,
      "loss": 1.1749,
      "step": 309000
    },
    {
      "epoch": 4.072743542168357,
      "grad_norm": 2.3893465995788574,
      "learning_rate": 8.017173777701244e-05,
      "loss": 1.1687,
      "step": 309500
    },
    {
      "epoch": 4.079323095548274,
      "grad_norm": 2.3559954166412354,
      "learning_rate": 8.013862212390553e-05,
      "loss": 1.1752,
      "step": 310000
    },
    {
      "epoch": 4.085902648928191,
      "grad_norm": 2.318516254425049,
      "learning_rate": 8.010550647079862e-05,
      "loss": 1.1683,
      "step": 310500
    },
    {
      "epoch": 4.0924822023081076,
      "grad_norm": 2.654291868209839,
      "learning_rate": 8.007239081769171e-05,
      "loss": 1.1795,
      "step": 311000
    },
    {
      "epoch": 4.099061755688024,
      "grad_norm": 2.2922372817993164,
      "learning_rate": 8.0039341395891e-05,
      "loss": 1.1705,
      "step": 311500
    },
    {
      "epoch": 4.1056413090679404,
      "grad_norm": 2.364032030105591,
      "learning_rate": 8.00062257427841e-05,
      "loss": 1.1749,
      "step": 312000
    },
    {
      "epoch": 4.112220862447857,
      "grad_norm": 2.4406864643096924,
      "learning_rate": 7.997311008967719e-05,
      "loss": 1.1816,
      "step": 312500
    },
    {
      "epoch": 4.118800415827773,
      "grad_norm": 2.3085551261901855,
      "learning_rate": 7.993999443657029e-05,
      "loss": 1.1752,
      "step": 313000
    },
    {
      "epoch": 4.12537996920769,
      "grad_norm": 2.859773635864258,
      "learning_rate": 7.990687878346337e-05,
      "loss": 1.1766,
      "step": 313500
    },
    {
      "epoch": 4.131959522587607,
      "grad_norm": 2.383622407913208,
      "learning_rate": 7.987376313035646e-05,
      "loss": 1.1823,
      "step": 314000
    },
    {
      "epoch": 4.138539075967524,
      "grad_norm": 2.5634286403656006,
      "learning_rate": 7.984064747724956e-05,
      "loss": 1.1743,
      "step": 314500
    },
    {
      "epoch": 4.14511862934744,
      "grad_norm": 2.2166714668273926,
      "learning_rate": 7.980753182414264e-05,
      "loss": 1.166,
      "step": 315000
    },
    {
      "epoch": 4.1516981827273565,
      "grad_norm": 2.4063637256622314,
      "learning_rate": 7.977441617103573e-05,
      "loss": 1.1712,
      "step": 315500
    },
    {
      "epoch": 4.158277736107273,
      "grad_norm": 2.3581361770629883,
      "learning_rate": 7.974130051792881e-05,
      "loss": 1.1706,
      "step": 316000
    },
    {
      "epoch": 4.164857289487189,
      "grad_norm": 2.4012577533721924,
      "learning_rate": 7.970825109612812e-05,
      "loss": 1.1711,
      "step": 316500
    },
    {
      "epoch": 4.171436842867106,
      "grad_norm": 2.3343889713287354,
      "learning_rate": 7.96751354430212e-05,
      "loss": 1.1742,
      "step": 317000
    },
    {
      "epoch": 4.178016396247023,
      "grad_norm": 2.2167980670928955,
      "learning_rate": 7.96420197899143e-05,
      "loss": 1.1794,
      "step": 317500
    },
    {
      "epoch": 4.18459594962694,
      "grad_norm": 2.3222687244415283,
      "learning_rate": 7.960890413680739e-05,
      "loss": 1.1667,
      "step": 318000
    },
    {
      "epoch": 4.191175503006856,
      "grad_norm": 2.511897563934326,
      "learning_rate": 7.957578848370047e-05,
      "loss": 1.1704,
      "step": 318500
    },
    {
      "epoch": 4.1977550563867725,
      "grad_norm": 2.3208396434783936,
      "learning_rate": 7.954267283059357e-05,
      "loss": 1.1664,
      "step": 319000
    },
    {
      "epoch": 4.204334609766689,
      "grad_norm": 2.368762254714966,
      "learning_rate": 7.950955717748666e-05,
      "loss": 1.1755,
      "step": 319500
    },
    {
      "epoch": 4.210914163146605,
      "grad_norm": 2.5871846675872803,
      "learning_rate": 7.947644152437976e-05,
      "loss": 1.1702,
      "step": 320000
    },
    {
      "epoch": 4.217493716526522,
      "grad_norm": 2.3679428100585938,
      "learning_rate": 7.944332587127283e-05,
      "loss": 1.1774,
      "step": 320500
    },
    {
      "epoch": 4.224073269906439,
      "grad_norm": 2.224456548690796,
      "learning_rate": 7.941021021816593e-05,
      "loss": 1.1771,
      "step": 321000
    },
    {
      "epoch": 4.230652823286356,
      "grad_norm": 2.4018735885620117,
      "learning_rate": 7.937716079636522e-05,
      "loss": 1.1656,
      "step": 321500
    },
    {
      "epoch": 4.237232376666272,
      "grad_norm": 2.6412768363952637,
      "learning_rate": 7.934411137456454e-05,
      "loss": 1.1713,
      "step": 322000
    },
    {
      "epoch": 4.2438119300461885,
      "grad_norm": 2.8690853118896484,
      "learning_rate": 7.931099572145762e-05,
      "loss": 1.1696,
      "step": 322500
    },
    {
      "epoch": 4.250391483426105,
      "grad_norm": 2.2740418910980225,
      "learning_rate": 7.927788006835071e-05,
      "loss": 1.1665,
      "step": 323000
    },
    {
      "epoch": 4.256971036806021,
      "grad_norm": 2.7466273307800293,
      "learning_rate": 7.924476441524379e-05,
      "loss": 1.1621,
      "step": 323500
    },
    {
      "epoch": 4.263550590185938,
      "grad_norm": 2.454329013824463,
      "learning_rate": 7.921164876213689e-05,
      "loss": 1.1702,
      "step": 324000
    },
    {
      "epoch": 4.270130143565854,
      "grad_norm": 2.369509696960449,
      "learning_rate": 7.91785993403362e-05,
      "loss": 1.1743,
      "step": 324500
    },
    {
      "epoch": 4.276709696945772,
      "grad_norm": 2.2599847316741943,
      "learning_rate": 7.914548368722928e-05,
      "loss": 1.1679,
      "step": 325000
    },
    {
      "epoch": 4.283289250325688,
      "grad_norm": 2.3831448554992676,
      "learning_rate": 7.911236803412238e-05,
      "loss": 1.1649,
      "step": 325500
    },
    {
      "epoch": 4.2898688037056045,
      "grad_norm": 2.3481438159942627,
      "learning_rate": 7.907925238101547e-05,
      "loss": 1.1728,
      "step": 326000
    },
    {
      "epoch": 4.296448357085521,
      "grad_norm": 10.510360717773438,
      "learning_rate": 7.904613672790855e-05,
      "loss": 1.1637,
      "step": 326500
    },
    {
      "epoch": 4.303027910465437,
      "grad_norm": 2.242670774459839,
      "learning_rate": 7.901302107480164e-05,
      "loss": 1.162,
      "step": 327000
    },
    {
      "epoch": 4.309607463845354,
      "grad_norm": 2.558157205581665,
      "learning_rate": 7.897990542169474e-05,
      "loss": 1.1724,
      "step": 327500
    },
    {
      "epoch": 4.316187017225271,
      "grad_norm": 2.5078272819519043,
      "learning_rate": 7.894678976858782e-05,
      "loss": 1.1714,
      "step": 328000
    },
    {
      "epoch": 4.322766570605188,
      "grad_norm": 2.3764703273773193,
      "learning_rate": 7.89136741154809e-05,
      "loss": 1.1574,
      "step": 328500
    },
    {
      "epoch": 4.329346123985104,
      "grad_norm": 2.4530715942382812,
      "learning_rate": 7.8880558462374e-05,
      "loss": 1.1646,
      "step": 329000
    },
    {
      "epoch": 4.3359256773650205,
      "grad_norm": 2.609919309616089,
      "learning_rate": 7.88475090405733e-05,
      "loss": 1.1595,
      "step": 329500
    },
    {
      "epoch": 4.342505230744937,
      "grad_norm": 2.527956962585449,
      "learning_rate": 7.88144596187726e-05,
      "loss": 1.1702,
      "step": 330000
    },
    {
      "epoch": 4.349084784124853,
      "grad_norm": 2.2943837642669678,
      "learning_rate": 7.87813439656657e-05,
      "loss": 1.1587,
      "step": 330500
    },
    {
      "epoch": 4.35566433750477,
      "grad_norm": 2.4824116230010986,
      "learning_rate": 7.874822831255879e-05,
      "loss": 1.1681,
      "step": 331000
    },
    {
      "epoch": 4.362243890884686,
      "grad_norm": 2.353828191757202,
      "learning_rate": 7.871511265945187e-05,
      "loss": 1.1695,
      "step": 331500
    },
    {
      "epoch": 4.368823444264604,
      "grad_norm": 2.2528090476989746,
      "learning_rate": 7.868199700634497e-05,
      "loss": 1.1667,
      "step": 332000
    },
    {
      "epoch": 4.37540299764452,
      "grad_norm": 2.382769823074341,
      "learning_rate": 7.864894758454426e-05,
      "loss": 1.1725,
      "step": 332500
    },
    {
      "epoch": 4.381982551024437,
      "grad_norm": 2.7050812244415283,
      "learning_rate": 7.861583193143736e-05,
      "loss": 1.1649,
      "step": 333000
    },
    {
      "epoch": 4.388562104404353,
      "grad_norm": 2.3551840782165527,
      "learning_rate": 7.858271627833045e-05,
      "loss": 1.163,
      "step": 333500
    },
    {
      "epoch": 4.3951416577842695,
      "grad_norm": 2.5145230293273926,
      "learning_rate": 7.854960062522353e-05,
      "loss": 1.1652,
      "step": 334000
    },
    {
      "epoch": 4.401721211164186,
      "grad_norm": 2.5143544673919678,
      "learning_rate": 7.851648497211662e-05,
      "loss": 1.1656,
      "step": 334500
    },
    {
      "epoch": 4.408300764544102,
      "grad_norm": 2.6212499141693115,
      "learning_rate": 7.848343555031593e-05,
      "loss": 1.166,
      "step": 335000
    },
    {
      "epoch": 4.41488031792402,
      "grad_norm": 2.636125087738037,
      "learning_rate": 7.845038612851523e-05,
      "loss": 1.1605,
      "step": 335500
    },
    {
      "epoch": 4.421459871303936,
      "grad_norm": 2.4039530754089355,
      "learning_rate": 7.841733670671453e-05,
      "loss": 1.1664,
      "step": 336000
    },
    {
      "epoch": 4.428039424683853,
      "grad_norm": 2.382403612136841,
      "learning_rate": 7.838422105360762e-05,
      "loss": 1.1684,
      "step": 336500
    },
    {
      "epoch": 4.434618978063769,
      "grad_norm": 2.3169827461242676,
      "learning_rate": 7.835110540050071e-05,
      "loss": 1.1589,
      "step": 337000
    },
    {
      "epoch": 4.4411985314436855,
      "grad_norm": 2.4434304237365723,
      "learning_rate": 7.83179897473938e-05,
      "loss": 1.1564,
      "step": 337500
    },
    {
      "epoch": 4.447778084823602,
      "grad_norm": 2.436652421951294,
      "learning_rate": 7.828487409428689e-05,
      "loss": 1.1638,
      "step": 338000
    },
    {
      "epoch": 4.454357638203518,
      "grad_norm": 2.640244245529175,
      "learning_rate": 7.825175844117998e-05,
      "loss": 1.1689,
      "step": 338500
    },
    {
      "epoch": 4.460937191583436,
      "grad_norm": 3.0943877696990967,
      "learning_rate": 7.821864278807307e-05,
      "loss": 1.1717,
      "step": 339000
    },
    {
      "epoch": 4.467516744963352,
      "grad_norm": 2.362135648727417,
      "learning_rate": 7.818552713496617e-05,
      "loss": 1.1617,
      "step": 339500
    },
    {
      "epoch": 4.474096298343269,
      "grad_norm": 2.4063570499420166,
      "learning_rate": 7.815247771316546e-05,
      "loss": 1.1518,
      "step": 340000
    },
    {
      "epoch": 4.480675851723185,
      "grad_norm": 2.3943772315979004,
      "learning_rate": 7.811936206005856e-05,
      "loss": 1.1594,
      "step": 340500
    },
    {
      "epoch": 4.4872554051031015,
      "grad_norm": 2.9817521572113037,
      "learning_rate": 7.808624640695164e-05,
      "loss": 1.1641,
      "step": 341000
    },
    {
      "epoch": 4.493834958483018,
      "grad_norm": 2.3645451068878174,
      "learning_rate": 7.805313075384473e-05,
      "loss": 1.1617,
      "step": 341500
    },
    {
      "epoch": 4.500414511862934,
      "grad_norm": 2.746504306793213,
      "learning_rate": 7.802001510073781e-05,
      "loss": 1.1714,
      "step": 342000
    },
    {
      "epoch": 4.506994065242852,
      "grad_norm": 2.517317295074463,
      "learning_rate": 7.798689944763091e-05,
      "loss": 1.169,
      "step": 342500
    },
    {
      "epoch": 4.513573618622768,
      "grad_norm": 2.462679147720337,
      "learning_rate": 7.7953783794524e-05,
      "loss": 1.1637,
      "step": 343000
    },
    {
      "epoch": 4.520153172002685,
      "grad_norm": 2.7375524044036865,
      "learning_rate": 7.792066814141708e-05,
      "loss": 1.1574,
      "step": 343500
    },
    {
      "epoch": 4.526732725382601,
      "grad_norm": 2.596801519393921,
      "learning_rate": 7.788755248831018e-05,
      "loss": 1.1616,
      "step": 344000
    },
    {
      "epoch": 4.5333122787625175,
      "grad_norm": 2.1644234657287598,
      "learning_rate": 7.785450306650947e-05,
      "loss": 1.1634,
      "step": 344500
    },
    {
      "epoch": 4.539891832142434,
      "grad_norm": 2.3578009605407715,
      "learning_rate": 7.782138741340257e-05,
      "loss": 1.1639,
      "step": 345000
    },
    {
      "epoch": 4.54647138552235,
      "grad_norm": 2.4385814666748047,
      "learning_rate": 7.778827176029566e-05,
      "loss": 1.1484,
      "step": 345500
    },
    {
      "epoch": 4.553050938902267,
      "grad_norm": 2.2542974948883057,
      "learning_rate": 7.775515610718876e-05,
      "loss": 1.1605,
      "step": 346000
    },
    {
      "epoch": 4.559630492282184,
      "grad_norm": 2.4314639568328857,
      "learning_rate": 7.772204045408183e-05,
      "loss": 1.1529,
      "step": 346500
    },
    {
      "epoch": 4.566210045662101,
      "grad_norm": 4.71856164932251,
      "learning_rate": 7.768892480097493e-05,
      "loss": 1.1633,
      "step": 347000
    },
    {
      "epoch": 4.572789599042017,
      "grad_norm": 2.309116840362549,
      "learning_rate": 7.765580914786801e-05,
      "loss": 1.1726,
      "step": 347500
    },
    {
      "epoch": 4.5793691524219335,
      "grad_norm": 2.5582869052886963,
      "learning_rate": 7.762269349476111e-05,
      "loss": 1.1456,
      "step": 348000
    },
    {
      "epoch": 4.58594870580185,
      "grad_norm": 2.224832534790039,
      "learning_rate": 7.75895778416542e-05,
      "loss": 1.1614,
      "step": 348500
    },
    {
      "epoch": 4.592528259181766,
      "grad_norm": 2.610032796859741,
      "learning_rate": 7.75565284198535e-05,
      "loss": 1.1633,
      "step": 349000
    },
    {
      "epoch": 4.599107812561684,
      "grad_norm": 2.302778720855713,
      "learning_rate": 7.75234127667466e-05,
      "loss": 1.1522,
      "step": 349500
    },
    {
      "epoch": 4.6056873659416,
      "grad_norm": 2.219428062438965,
      "learning_rate": 7.749029711363967e-05,
      "loss": 1.1503,
      "step": 350000
    },
    {
      "epoch": 4.612266919321517,
      "grad_norm": 2.6157732009887695,
      "learning_rate": 7.745718146053277e-05,
      "loss": 1.1545,
      "step": 350500
    },
    {
      "epoch": 4.618846472701433,
      "grad_norm": 2.5879452228546143,
      "learning_rate": 7.742406580742586e-05,
      "loss": 1.1585,
      "step": 351000
    },
    {
      "epoch": 4.62542602608135,
      "grad_norm": 2.50014591217041,
      "learning_rate": 7.739095015431894e-05,
      "loss": 1.1629,
      "step": 351500
    },
    {
      "epoch": 4.632005579461266,
      "grad_norm": 2.6435887813568115,
      "learning_rate": 7.735783450121204e-05,
      "loss": 1.1527,
      "step": 352000
    },
    {
      "epoch": 4.6385851328411825,
      "grad_norm": 2.5283265113830566,
      "learning_rate": 7.732471884810513e-05,
      "loss": 1.1509,
      "step": 352500
    },
    {
      "epoch": 4.645164686221099,
      "grad_norm": 2.0337929725646973,
      "learning_rate": 7.729160319499822e-05,
      "loss": 1.1557,
      "step": 353000
    },
    {
      "epoch": 4.651744239601016,
      "grad_norm": 2.4232330322265625,
      "learning_rate": 7.725855377319752e-05,
      "loss": 1.1509,
      "step": 353500
    },
    {
      "epoch": 4.658323792980933,
      "grad_norm": 2.5143604278564453,
      "learning_rate": 7.722543812009062e-05,
      "loss": 1.1572,
      "step": 354000
    },
    {
      "epoch": 4.664903346360849,
      "grad_norm": 2.4410948753356934,
      "learning_rate": 7.71923224669837e-05,
      "loss": 1.1538,
      "step": 354500
    },
    {
      "epoch": 4.671482899740766,
      "grad_norm": 2.382622480392456,
      "learning_rate": 7.7159273045183e-05,
      "loss": 1.1626,
      "step": 355000
    },
    {
      "epoch": 4.678062453120682,
      "grad_norm": 2.334421396255493,
      "learning_rate": 7.712615739207609e-05,
      "loss": 1.153,
      "step": 355500
    },
    {
      "epoch": 4.6846420065005985,
      "grad_norm": 2.6257834434509277,
      "learning_rate": 7.709304173896919e-05,
      "loss": 1.1503,
      "step": 356000
    },
    {
      "epoch": 4.691221559880515,
      "grad_norm": 2.5762500762939453,
      "learning_rate": 7.705992608586226e-05,
      "loss": 1.1547,
      "step": 356500
    },
    {
      "epoch": 4.697801113260432,
      "grad_norm": 2.475600004196167,
      "learning_rate": 7.702681043275536e-05,
      "loss": 1.1502,
      "step": 357000
    },
    {
      "epoch": 4.704380666640349,
      "grad_norm": 2.5387635231018066,
      "learning_rate": 7.699369477964845e-05,
      "loss": 1.1443,
      "step": 357500
    },
    {
      "epoch": 4.710960220020265,
      "grad_norm": 2.6443521976470947,
      "learning_rate": 7.696057912654153e-05,
      "loss": 1.1521,
      "step": 358000
    },
    {
      "epoch": 4.717539773400182,
      "grad_norm": 2.470890760421753,
      "learning_rate": 7.692746347343463e-05,
      "loss": 1.1602,
      "step": 358500
    },
    {
      "epoch": 4.724119326780098,
      "grad_norm": 2.3856937885284424,
      "learning_rate": 7.689434782032771e-05,
      "loss": 1.1465,
      "step": 359000
    },
    {
      "epoch": 4.7306988801600145,
      "grad_norm": 2.68673038482666,
      "learning_rate": 7.686143086113945e-05,
      "loss": 1.1523,
      "step": 359500
    },
    {
      "epoch": 4.737278433539931,
      "grad_norm": 2.401737689971924,
      "learning_rate": 7.682831520803254e-05,
      "loss": 1.1468,
      "step": 360000
    },
    {
      "epoch": 4.743857986919847,
      "grad_norm": 2.3861732482910156,
      "learning_rate": 7.679519955492563e-05,
      "loss": 1.1423,
      "step": 360500
    },
    {
      "epoch": 4.750437540299765,
      "grad_norm": 2.5515754222869873,
      "learning_rate": 7.676215013312493e-05,
      "loss": 1.1426,
      "step": 361000
    },
    {
      "epoch": 4.757017093679681,
      "grad_norm": 2.355903387069702,
      "learning_rate": 7.672903448001802e-05,
      "loss": 1.1478,
      "step": 361500
    },
    {
      "epoch": 4.763596647059598,
      "grad_norm": 2.426964521408081,
      "learning_rate": 7.66959188269111e-05,
      "loss": 1.153,
      "step": 362000
    },
    {
      "epoch": 4.770176200439514,
      "grad_norm": 2.4509127140045166,
      "learning_rate": 7.66628031738042e-05,
      "loss": 1.1595,
      "step": 362500
    },
    {
      "epoch": 4.7767557538194305,
      "grad_norm": 2.7454993724823,
      "learning_rate": 7.662968752069729e-05,
      "loss": 1.1497,
      "step": 363000
    },
    {
      "epoch": 4.783335307199347,
      "grad_norm": 2.535116195678711,
      "learning_rate": 7.659657186759037e-05,
      "loss": 1.156,
      "step": 363500
    },
    {
      "epoch": 4.789914860579264,
      "grad_norm": 1.968102216720581,
      "learning_rate": 7.656345621448346e-05,
      "loss": 1.1519,
      "step": 364000
    },
    {
      "epoch": 4.796494413959181,
      "grad_norm": 2.43784761428833,
      "learning_rate": 7.653034056137656e-05,
      "loss": 1.1398,
      "step": 364500
    },
    {
      "epoch": 4.803073967339097,
      "grad_norm": 2.537285566329956,
      "learning_rate": 7.649722490826964e-05,
      "loss": 1.1439,
      "step": 365000
    },
    {
      "epoch": 4.809653520719014,
      "grad_norm": 2.4327850341796875,
      "learning_rate": 7.646410925516273e-05,
      "loss": 1.1458,
      "step": 365500
    },
    {
      "epoch": 4.81623307409893,
      "grad_norm": 2.534571886062622,
      "learning_rate": 7.643099360205583e-05,
      "loss": 1.1493,
      "step": 366000
    },
    {
      "epoch": 4.8228126274788465,
      "grad_norm": 2.554833173751831,
      "learning_rate": 7.639787794894891e-05,
      "loss": 1.1533,
      "step": 366500
    },
    {
      "epoch": 4.829392180858763,
      "grad_norm": 2.4664499759674072,
      "learning_rate": 7.636476229584201e-05,
      "loss": 1.1405,
      "step": 367000
    },
    {
      "epoch": 4.835971734238679,
      "grad_norm": 2.4986445903778076,
      "learning_rate": 7.633164664273508e-05,
      "loss": 1.1392,
      "step": 367500
    },
    {
      "epoch": 4.842551287618597,
      "grad_norm": 2.282360553741455,
      "learning_rate": 7.629853098962818e-05,
      "loss": 1.1481,
      "step": 368000
    },
    {
      "epoch": 4.849130840998513,
      "grad_norm": 2.2695279121398926,
      "learning_rate": 7.626541533652127e-05,
      "loss": 1.1507,
      "step": 368500
    },
    {
      "epoch": 4.85571039437843,
      "grad_norm": 2.457422971725464,
      "learning_rate": 7.623229968341437e-05,
      "loss": 1.1346,
      "step": 369000
    },
    {
      "epoch": 4.862289947758346,
      "grad_norm": 2.5155982971191406,
      "learning_rate": 7.619925026161366e-05,
      "loss": 1.1442,
      "step": 369500
    },
    {
      "epoch": 4.868869501138263,
      "grad_norm": 2.3263862133026123,
      "learning_rate": 7.616613460850676e-05,
      "loss": 1.1498,
      "step": 370000
    },
    {
      "epoch": 4.875449054518179,
      "grad_norm": 3.092434883117676,
      "learning_rate": 7.613301895539984e-05,
      "loss": 1.1361,
      "step": 370500
    },
    {
      "epoch": 4.882028607898096,
      "grad_norm": 2.1523241996765137,
      "learning_rate": 7.609996953359915e-05,
      "loss": 1.1357,
      "step": 371000
    },
    {
      "epoch": 4.888608161278013,
      "grad_norm": 2.6484737396240234,
      "learning_rate": 7.606685388049223e-05,
      "loss": 1.1368,
      "step": 371500
    },
    {
      "epoch": 4.895187714657929,
      "grad_norm": 2.1311657428741455,
      "learning_rate": 7.603373822738532e-05,
      "loss": 1.1497,
      "step": 372000
    },
    {
      "epoch": 4.901767268037846,
      "grad_norm": 2.3548803329467773,
      "learning_rate": 7.600062257427842e-05,
      "loss": 1.1381,
      "step": 372500
    },
    {
      "epoch": 4.908346821417762,
      "grad_norm": 2.3356587886810303,
      "learning_rate": 7.59675069211715e-05,
      "loss": 1.1481,
      "step": 373000
    },
    {
      "epoch": 4.914926374797679,
      "grad_norm": 2.281709909439087,
      "learning_rate": 7.59343912680646e-05,
      "loss": 1.1424,
      "step": 373500
    },
    {
      "epoch": 4.921505928177595,
      "grad_norm": 2.4379806518554688,
      "learning_rate": 7.590127561495767e-05,
      "loss": 1.1495,
      "step": 374000
    },
    {
      "epoch": 4.9280854815575115,
      "grad_norm": 2.5542099475860596,
      "learning_rate": 7.586822619315699e-05,
      "loss": 1.1558,
      "step": 374500
    },
    {
      "epoch": 4.934665034937429,
      "grad_norm": 2.3247616291046143,
      "learning_rate": 7.583511054005006e-05,
      "loss": 1.1382,
      "step": 375000
    },
    {
      "epoch": 4.941244588317345,
      "grad_norm": 2.440821409225464,
      "learning_rate": 7.580199488694316e-05,
      "loss": 1.1372,
      "step": 375500
    },
    {
      "epoch": 4.947824141697262,
      "grad_norm": 2.44916033744812,
      "learning_rate": 7.576887923383626e-05,
      "loss": 1.1459,
      "step": 376000
    },
    {
      "epoch": 4.954403695077178,
      "grad_norm": 2.139402151107788,
      "learning_rate": 7.573582981203555e-05,
      "loss": 1.1366,
      "step": 376500
    },
    {
      "epoch": 4.960983248457095,
      "grad_norm": 2.3127048015594482,
      "learning_rate": 7.570271415892865e-05,
      "loss": 1.1418,
      "step": 377000
    },
    {
      "epoch": 4.967562801837011,
      "grad_norm": 2.2646732330322266,
      "learning_rate": 7.566959850582174e-05,
      "loss": 1.1448,
      "step": 377500
    },
    {
      "epoch": 4.9741423552169275,
      "grad_norm": 2.2515718936920166,
      "learning_rate": 7.563648285271483e-05,
      "loss": 1.1471,
      "step": 378000
    },
    {
      "epoch": 4.980721908596845,
      "grad_norm": 2.4985926151275635,
      "learning_rate": 7.56033671996079e-05,
      "loss": 1.136,
      "step": 378500
    },
    {
      "epoch": 4.987301461976761,
      "grad_norm": 2.184616804122925,
      "learning_rate": 7.5570251546501e-05,
      "loss": 1.1483,
      "step": 379000
    },
    {
      "epoch": 4.993881015356678,
      "grad_norm": 2.3420846462249756,
      "learning_rate": 7.553713589339409e-05,
      "loss": 1.1507,
      "step": 379500
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.0848976373672485,
      "eval_runtime": 49.9573,
      "eval_samples_per_second": 2001.708,
      "eval_steps_per_second": 15.653,
      "step": 379965
    },
    {
      "epoch": 5.000460568736594,
      "grad_norm": 2.5178568363189697,
      "learning_rate": 7.550402024028719e-05,
      "loss": 1.1389,
      "step": 380000
    },
    {
      "epoch": 5.007040122116511,
      "grad_norm": 2.6647751331329346,
      "learning_rate": 7.547090458718027e-05,
      "loss": 1.1337,
      "step": 380500
    },
    {
      "epoch": 5.013619675496427,
      "grad_norm": 2.3722431659698486,
      "learning_rate": 7.543778893407336e-05,
      "loss": 1.1302,
      "step": 381000
    },
    {
      "epoch": 5.0201992288763435,
      "grad_norm": 2.4553287029266357,
      "learning_rate": 7.540467328096646e-05,
      "loss": 1.1368,
      "step": 381500
    },
    {
      "epoch": 5.026778782256261,
      "grad_norm": 2.212754964828491,
      "learning_rate": 7.537155762785953e-05,
      "loss": 1.1258,
      "step": 382000
    },
    {
      "epoch": 5.033358335636177,
      "grad_norm": 2.560650587081909,
      "learning_rate": 7.533850820605885e-05,
      "loss": 1.1381,
      "step": 382500
    },
    {
      "epoch": 5.039937889016094,
      "grad_norm": 2.872826099395752,
      "learning_rate": 7.530552501556436e-05,
      "loss": 1.131,
      "step": 383000
    },
    {
      "epoch": 5.04651744239601,
      "grad_norm": 2.448777675628662,
      "learning_rate": 7.527240936245746e-05,
      "loss": 1.1311,
      "step": 383500
    },
    {
      "epoch": 5.053096995775927,
      "grad_norm": 2.5181448459625244,
      "learning_rate": 7.523929370935054e-05,
      "loss": 1.1312,
      "step": 384000
    },
    {
      "epoch": 5.059676549155843,
      "grad_norm": 2.4626035690307617,
      "learning_rate": 7.520617805624363e-05,
      "loss": 1.1383,
      "step": 384500
    },
    {
      "epoch": 5.0662561025357595,
      "grad_norm": 2.2409508228302,
      "learning_rate": 7.517306240313671e-05,
      "loss": 1.1332,
      "step": 385000
    },
    {
      "epoch": 5.072835655915677,
      "grad_norm": 2.262699842453003,
      "learning_rate": 7.513994675002981e-05,
      "loss": 1.1228,
      "step": 385500
    },
    {
      "epoch": 5.079415209295593,
      "grad_norm": 2.4333655834198,
      "learning_rate": 7.510683109692289e-05,
      "loss": 1.1335,
      "step": 386000
    },
    {
      "epoch": 5.08599476267551,
      "grad_norm": 2.025927782058716,
      "learning_rate": 7.50737816751222e-05,
      "loss": 1.1264,
      "step": 386500
    },
    {
      "epoch": 5.092574316055426,
      "grad_norm": 2.4924073219299316,
      "learning_rate": 7.504066602201529e-05,
      "loss": 1.1278,
      "step": 387000
    },
    {
      "epoch": 5.099153869435343,
      "grad_norm": 2.4533708095550537,
      "learning_rate": 7.500755036890837e-05,
      "loss": 1.1368,
      "step": 387500
    },
    {
      "epoch": 5.105733422815259,
      "grad_norm": 2.687056303024292,
      "learning_rate": 7.497443471580147e-05,
      "loss": 1.1305,
      "step": 388000
    },
    {
      "epoch": 5.1123129761951756,
      "grad_norm": 2.1889021396636963,
      "learning_rate": 7.494131906269456e-05,
      "loss": 1.1353,
      "step": 388500
    },
    {
      "epoch": 5.118892529575092,
      "grad_norm": 2.4827685356140137,
      "learning_rate": 7.490820340958766e-05,
      "loss": 1.1352,
      "step": 389000
    },
    {
      "epoch": 5.125472082955009,
      "grad_norm": 2.4609262943267822,
      "learning_rate": 7.487508775648073e-05,
      "loss": 1.1279,
      "step": 389500
    },
    {
      "epoch": 5.132051636334926,
      "grad_norm": 2.5409295558929443,
      "learning_rate": 7.484197210337383e-05,
      "loss": 1.1338,
      "step": 390000
    },
    {
      "epoch": 5.138631189714842,
      "grad_norm": 2.4015135765075684,
      "learning_rate": 7.480885645026691e-05,
      "loss": 1.121,
      "step": 390500
    },
    {
      "epoch": 5.145210743094759,
      "grad_norm": 2.4674136638641357,
      "learning_rate": 7.477580702846622e-05,
      "loss": 1.1276,
      "step": 391000
    },
    {
      "epoch": 5.151790296474675,
      "grad_norm": 5.559149265289307,
      "learning_rate": 7.474275760666552e-05,
      "loss": 1.1369,
      "step": 391500
    },
    {
      "epoch": 5.158369849854592,
      "grad_norm": 2.469104290008545,
      "learning_rate": 7.470964195355861e-05,
      "loss": 1.1235,
      "step": 392000
    },
    {
      "epoch": 5.164949403234508,
      "grad_norm": 2.3622822761535645,
      "learning_rate": 7.46765263004517e-05,
      "loss": 1.1233,
      "step": 392500
    },
    {
      "epoch": 5.171528956614425,
      "grad_norm": 2.564804792404175,
      "learning_rate": 7.464341064734479e-05,
      "loss": 1.1216,
      "step": 393000
    },
    {
      "epoch": 5.178108509994342,
      "grad_norm": 2.564910411834717,
      "learning_rate": 7.461029499423788e-05,
      "loss": 1.1356,
      "step": 393500
    },
    {
      "epoch": 5.184688063374258,
      "grad_norm": 2.499943733215332,
      "learning_rate": 7.457717934113096e-05,
      "loss": 1.1364,
      "step": 394000
    },
    {
      "epoch": 5.191267616754175,
      "grad_norm": 2.4597842693328857,
      "learning_rate": 7.454406368802406e-05,
      "loss": 1.1332,
      "step": 394500
    },
    {
      "epoch": 5.197847170134091,
      "grad_norm": 2.4315643310546875,
      "learning_rate": 7.451094803491715e-05,
      "loss": 1.1356,
      "step": 395000
    },
    {
      "epoch": 5.204426723514008,
      "grad_norm": 2.3930745124816895,
      "learning_rate": 7.447783238181025e-05,
      "loss": 1.1261,
      "step": 395500
    },
    {
      "epoch": 5.211006276893924,
      "grad_norm": 2.4039793014526367,
      "learning_rate": 7.444471672870332e-05,
      "loss": 1.1274,
      "step": 396000
    },
    {
      "epoch": 5.217585830273841,
      "grad_norm": 2.4360427856445312,
      "learning_rate": 7.441179976951506e-05,
      "loss": 1.1308,
      "step": 396500
    },
    {
      "epoch": 5.224165383653758,
      "grad_norm": 2.1750683784484863,
      "learning_rate": 7.437875034771437e-05,
      "loss": 1.1339,
      "step": 397000
    },
    {
      "epoch": 5.230744937033674,
      "grad_norm": 2.7809462547302246,
      "learning_rate": 7.434563469460745e-05,
      "loss": 1.1299,
      "step": 397500
    },
    {
      "epoch": 5.237324490413591,
      "grad_norm": 2.5993616580963135,
      "learning_rate": 7.431258527280676e-05,
      "loss": 1.1363,
      "step": 398000
    },
    {
      "epoch": 5.243904043793507,
      "grad_norm": 2.4637367725372314,
      "learning_rate": 7.427946961969984e-05,
      "loss": 1.1386,
      "step": 398500
    },
    {
      "epoch": 5.250483597173424,
      "grad_norm": 2.529304027557373,
      "learning_rate": 7.424635396659293e-05,
      "loss": 1.1316,
      "step": 399000
    },
    {
      "epoch": 5.25706315055334,
      "grad_norm": 2.247370481491089,
      "learning_rate": 7.421323831348603e-05,
      "loss": 1.1313,
      "step": 399500
    },
    {
      "epoch": 5.263642703933257,
      "grad_norm": 2.6360650062561035,
      "learning_rate": 7.418012266037911e-05,
      "loss": 1.122,
      "step": 400000
    },
    {
      "epoch": 5.270222257313174,
      "grad_norm": 2.423912763595581,
      "learning_rate": 7.41470070072722e-05,
      "loss": 1.1288,
      "step": 400500
    },
    {
      "epoch": 5.27680181069309,
      "grad_norm": 2.2336313724517822,
      "learning_rate": 7.411389135416528e-05,
      "loss": 1.1174,
      "step": 401000
    },
    {
      "epoch": 5.283381364073007,
      "grad_norm": 2.3506040573120117,
      "learning_rate": 7.408077570105838e-05,
      "loss": 1.1284,
      "step": 401500
    },
    {
      "epoch": 5.289960917452923,
      "grad_norm": 2.4965455532073975,
      "learning_rate": 7.404766004795147e-05,
      "loss": 1.1314,
      "step": 402000
    },
    {
      "epoch": 5.29654047083284,
      "grad_norm": 2.227682113647461,
      "learning_rate": 7.401454439484455e-05,
      "loss": 1.1187,
      "step": 402500
    },
    {
      "epoch": 5.303120024212756,
      "grad_norm": 2.5321543216705322,
      "learning_rate": 7.398142874173765e-05,
      "loss": 1.1271,
      "step": 403000
    },
    {
      "epoch": 5.309699577592673,
      "grad_norm": 2.3653008937835693,
      "learning_rate": 7.394831308863074e-05,
      "loss": 1.1243,
      "step": 403500
    },
    {
      "epoch": 5.31627913097259,
      "grad_norm": 2.254655122756958,
      "learning_rate": 7.391519743552383e-05,
      "loss": 1.1313,
      "step": 404000
    },
    {
      "epoch": 5.322858684352506,
      "grad_norm": 2.457047700881958,
      "learning_rate": 7.38820817824169e-05,
      "loss": 1.1214,
      "step": 404500
    },
    {
      "epoch": 5.329438237732423,
      "grad_norm": 2.2707748413085938,
      "learning_rate": 7.384896612931e-05,
      "loss": 1.1237,
      "step": 405000
    },
    {
      "epoch": 5.336017791112339,
      "grad_norm": 2.8224291801452637,
      "learning_rate": 7.381585047620309e-05,
      "loss": 1.1198,
      "step": 405500
    },
    {
      "epoch": 5.342597344492256,
      "grad_norm": 2.3803062438964844,
      "learning_rate": 7.378273482309619e-05,
      "loss": 1.1257,
      "step": 406000
    },
    {
      "epoch": 5.349176897872172,
      "grad_norm": 2.6056747436523438,
      "learning_rate": 7.374961916998928e-05,
      "loss": 1.1243,
      "step": 406500
    },
    {
      "epoch": 5.355756451252089,
      "grad_norm": 2.463562488555908,
      "learning_rate": 7.371656974818858e-05,
      "loss": 1.1371,
      "step": 407000
    },
    {
      "epoch": 5.362336004632006,
      "grad_norm": 2.2453246116638184,
      "learning_rate": 7.368345409508168e-05,
      "loss": 1.1307,
      "step": 407500
    },
    {
      "epoch": 5.368915558011922,
      "grad_norm": 2.981584072113037,
      "learning_rate": 7.365033844197475e-05,
      "loss": 1.1233,
      "step": 408000
    },
    {
      "epoch": 5.375495111391839,
      "grad_norm": 2.5733072757720947,
      "learning_rate": 7.361728902017407e-05,
      "loss": 1.1219,
      "step": 408500
    },
    {
      "epoch": 5.382074664771755,
      "grad_norm": 2.479353666305542,
      "learning_rate": 7.358417336706714e-05,
      "loss": 1.1203,
      "step": 409000
    },
    {
      "epoch": 5.388654218151672,
      "grad_norm": 2.402780055999756,
      "learning_rate": 7.355105771396024e-05,
      "loss": 1.1225,
      "step": 409500
    },
    {
      "epoch": 5.395233771531588,
      "grad_norm": 2.553281307220459,
      "learning_rate": 7.351794206085332e-05,
      "loss": 1.1206,
      "step": 410000
    },
    {
      "epoch": 5.401813324911505,
      "grad_norm": 2.3416848182678223,
      "learning_rate": 7.348482640774642e-05,
      "loss": 1.122,
      "step": 410500
    },
    {
      "epoch": 5.408392878291422,
      "grad_norm": 2.394042730331421,
      "learning_rate": 7.345171075463951e-05,
      "loss": 1.1265,
      "step": 411000
    },
    {
      "epoch": 5.414972431671338,
      "grad_norm": 2.104586362838745,
      "learning_rate": 7.34185951015326e-05,
      "loss": 1.1239,
      "step": 411500
    },
    {
      "epoch": 5.421551985051255,
      "grad_norm": 2.4536426067352295,
      "learning_rate": 7.338547944842569e-05,
      "loss": 1.1274,
      "step": 412000
    },
    {
      "epoch": 5.428131538431171,
      "grad_norm": 2.7629101276397705,
      "learning_rate": 7.335236379531878e-05,
      "loss": 1.1224,
      "step": 412500
    },
    {
      "epoch": 5.434711091811088,
      "grad_norm": 2.1670525074005127,
      "learning_rate": 7.331924814221186e-05,
      "loss": 1.1189,
      "step": 413000
    },
    {
      "epoch": 5.441290645191004,
      "grad_norm": 2.3601973056793213,
      "learning_rate": 7.328613248910495e-05,
      "loss": 1.1206,
      "step": 413500
    },
    {
      "epoch": 5.4478701985709215,
      "grad_norm": 2.533322811126709,
      "learning_rate": 7.325301683599805e-05,
      "loss": 1.1223,
      "step": 414000
    },
    {
      "epoch": 5.454449751950838,
      "grad_norm": 2.5271947383880615,
      "learning_rate": 7.321996741419734e-05,
      "loss": 1.1248,
      "step": 414500
    },
    {
      "epoch": 5.461029305330754,
      "grad_norm": 3.428253650665283,
      "learning_rate": 7.318685176109044e-05,
      "loss": 1.1248,
      "step": 415000
    },
    {
      "epoch": 5.467608858710671,
      "grad_norm": 2.401284694671631,
      "learning_rate": 7.315373610798352e-05,
      "loss": 1.1254,
      "step": 415500
    },
    {
      "epoch": 5.474188412090587,
      "grad_norm": 2.1516873836517334,
      "learning_rate": 7.312062045487661e-05,
      "loss": 1.1303,
      "step": 416000
    },
    {
      "epoch": 5.480767965470504,
      "grad_norm": 2.3103177547454834,
      "learning_rate": 7.308750480176971e-05,
      "loss": 1.1157,
      "step": 416500
    },
    {
      "epoch": 5.48734751885042,
      "grad_norm": 2.3527286052703857,
      "learning_rate": 7.305438914866279e-05,
      "loss": 1.122,
      "step": 417000
    },
    {
      "epoch": 5.493927072230337,
      "grad_norm": 2.4503815174102783,
      "learning_rate": 7.302127349555589e-05,
      "loss": 1.1161,
      "step": 417500
    },
    {
      "epoch": 5.500506625610254,
      "grad_norm": 2.1717066764831543,
      "learning_rate": 7.298815784244896e-05,
      "loss": 1.1178,
      "step": 418000
    },
    {
      "epoch": 5.50708617899017,
      "grad_norm": 2.392498254776001,
      "learning_rate": 7.295504218934206e-05,
      "loss": 1.1188,
      "step": 418500
    },
    {
      "epoch": 5.513665732370087,
      "grad_norm": 2.6528284549713135,
      "learning_rate": 7.292192653623515e-05,
      "loss": 1.1249,
      "step": 419000
    },
    {
      "epoch": 5.520245285750003,
      "grad_norm": 2.3781838417053223,
      "learning_rate": 7.288887711443445e-05,
      "loss": 1.1131,
      "step": 419500
    },
    {
      "epoch": 5.52682483912992,
      "grad_norm": 2.505140781402588,
      "learning_rate": 7.285576146132754e-05,
      "loss": 1.1102,
      "step": 420000
    },
    {
      "epoch": 5.533404392509836,
      "grad_norm": 2.034761905670166,
      "learning_rate": 7.282264580822064e-05,
      "loss": 1.1193,
      "step": 420500
    },
    {
      "epoch": 5.539983945889753,
      "grad_norm": 2.2184972763061523,
      "learning_rate": 7.278953015511372e-05,
      "loss": 1.1126,
      "step": 421000
    },
    {
      "epoch": 5.54656349926967,
      "grad_norm": 2.4671216011047363,
      "learning_rate": 7.275648073331303e-05,
      "loss": 1.1122,
      "step": 421500
    },
    {
      "epoch": 5.553143052649586,
      "grad_norm": 2.3921215534210205,
      "learning_rate": 7.272336508020611e-05,
      "loss": 1.1156,
      "step": 422000
    },
    {
      "epoch": 5.559722606029503,
      "grad_norm": 2.705256223678589,
      "learning_rate": 7.26902494270992e-05,
      "loss": 1.1195,
      "step": 422500
    },
    {
      "epoch": 5.566302159409419,
      "grad_norm": 2.590806722640991,
      "learning_rate": 7.26571337739923e-05,
      "loss": 1.1137,
      "step": 423000
    },
    {
      "epoch": 5.572881712789336,
      "grad_norm": 2.29480242729187,
      "learning_rate": 7.26240843521916e-05,
      "loss": 1.1107,
      "step": 423500
    },
    {
      "epoch": 5.579461266169252,
      "grad_norm": 2.218973398208618,
      "learning_rate": 7.259096869908469e-05,
      "loss": 1.1219,
      "step": 424000
    },
    {
      "epoch": 5.586040819549169,
      "grad_norm": 2.2722394466400146,
      "learning_rate": 7.255785304597777e-05,
      "loss": 1.1188,
      "step": 424500
    },
    {
      "epoch": 5.592620372929085,
      "grad_norm": 2.426806926727295,
      "learning_rate": 7.252473739287087e-05,
      "loss": 1.1115,
      "step": 425000
    },
    {
      "epoch": 5.599199926309002,
      "grad_norm": 2.203651189804077,
      "learning_rate": 7.249168797107016e-05,
      "loss": 1.1193,
      "step": 425500
    },
    {
      "epoch": 5.605779479688919,
      "grad_norm": 2.574148654937744,
      "learning_rate": 7.245857231796326e-05,
      "loss": 1.1068,
      "step": 426000
    },
    {
      "epoch": 5.612359033068835,
      "grad_norm": 2.4552841186523438,
      "learning_rate": 7.242552289616255e-05,
      "loss": 1.1188,
      "step": 426500
    },
    {
      "epoch": 5.618938586448752,
      "grad_norm": 2.4836225509643555,
      "learning_rate": 7.239240724305565e-05,
      "loss": 1.1148,
      "step": 427000
    },
    {
      "epoch": 5.625518139828668,
      "grad_norm": 2.1956357955932617,
      "learning_rate": 7.235929158994874e-05,
      "loss": 1.1171,
      "step": 427500
    },
    {
      "epoch": 5.632097693208585,
      "grad_norm": 2.5563626289367676,
      "learning_rate": 7.232617593684184e-05,
      "loss": 1.1147,
      "step": 428000
    },
    {
      "epoch": 5.638677246588502,
      "grad_norm": 2.368164300918579,
      "learning_rate": 7.229306028373492e-05,
      "loss": 1.1161,
      "step": 428500
    },
    {
      "epoch": 5.6452567999684184,
      "grad_norm": 2.45847749710083,
      "learning_rate": 7.2259944630628e-05,
      "loss": 1.112,
      "step": 429000
    },
    {
      "epoch": 5.651836353348335,
      "grad_norm": 2.7871556282043457,
      "learning_rate": 7.222689520882731e-05,
      "loss": 1.1134,
      "step": 429500
    },
    {
      "epoch": 5.658415906728251,
      "grad_norm": 2.403024911880493,
      "learning_rate": 7.21937795557204e-05,
      "loss": 1.1151,
      "step": 430000
    },
    {
      "epoch": 5.664995460108168,
      "grad_norm": 2.3245487213134766,
      "learning_rate": 7.21606639026135e-05,
      "loss": 1.1181,
      "step": 430500
    },
    {
      "epoch": 5.671575013488084,
      "grad_norm": 2.169771671295166,
      "learning_rate": 7.212754824950658e-05,
      "loss": 1.1046,
      "step": 431000
    },
    {
      "epoch": 5.678154566868001,
      "grad_norm": 2.721892833709717,
      "learning_rate": 7.209443259639968e-05,
      "loss": 1.1133,
      "step": 431500
    },
    {
      "epoch": 5.684734120247917,
      "grad_norm": 2.1072070598602295,
      "learning_rate": 7.206138317459897e-05,
      "loss": 1.1208,
      "step": 432000
    },
    {
      "epoch": 5.6913136736278345,
      "grad_norm": 2.6502788066864014,
      "learning_rate": 7.202826752149207e-05,
      "loss": 1.1104,
      "step": 432500
    },
    {
      "epoch": 5.697893227007751,
      "grad_norm": 2.598518133163452,
      "learning_rate": 7.199515186838514e-05,
      "loss": 1.1094,
      "step": 433000
    },
    {
      "epoch": 5.704472780387667,
      "grad_norm": 2.5360429286956787,
      "learning_rate": 7.196203621527824e-05,
      "loss": 1.1149,
      "step": 433500
    },
    {
      "epoch": 5.711052333767584,
      "grad_norm": 2.527641534805298,
      "learning_rate": 7.192892056217133e-05,
      "loss": 1.1091,
      "step": 434000
    },
    {
      "epoch": 5.7176318871475,
      "grad_norm": 2.4058635234832764,
      "learning_rate": 7.189580490906442e-05,
      "loss": 1.1145,
      "step": 434500
    },
    {
      "epoch": 5.724211440527417,
      "grad_norm": 2.24342679977417,
      "learning_rate": 7.186268925595751e-05,
      "loss": 1.1105,
      "step": 435000
    },
    {
      "epoch": 5.730790993907334,
      "grad_norm": 2.1084721088409424,
      "learning_rate": 7.182963983415681e-05,
      "loss": 1.114,
      "step": 435500
    },
    {
      "epoch": 5.7373705472872505,
      "grad_norm": 2.511122465133667,
      "learning_rate": 7.179652418104991e-05,
      "loss": 1.1079,
      "step": 436000
    },
    {
      "epoch": 5.743950100667167,
      "grad_norm": 2.4917514324188232,
      "learning_rate": 7.176340852794298e-05,
      "loss": 1.1086,
      "step": 436500
    },
    {
      "epoch": 5.750529654047083,
      "grad_norm": 2.7593486309051514,
      "learning_rate": 7.173029287483608e-05,
      "loss": 1.1158,
      "step": 437000
    },
    {
      "epoch": 5.757109207427,
      "grad_norm": 2.1524524688720703,
      "learning_rate": 7.169717722172917e-05,
      "loss": 1.1097,
      "step": 437500
    },
    {
      "epoch": 5.763688760806916,
      "grad_norm": 2.399083137512207,
      "learning_rate": 7.166406156862227e-05,
      "loss": 1.1196,
      "step": 438000
    },
    {
      "epoch": 5.770268314186833,
      "grad_norm": 2.5943987369537354,
      "learning_rate": 7.163094591551534e-05,
      "loss": 1.1194,
      "step": 438500
    },
    {
      "epoch": 5.776847867566749,
      "grad_norm": 2.167064905166626,
      "learning_rate": 7.159789649371466e-05,
      "loss": 1.1171,
      "step": 439000
    },
    {
      "epoch": 5.7834274209466665,
      "grad_norm": 2.223388433456421,
      "learning_rate": 7.156478084060774e-05,
      "loss": 1.1156,
      "step": 439500
    },
    {
      "epoch": 5.790006974326583,
      "grad_norm": 2.312055826187134,
      "learning_rate": 7.153173141880705e-05,
      "loss": 1.1105,
      "step": 440000
    },
    {
      "epoch": 5.796586527706499,
      "grad_norm": 2.5245368480682373,
      "learning_rate": 7.149861576570013e-05,
      "loss": 1.113,
      "step": 440500
    },
    {
      "epoch": 5.803166081086416,
      "grad_norm": 2.3415069580078125,
      "learning_rate": 7.146550011259322e-05,
      "loss": 1.1119,
      "step": 441000
    },
    {
      "epoch": 5.809745634466332,
      "grad_norm": 2.433417797088623,
      "learning_rate": 7.143238445948632e-05,
      "loss": 1.1107,
      "step": 441500
    },
    {
      "epoch": 5.816325187846249,
      "grad_norm": 2.27118182182312,
      "learning_rate": 7.13992688063794e-05,
      "loss": 1.1178,
      "step": 442000
    },
    {
      "epoch": 5.822904741226165,
      "grad_norm": 2.2616677284240723,
      "learning_rate": 7.13661531532725e-05,
      "loss": 1.114,
      "step": 442500
    },
    {
      "epoch": 5.8294842946060825,
      "grad_norm": 2.402179718017578,
      "learning_rate": 7.133303750016557e-05,
      "loss": 1.106,
      "step": 443000
    },
    {
      "epoch": 5.836063847985999,
      "grad_norm": 2.2446954250335693,
      "learning_rate": 7.129992184705867e-05,
      "loss": 1.1137,
      "step": 443500
    },
    {
      "epoch": 5.842643401365915,
      "grad_norm": 2.477332830429077,
      "learning_rate": 7.126680619395176e-05,
      "loss": 1.1042,
      "step": 444000
    },
    {
      "epoch": 5.849222954745832,
      "grad_norm": 2.406822443008423,
      "learning_rate": 7.123369054084486e-05,
      "loss": 1.1013,
      "step": 444500
    },
    {
      "epoch": 5.855802508125748,
      "grad_norm": 2.5992846488952637,
      "learning_rate": 7.120064111904415e-05,
      "loss": 1.1073,
      "step": 445000
    },
    {
      "epoch": 5.862382061505665,
      "grad_norm": 2.3426406383514404,
      "learning_rate": 7.116752546593725e-05,
      "loss": 1.1048,
      "step": 445500
    },
    {
      "epoch": 5.868961614885581,
      "grad_norm": 2.4713199138641357,
      "learning_rate": 7.113440981283033e-05,
      "loss": 1.1106,
      "step": 446000
    },
    {
      "epoch": 5.875541168265498,
      "grad_norm": 2.10608172416687,
      "learning_rate": 7.110129415972342e-05,
      "loss": 1.1066,
      "step": 446500
    },
    {
      "epoch": 5.882120721645415,
      "grad_norm": 2.5569839477539062,
      "learning_rate": 7.106817850661652e-05,
      "loss": 1.103,
      "step": 447000
    },
    {
      "epoch": 5.888700275025331,
      "grad_norm": 2.543036937713623,
      "learning_rate": 7.10350628535096e-05,
      "loss": 1.1144,
      "step": 447500
    },
    {
      "epoch": 5.895279828405248,
      "grad_norm": 2.255427122116089,
      "learning_rate": 7.100194720040269e-05,
      "loss": 1.1025,
      "step": 448000
    },
    {
      "epoch": 5.901859381785164,
      "grad_norm": 2.3939743041992188,
      "learning_rate": 7.096883154729577e-05,
      "loss": 1.1099,
      "step": 448500
    },
    {
      "epoch": 5.908438935165081,
      "grad_norm": 2.405148983001709,
      "learning_rate": 7.093571589418887e-05,
      "loss": 1.1107,
      "step": 449000
    },
    {
      "epoch": 5.915018488544997,
      "grad_norm": 2.2783570289611816,
      "learning_rate": 7.090260024108196e-05,
      "loss": 1.1101,
      "step": 449500
    },
    {
      "epoch": 5.921598041924915,
      "grad_norm": 2.382747173309326,
      "learning_rate": 7.086948458797504e-05,
      "loss": 1.1098,
      "step": 450000
    },
    {
      "epoch": 5.928177595304831,
      "grad_norm": 2.476557731628418,
      "learning_rate": 7.083636893486814e-05,
      "loss": 1.1042,
      "step": 450500
    },
    {
      "epoch": 5.9347571486847475,
      "grad_norm": 2.3767993450164795,
      "learning_rate": 7.080331951306745e-05,
      "loss": 1.1035,
      "step": 451000
    },
    {
      "epoch": 5.941336702064664,
      "grad_norm": 2.47698712348938,
      "learning_rate": 7.077027009126675e-05,
      "loss": 1.1192,
      "step": 451500
    },
    {
      "epoch": 5.94791625544458,
      "grad_norm": 2.4184205532073975,
      "learning_rate": 7.073722066946604e-05,
      "loss": 1.1089,
      "step": 452000
    },
    {
      "epoch": 5.954495808824497,
      "grad_norm": 2.524925708770752,
      "learning_rate": 7.070410501635914e-05,
      "loss": 1.1041,
      "step": 452500
    },
    {
      "epoch": 5.961075362204413,
      "grad_norm": 2.8864786624908447,
      "learning_rate": 7.067098936325223e-05,
      "loss": 1.1091,
      "step": 453000
    },
    {
      "epoch": 5.96765491558433,
      "grad_norm": 2.37845516204834,
      "learning_rate": 7.063787371014532e-05,
      "loss": 1.1011,
      "step": 453500
    },
    {
      "epoch": 5.974234468964247,
      "grad_norm": 2.3558502197265625,
      "learning_rate": 7.06047580570384e-05,
      "loss": 1.112,
      "step": 454000
    },
    {
      "epoch": 5.9808140223441635,
      "grad_norm": 2.272486925125122,
      "learning_rate": 7.05716424039315e-05,
      "loss": 1.098,
      "step": 454500
    },
    {
      "epoch": 5.98739357572408,
      "grad_norm": 2.39921498298645,
      "learning_rate": 7.053852675082458e-05,
      "loss": 1.1164,
      "step": 455000
    },
    {
      "epoch": 5.993973129103996,
      "grad_norm": 2.374601125717163,
      "learning_rate": 7.050541109771768e-05,
      "loss": 1.103,
      "step": 455500
    },
    {
      "epoch": 6.0,
      "eval_loss": 1.0509576797485352,
      "eval_runtime": 49.7994,
      "eval_samples_per_second": 2008.058,
      "eval_steps_per_second": 15.703,
      "step": 455958
    }
  ],
  "logging_steps": 500,
  "max_steps": 1519860,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.314965891134014e+18,
  "train_batch_size": 128,
  "trial_name": null,
  "trial_params": null
}
