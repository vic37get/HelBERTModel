{
  "best_metric": 0.8748059868812561,
  "best_model_checkpoint": "../Modelos/HelBERT-uncased-fs/checkpoint-1063902",
  "epoch": 14.0,
  "eval_steps": 500,
  "global_step": 1063902,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006579553379916571,
      "grad_norm": 3.139854907989502,
      "learning_rate": 5e-06,
      "loss": 9.084,
      "step": 500
    },
    {
      "epoch": 0.013159106759833143,
      "grad_norm": 2.2373688220977783,
      "learning_rate": 1e-05,
      "loss": 7.1712,
      "step": 1000
    },
    {
      "epoch": 0.019738660139749713,
      "grad_norm": 1.789867877960205,
      "learning_rate": 1.5e-05,
      "loss": 6.3245,
      "step": 1500
    },
    {
      "epoch": 0.026318213519666286,
      "grad_norm": 2.5151095390319824,
      "learning_rate": 2e-05,
      "loss": 6.1247,
      "step": 2000
    },
    {
      "epoch": 0.032897766899582855,
      "grad_norm": 2.3780860900878906,
      "learning_rate": 2.5e-05,
      "loss": 5.9748,
      "step": 2500
    },
    {
      "epoch": 0.03947732027949943,
      "grad_norm": 2.5433366298675537,
      "learning_rate": 3e-05,
      "loss": 5.8166,
      "step": 3000
    },
    {
      "epoch": 0.046056873659416,
      "grad_norm": 3.067661762237549,
      "learning_rate": 3.5e-05,
      "loss": 5.6112,
      "step": 3500
    },
    {
      "epoch": 0.05263642703933257,
      "grad_norm": 3.3288516998291016,
      "learning_rate": 4e-05,
      "loss": 5.3119,
      "step": 4000
    },
    {
      "epoch": 0.059215980419249144,
      "grad_norm": 3.9524641036987305,
      "learning_rate": 4.5e-05,
      "loss": 4.9226,
      "step": 4500
    },
    {
      "epoch": 0.06579553379916571,
      "grad_norm": 3.847578525543213,
      "learning_rate": 5e-05,
      "loss": 4.5269,
      "step": 5000
    },
    {
      "epoch": 0.07237508717908228,
      "grad_norm": 4.049356460571289,
      "learning_rate": 5.500000000000001e-05,
      "loss": 4.1464,
      "step": 5500
    },
    {
      "epoch": 0.07895464055899885,
      "grad_norm": 3.4120535850524902,
      "learning_rate": 6e-05,
      "loss": 3.832,
      "step": 6000
    },
    {
      "epoch": 0.08553419393891543,
      "grad_norm": 3.045287609100342,
      "learning_rate": 6.500000000000001e-05,
      "loss": 3.5526,
      "step": 6500
    },
    {
      "epoch": 0.092113747318832,
      "grad_norm": 2.9033312797546387,
      "learning_rate": 7e-05,
      "loss": 3.3681,
      "step": 7000
    },
    {
      "epoch": 0.09869330069874857,
      "grad_norm": 3.1457715034484863,
      "learning_rate": 7.500000000000001e-05,
      "loss": 3.202,
      "step": 7500
    },
    {
      "epoch": 0.10527285407866514,
      "grad_norm": 3.3716514110565186,
      "learning_rate": 8e-05,
      "loss": 3.0826,
      "step": 8000
    },
    {
      "epoch": 0.11185240745858172,
      "grad_norm": 3.0209219455718994,
      "learning_rate": 8.499e-05,
      "loss": 2.9587,
      "step": 8500
    },
    {
      "epoch": 0.11843196083849829,
      "grad_norm": 3.3128323554992676,
      "learning_rate": 8.999000000000001e-05,
      "loss": 2.8665,
      "step": 9000
    },
    {
      "epoch": 0.12501151421841486,
      "grad_norm": 2.910661458969116,
      "learning_rate": 9.499e-05,
      "loss": 2.7729,
      "step": 9500
    },
    {
      "epoch": 0.13159106759833142,
      "grad_norm": 2.710343360900879,
      "learning_rate": 9.999000000000001e-05,
      "loss": 2.7214,
      "step": 10000
    },
    {
      "epoch": 0.138170620978248,
      "grad_norm": 2.936994791030884,
      "learning_rate": 9.996701680950552e-05,
      "loss": 2.6441,
      "step": 10500
    },
    {
      "epoch": 0.14475017435816456,
      "grad_norm": 2.7759897708892822,
      "learning_rate": 9.993390115639861e-05,
      "loss": 2.5765,
      "step": 11000
    },
    {
      "epoch": 0.15132972773808115,
      "grad_norm": 2.923658847808838,
      "learning_rate": 9.99007855032917e-05,
      "loss": 2.5162,
      "step": 11500
    },
    {
      "epoch": 0.1579092811179977,
      "grad_norm": 2.9793031215667725,
      "learning_rate": 9.986766985018479e-05,
      "loss": 2.463,
      "step": 12000
    },
    {
      "epoch": 0.1644888344979143,
      "grad_norm": 2.766552686691284,
      "learning_rate": 9.983462042838409e-05,
      "loss": 2.4181,
      "step": 12500
    },
    {
      "epoch": 0.17106838787783085,
      "grad_norm": 2.8277833461761475,
      "learning_rate": 9.980150477527719e-05,
      "loss": 2.3695,
      "step": 13000
    },
    {
      "epoch": 0.1776479412577474,
      "grad_norm": 3.004333019256592,
      "learning_rate": 9.976838912217027e-05,
      "loss": 2.3361,
      "step": 13500
    },
    {
      "epoch": 0.184227494637664,
      "grad_norm": 2.8478944301605225,
      "learning_rate": 9.973527346906336e-05,
      "loss": 2.302,
      "step": 14000
    },
    {
      "epoch": 0.19080704801758055,
      "grad_norm": 2.788205146789551,
      "learning_rate": 9.970215781595646e-05,
      "loss": 2.2567,
      "step": 14500
    },
    {
      "epoch": 0.19738660139749714,
      "grad_norm": 2.9470596313476562,
      "learning_rate": 9.966904216284953e-05,
      "loss": 2.234,
      "step": 15000
    },
    {
      "epoch": 0.2039661547774137,
      "grad_norm": 2.5772016048431396,
      "learning_rate": 9.963599274104885e-05,
      "loss": 2.1989,
      "step": 15500
    },
    {
      "epoch": 0.21054570815733029,
      "grad_norm": 2.542638063430786,
      "learning_rate": 9.960287708794193e-05,
      "loss": 2.1804,
      "step": 16000
    },
    {
      "epoch": 0.21712526153724684,
      "grad_norm": 2.7925097942352295,
      "learning_rate": 9.956976143483502e-05,
      "loss": 2.163,
      "step": 16500
    },
    {
      "epoch": 0.22370481491716343,
      "grad_norm": 2.9346425533294678,
      "learning_rate": 9.95366457817281e-05,
      "loss": 2.132,
      "step": 17000
    },
    {
      "epoch": 0.23028436829708,
      "grad_norm": 2.594303607940674,
      "learning_rate": 9.95035301286212e-05,
      "loss": 2.1195,
      "step": 17500
    },
    {
      "epoch": 0.23686392167699657,
      "grad_norm": 2.908281087875366,
      "learning_rate": 9.94704807068205e-05,
      "loss": 2.102,
      "step": 18000
    },
    {
      "epoch": 0.24344347505691313,
      "grad_norm": 2.6272025108337402,
      "learning_rate": 9.94373650537136e-05,
      "loss": 2.0817,
      "step": 18500
    },
    {
      "epoch": 0.2500230284368297,
      "grad_norm": 2.709420919418335,
      "learning_rate": 9.940424940060669e-05,
      "loss": 2.0516,
      "step": 19000
    },
    {
      "epoch": 0.2566025818167463,
      "grad_norm": 2.9833767414093018,
      "learning_rate": 9.937113374749978e-05,
      "loss": 2.0376,
      "step": 19500
    },
    {
      "epoch": 0.26318213519666284,
      "grad_norm": 2.6850810050964355,
      "learning_rate": 9.933808432569908e-05,
      "loss": 2.012,
      "step": 20000
    },
    {
      "epoch": 0.2697616885765794,
      "grad_norm": 2.8599801063537598,
      "learning_rate": 9.930496867259217e-05,
      "loss": 2.005,
      "step": 20500
    },
    {
      "epoch": 0.276341241956496,
      "grad_norm": 2.734727621078491,
      "learning_rate": 9.927185301948525e-05,
      "loss": 1.9816,
      "step": 21000
    },
    {
      "epoch": 0.28292079533641257,
      "grad_norm": 2.8560431003570557,
      "learning_rate": 9.923873736637834e-05,
      "loss": 1.9766,
      "step": 21500
    },
    {
      "epoch": 0.2895003487163291,
      "grad_norm": 2.7166988849639893,
      "learning_rate": 9.920562171327144e-05,
      "loss": 1.9639,
      "step": 22000
    },
    {
      "epoch": 0.2960799020962457,
      "grad_norm": 2.569251775741577,
      "learning_rate": 9.917250606016452e-05,
      "loss": 1.9496,
      "step": 22500
    },
    {
      "epoch": 0.3026594554761623,
      "grad_norm": 2.682354211807251,
      "learning_rate": 9.913945663836383e-05,
      "loss": 1.9314,
      "step": 23000
    },
    {
      "epoch": 0.30923900885607886,
      "grad_norm": 2.4927756786346436,
      "learning_rate": 9.910634098525691e-05,
      "loss": 1.9201,
      "step": 23500
    },
    {
      "epoch": 0.3158185622359954,
      "grad_norm": 2.51750111579895,
      "learning_rate": 9.907322533215001e-05,
      "loss": 1.9136,
      "step": 24000
    },
    {
      "epoch": 0.322398115615912,
      "grad_norm": 2.5202322006225586,
      "learning_rate": 9.90401096790431e-05,
      "loss": 1.8938,
      "step": 24500
    },
    {
      "epoch": 0.3289776689958286,
      "grad_norm": 2.753662347793579,
      "learning_rate": 9.900699402593618e-05,
      "loss": 1.8911,
      "step": 25000
    },
    {
      "epoch": 0.33555722237574515,
      "grad_norm": 2.8912603855133057,
      "learning_rate": 9.897394460413549e-05,
      "loss": 1.8904,
      "step": 25500
    },
    {
      "epoch": 0.3421367757556617,
      "grad_norm": 2.6139957904815674,
      "learning_rate": 9.894082895102857e-05,
      "loss": 1.8718,
      "step": 26000
    },
    {
      "epoch": 0.34871632913557826,
      "grad_norm": 2.565263032913208,
      "learning_rate": 9.890771329792167e-05,
      "loss": 1.8551,
      "step": 26500
    },
    {
      "epoch": 0.3552958825154948,
      "grad_norm": 2.3705434799194336,
      "learning_rate": 9.887459764481476e-05,
      "loss": 1.8422,
      "step": 27000
    },
    {
      "epoch": 0.36187543589541143,
      "grad_norm": 2.4775021076202393,
      "learning_rate": 9.884148199170784e-05,
      "loss": 1.84,
      "step": 27500
    },
    {
      "epoch": 0.368454989275328,
      "grad_norm": 2.8447675704956055,
      "learning_rate": 9.880843256990715e-05,
      "loss": 1.8369,
      "step": 28000
    },
    {
      "epoch": 0.37503454265524455,
      "grad_norm": 2.6742067337036133,
      "learning_rate": 9.877531691680025e-05,
      "loss": 1.835,
      "step": 28500
    },
    {
      "epoch": 0.3816140960351611,
      "grad_norm": 2.7340962886810303,
      "learning_rate": 9.874220126369332e-05,
      "loss": 1.8231,
      "step": 29000
    },
    {
      "epoch": 0.3881936494150777,
      "grad_norm": 2.4240806102752686,
      "learning_rate": 9.870908561058642e-05,
      "loss": 1.8178,
      "step": 29500
    },
    {
      "epoch": 0.3947732027949943,
      "grad_norm": 2.677680015563965,
      "learning_rate": 9.867603618878572e-05,
      "loss": 1.8057,
      "step": 30000
    },
    {
      "epoch": 0.40135275617491084,
      "grad_norm": 2.5074996948242188,
      "learning_rate": 9.86429205356788e-05,
      "loss": 1.804,
      "step": 30500
    },
    {
      "epoch": 0.4079323095548274,
      "grad_norm": 2.8356595039367676,
      "learning_rate": 9.86098048825719e-05,
      "loss": 1.7902,
      "step": 31000
    },
    {
      "epoch": 0.414511862934744,
      "grad_norm": 2.581878662109375,
      "learning_rate": 9.857668922946499e-05,
      "loss": 1.7897,
      "step": 31500
    },
    {
      "epoch": 0.42109141631466057,
      "grad_norm": 2.630223274230957,
      "learning_rate": 9.85436398076643e-05,
      "loss": 1.7788,
      "step": 32000
    },
    {
      "epoch": 0.42767096969457713,
      "grad_norm": 2.614901542663574,
      "learning_rate": 9.851052415455738e-05,
      "loss": 1.7886,
      "step": 32500
    },
    {
      "epoch": 0.4342505230744937,
      "grad_norm": 2.818523406982422,
      "learning_rate": 9.847740850145048e-05,
      "loss": 1.7667,
      "step": 33000
    },
    {
      "epoch": 0.4408300764544103,
      "grad_norm": 2.6949453353881836,
      "learning_rate": 9.844429284834355e-05,
      "loss": 1.7614,
      "step": 33500
    },
    {
      "epoch": 0.44740962983432686,
      "grad_norm": 2.7326786518096924,
      "learning_rate": 9.841124342654287e-05,
      "loss": 1.7541,
      "step": 34000
    },
    {
      "epoch": 0.4539891832142434,
      "grad_norm": 2.591294288635254,
      "learning_rate": 9.837812777343596e-05,
      "loss": 1.7591,
      "step": 34500
    },
    {
      "epoch": 0.46056873659416,
      "grad_norm": 2.5655293464660645,
      "learning_rate": 9.834501212032904e-05,
      "loss": 1.7418,
      "step": 35000
    },
    {
      "epoch": 0.46714828997407654,
      "grad_norm": 2.489513635635376,
      "learning_rate": 9.831189646722213e-05,
      "loss": 1.7324,
      "step": 35500
    },
    {
      "epoch": 0.47372784335399315,
      "grad_norm": 2.758307933807373,
      "learning_rate": 9.827884704542143e-05,
      "loss": 1.7222,
      "step": 36000
    },
    {
      "epoch": 0.4803073967339097,
      "grad_norm": 2.526388645172119,
      "learning_rate": 9.824573139231452e-05,
      "loss": 1.7251,
      "step": 36500
    },
    {
      "epoch": 0.48688695011382627,
      "grad_norm": 2.575160503387451,
      "learning_rate": 9.821261573920761e-05,
      "loss": 1.7306,
      "step": 37000
    },
    {
      "epoch": 0.4934665034937428,
      "grad_norm": 2.6585066318511963,
      "learning_rate": 9.81795000861007e-05,
      "loss": 1.7211,
      "step": 37500
    },
    {
      "epoch": 0.5000460568736594,
      "grad_norm": 2.514080047607422,
      "learning_rate": 9.81464506643e-05,
      "loss": 1.7054,
      "step": 38000
    },
    {
      "epoch": 0.506625610253576,
      "grad_norm": 2.4176418781280518,
      "learning_rate": 9.81133350111931e-05,
      "loss": 1.7052,
      "step": 38500
    },
    {
      "epoch": 0.5132051636334926,
      "grad_norm": 2.7782368659973145,
      "learning_rate": 9.808021935808619e-05,
      "loss": 1.6999,
      "step": 39000
    },
    {
      "epoch": 0.5197847170134091,
      "grad_norm": 2.5960333347320557,
      "learning_rate": 9.804710370497927e-05,
      "loss": 1.6846,
      "step": 39500
    },
    {
      "epoch": 0.5263642703933257,
      "grad_norm": 2.558537721633911,
      "learning_rate": 9.801405428317858e-05,
      "loss": 1.6846,
      "step": 40000
    },
    {
      "epoch": 0.5329438237732422,
      "grad_norm": 2.9697511196136475,
      "learning_rate": 9.798093863007168e-05,
      "loss": 1.6739,
      "step": 40500
    },
    {
      "epoch": 0.5395233771531588,
      "grad_norm": 2.5659332275390625,
      "learning_rate": 9.794782297696475e-05,
      "loss": 1.6779,
      "step": 41000
    },
    {
      "epoch": 0.5461029305330755,
      "grad_norm": 2.670562982559204,
      "learning_rate": 9.791470732385785e-05,
      "loss": 1.669,
      "step": 41500
    },
    {
      "epoch": 0.552682483912992,
      "grad_norm": 2.676208972930908,
      "learning_rate": 9.788165790205714e-05,
      "loss": 1.6672,
      "step": 42000
    },
    {
      "epoch": 0.5592620372929086,
      "grad_norm": 2.7387876510620117,
      "learning_rate": 9.784854224895024e-05,
      "loss": 1.6684,
      "step": 42500
    },
    {
      "epoch": 0.5658415906728251,
      "grad_norm": 2.6045238971710205,
      "learning_rate": 9.781542659584332e-05,
      "loss": 1.6672,
      "step": 43000
    },
    {
      "epoch": 0.5724211440527417,
      "grad_norm": 2.3740663528442383,
      "learning_rate": 9.778231094273642e-05,
      "loss": 1.6556,
      "step": 43500
    },
    {
      "epoch": 0.5790006974326583,
      "grad_norm": 2.443227529525757,
      "learning_rate": 9.774926152093571e-05,
      "loss": 1.6512,
      "step": 44000
    },
    {
      "epoch": 0.5855802508125748,
      "grad_norm": 2.7779996395111084,
      "learning_rate": 9.771614586782881e-05,
      "loss": 1.6438,
      "step": 44500
    },
    {
      "epoch": 0.5921598041924914,
      "grad_norm": 2.660731315612793,
      "learning_rate": 9.76830302147219e-05,
      "loss": 1.6457,
      "step": 45000
    },
    {
      "epoch": 0.598739357572408,
      "grad_norm": 2.543558359146118,
      "learning_rate": 9.764991456161498e-05,
      "loss": 1.6449,
      "step": 45500
    },
    {
      "epoch": 0.6053189109523246,
      "grad_norm": 3.085667848587036,
      "learning_rate": 9.761686513981429e-05,
      "loss": 1.6394,
      "step": 46000
    },
    {
      "epoch": 0.6118984643322412,
      "grad_norm": 2.5786473751068115,
      "learning_rate": 9.758374948670737e-05,
      "loss": 1.6342,
      "step": 46500
    },
    {
      "epoch": 0.6184780177121577,
      "grad_norm": 2.6644365787506104,
      "learning_rate": 9.755063383360047e-05,
      "loss": 1.643,
      "step": 47000
    },
    {
      "epoch": 0.6250575710920743,
      "grad_norm": 2.7152504920959473,
      "learning_rate": 9.751751818049356e-05,
      "loss": 1.622,
      "step": 47500
    },
    {
      "epoch": 0.6316371244719908,
      "grad_norm": 2.3618733882904053,
      "learning_rate": 9.748440252738666e-05,
      "loss": 1.6172,
      "step": 48000
    },
    {
      "epoch": 0.6382166778519074,
      "grad_norm": 2.5592072010040283,
      "learning_rate": 9.745128687427973e-05,
      "loss": 1.6303,
      "step": 48500
    },
    {
      "epoch": 0.644796231231824,
      "grad_norm": 2.577571153640747,
      "learning_rate": 9.741823745247905e-05,
      "loss": 1.6111,
      "step": 49000
    },
    {
      "epoch": 0.6513757846117405,
      "grad_norm": 2.6548919677734375,
      "learning_rate": 9.738512179937213e-05,
      "loss": 1.6163,
      "step": 49500
    },
    {
      "epoch": 0.6579553379916572,
      "grad_norm": 2.448660135269165,
      "learning_rate": 9.735200614626522e-05,
      "loss": 1.6023,
      "step": 50000
    },
    {
      "epoch": 0.6645348913715737,
      "grad_norm": 2.6322059631347656,
      "learning_rate": 9.731889049315832e-05,
      "loss": 1.6112,
      "step": 50500
    },
    {
      "epoch": 0.6711144447514903,
      "grad_norm": 2.568981885910034,
      "learning_rate": 9.728584107135761e-05,
      "loss": 1.6083,
      "step": 51000
    },
    {
      "epoch": 0.6776939981314068,
      "grad_norm": 2.624885082244873,
      "learning_rate": 9.725272541825071e-05,
      "loss": 1.6092,
      "step": 51500
    },
    {
      "epoch": 0.6842735515113234,
      "grad_norm": 2.4277708530426025,
      "learning_rate": 9.721960976514379e-05,
      "loss": 1.5993,
      "step": 52000
    },
    {
      "epoch": 0.69085310489124,
      "grad_norm": 2.6746268272399902,
      "learning_rate": 9.718649411203689e-05,
      "loss": 1.5834,
      "step": 52500
    },
    {
      "epoch": 0.6974326582711565,
      "grad_norm": 2.537621259689331,
      "learning_rate": 9.715344469023618e-05,
      "loss": 1.5853,
      "step": 53000
    },
    {
      "epoch": 0.7040122116510731,
      "grad_norm": 2.833770275115967,
      "learning_rate": 9.712032903712928e-05,
      "loss": 1.5861,
      "step": 53500
    },
    {
      "epoch": 0.7105917650309896,
      "grad_norm": 2.5488831996917725,
      "learning_rate": 9.708721338402237e-05,
      "loss": 1.5782,
      "step": 54000
    },
    {
      "epoch": 0.7171713184109063,
      "grad_norm": 2.37758207321167,
      "learning_rate": 9.705409773091545e-05,
      "loss": 1.5952,
      "step": 54500
    },
    {
      "epoch": 0.7237508717908229,
      "grad_norm": 2.6457512378692627,
      "learning_rate": 9.702104830911476e-05,
      "loss": 1.5899,
      "step": 55000
    },
    {
      "epoch": 0.7303304251707394,
      "grad_norm": 2.543125867843628,
      "learning_rate": 9.698793265600786e-05,
      "loss": 1.5731,
      "step": 55500
    },
    {
      "epoch": 0.736909978550656,
      "grad_norm": 2.7919082641601562,
      "learning_rate": 9.695481700290093e-05,
      "loss": 1.569,
      "step": 56000
    },
    {
      "epoch": 0.7434895319305725,
      "grad_norm": 2.2876229286193848,
      "learning_rate": 9.692170134979403e-05,
      "loss": 1.5806,
      "step": 56500
    },
    {
      "epoch": 0.7500690853104891,
      "grad_norm": 2.528977632522583,
      "learning_rate": 9.688858569668711e-05,
      "loss": 1.5768,
      "step": 57000
    },
    {
      "epoch": 0.7566486386904057,
      "grad_norm": 2.6582412719726562,
      "learning_rate": 9.685553627488642e-05,
      "loss": 1.5711,
      "step": 57500
    },
    {
      "epoch": 0.7632281920703222,
      "grad_norm": 2.4359960556030273,
      "learning_rate": 9.682248685308572e-05,
      "loss": 1.5612,
      "step": 58000
    },
    {
      "epoch": 0.7698077454502389,
      "grad_norm": 2.546227216720581,
      "learning_rate": 9.67893711999788e-05,
      "loss": 1.5625,
      "step": 58500
    },
    {
      "epoch": 0.7763872988301554,
      "grad_norm": 2.848630428314209,
      "learning_rate": 9.67562555468719e-05,
      "loss": 1.5617,
      "step": 59000
    },
    {
      "epoch": 0.782966852210072,
      "grad_norm": 2.527367353439331,
      "learning_rate": 9.672313989376499e-05,
      "loss": 1.5537,
      "step": 59500
    },
    {
      "epoch": 0.7895464055899886,
      "grad_norm": 2.4462690353393555,
      "learning_rate": 9.669002424065809e-05,
      "loss": 1.5477,
      "step": 60000
    },
    {
      "epoch": 0.7961259589699051,
      "grad_norm": 2.6439120769500732,
      "learning_rate": 9.665690858755116e-05,
      "loss": 1.5507,
      "step": 60500
    },
    {
      "epoch": 0.8027055123498217,
      "grad_norm": 2.599815845489502,
      "learning_rate": 9.662379293444426e-05,
      "loss": 1.5577,
      "step": 61000
    },
    {
      "epoch": 0.8092850657297382,
      "grad_norm": 4.1163506507873535,
      "learning_rate": 9.659067728133735e-05,
      "loss": 1.5402,
      "step": 61500
    },
    {
      "epoch": 0.8158646191096548,
      "grad_norm": 2.3943114280700684,
      "learning_rate": 9.655762785953665e-05,
      "loss": 1.5343,
      "step": 62000
    },
    {
      "epoch": 0.8224441724895714,
      "grad_norm": 2.731081247329712,
      "learning_rate": 9.652451220642974e-05,
      "loss": 1.543,
      "step": 62500
    },
    {
      "epoch": 0.829023725869488,
      "grad_norm": 2.45835280418396,
      "learning_rate": 9.649139655332283e-05,
      "loss": 1.5422,
      "step": 63000
    },
    {
      "epoch": 0.8356032792494046,
      "grad_norm": 2.6275525093078613,
      "learning_rate": 9.645828090021592e-05,
      "loss": 1.5392,
      "step": 63500
    },
    {
      "epoch": 0.8421828326293211,
      "grad_norm": 2.5304453372955322,
      "learning_rate": 9.642523147841522e-05,
      "loss": 1.5361,
      "step": 64000
    },
    {
      "epoch": 0.8487623860092377,
      "grad_norm": 2.66290020942688,
      "learning_rate": 9.639211582530831e-05,
      "loss": 1.5291,
      "step": 64500
    },
    {
      "epoch": 0.8553419393891543,
      "grad_norm": 2.6809182167053223,
      "learning_rate": 9.63590001722014e-05,
      "loss": 1.5327,
      "step": 65000
    },
    {
      "epoch": 0.8619214927690708,
      "grad_norm": 2.6487650871276855,
      "learning_rate": 9.63258845190945e-05,
      "loss": 1.5336,
      "step": 65500
    },
    {
      "epoch": 0.8685010461489874,
      "grad_norm": 2.648463487625122,
      "learning_rate": 9.62928350972938e-05,
      "loss": 1.5269,
      "step": 66000
    },
    {
      "epoch": 0.8750805995289039,
      "grad_norm": 2.3581769466400146,
      "learning_rate": 9.625971944418688e-05,
      "loss": 1.5167,
      "step": 66500
    },
    {
      "epoch": 0.8816601529088206,
      "grad_norm": 2.7865593433380127,
      "learning_rate": 9.622660379107997e-05,
      "loss": 1.5239,
      "step": 67000
    },
    {
      "epoch": 0.8882397062887372,
      "grad_norm": 2.4453885555267334,
      "learning_rate": 9.619348813797307e-05,
      "loss": 1.524,
      "step": 67500
    },
    {
      "epoch": 0.8948192596686537,
      "grad_norm": 2.416538953781128,
      "learning_rate": 9.616043871617236e-05,
      "loss": 1.5155,
      "step": 68000
    },
    {
      "epoch": 0.9013988130485703,
      "grad_norm": 2.344197988510132,
      "learning_rate": 9.612732306306546e-05,
      "loss": 1.5205,
      "step": 68500
    },
    {
      "epoch": 0.9079783664284868,
      "grad_norm": 2.5111892223358154,
      "learning_rate": 9.609420740995854e-05,
      "loss": 1.5039,
      "step": 69000
    },
    {
      "epoch": 0.9145579198084034,
      "grad_norm": 2.490450143814087,
      "learning_rate": 9.606109175685163e-05,
      "loss": 1.505,
      "step": 69500
    },
    {
      "epoch": 0.92113747318832,
      "grad_norm": 2.741502046585083,
      "learning_rate": 9.602804233505093e-05,
      "loss": 1.5032,
      "step": 70000
    },
    {
      "epoch": 0.9277170265682365,
      "grad_norm": 2.4418699741363525,
      "learning_rate": 9.599492668194403e-05,
      "loss": 1.4999,
      "step": 70500
    },
    {
      "epoch": 0.9342965799481531,
      "grad_norm": 2.313133716583252,
      "learning_rate": 9.596181102883712e-05,
      "loss": 1.5031,
      "step": 71000
    },
    {
      "epoch": 0.9408761333280697,
      "grad_norm": 2.509791851043701,
      "learning_rate": 9.59286953757302e-05,
      "loss": 1.5154,
      "step": 71500
    },
    {
      "epoch": 0.9474556867079863,
      "grad_norm": 2.6315622329711914,
      "learning_rate": 9.589564595392951e-05,
      "loss": 1.5047,
      "step": 72000
    },
    {
      "epoch": 0.9540352400879029,
      "grad_norm": 2.643143653869629,
      "learning_rate": 9.58625303008226e-05,
      "loss": 1.502,
      "step": 72500
    },
    {
      "epoch": 0.9606147934678194,
      "grad_norm": 2.3634567260742188,
      "learning_rate": 9.582941464771569e-05,
      "loss": 1.4961,
      "step": 73000
    },
    {
      "epoch": 0.967194346847736,
      "grad_norm": 2.4076874256134033,
      "learning_rate": 9.579629899460878e-05,
      "loss": 1.4888,
      "step": 73500
    },
    {
      "epoch": 0.9737739002276525,
      "grad_norm": 2.5316977500915527,
      "learning_rate": 9.576318334150186e-05,
      "loss": 1.5042,
      "step": 74000
    },
    {
      "epoch": 0.9803534536075691,
      "grad_norm": 2.587129831314087,
      "learning_rate": 9.573006768839495e-05,
      "loss": 1.4837,
      "step": 74500
    },
    {
      "epoch": 0.9869330069874857,
      "grad_norm": 2.542154550552368,
      "learning_rate": 9.569701826659427e-05,
      "loss": 1.4896,
      "step": 75000
    },
    {
      "epoch": 0.9935125603674022,
      "grad_norm": 2.4580047130584717,
      "learning_rate": 9.566390261348734e-05,
      "loss": 1.4869,
      "step": 75500
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.414989709854126,
      "eval_runtime": 49.5503,
      "eval_samples_per_second": 2018.152,
      "eval_steps_per_second": 15.782,
      "step": 75993
    },
    {
      "epoch": 1.0000921137473189,
      "grad_norm": 2.6691248416900635,
      "learning_rate": 9.563078696038044e-05,
      "loss": 1.4799,
      "step": 76000
    },
    {
      "epoch": 1.0066716671272353,
      "grad_norm": 2.6922378540039062,
      "learning_rate": 9.559767130727352e-05,
      "loss": 1.4742,
      "step": 76500
    },
    {
      "epoch": 1.013251220507152,
      "grad_norm": 3.2475178241729736,
      "learning_rate": 9.556462188547283e-05,
      "loss": 1.4883,
      "step": 77000
    },
    {
      "epoch": 1.0198307738870684,
      "grad_norm": 2.691763162612915,
      "learning_rate": 9.553150623236591e-05,
      "loss": 1.4806,
      "step": 77500
    },
    {
      "epoch": 1.0264103272669851,
      "grad_norm": 2.492419958114624,
      "learning_rate": 9.549839057925901e-05,
      "loss": 1.4778,
      "step": 78000
    },
    {
      "epoch": 1.0329898806469018,
      "grad_norm": 2.347482442855835,
      "learning_rate": 9.54652749261521e-05,
      "loss": 1.4696,
      "step": 78500
    },
    {
      "epoch": 1.0395694340268182,
      "grad_norm": 2.419389486312866,
      "learning_rate": 9.54322255043514e-05,
      "loss": 1.4754,
      "step": 79000
    },
    {
      "epoch": 1.046148987406735,
      "grad_norm": 2.3591506481170654,
      "learning_rate": 9.53991098512445e-05,
      "loss": 1.479,
      "step": 79500
    },
    {
      "epoch": 1.0527285407866513,
      "grad_norm": 2.399709463119507,
      "learning_rate": 9.536599419813757e-05,
      "loss": 1.4686,
      "step": 80000
    },
    {
      "epoch": 1.059308094166568,
      "grad_norm": 2.5940134525299072,
      "learning_rate": 9.533287854503067e-05,
      "loss": 1.4637,
      "step": 80500
    },
    {
      "epoch": 1.0658876475464845,
      "grad_norm": 2.4470818042755127,
      "learning_rate": 9.529982912322998e-05,
      "loss": 1.4619,
      "step": 81000
    },
    {
      "epoch": 1.0724672009264011,
      "grad_norm": 2.541896343231201,
      "learning_rate": 9.526671347012306e-05,
      "loss": 1.4597,
      "step": 81500
    },
    {
      "epoch": 1.0790467543063178,
      "grad_norm": 2.530148983001709,
      "learning_rate": 9.523359781701615e-05,
      "loss": 1.461,
      "step": 82000
    },
    {
      "epoch": 1.0856263076862342,
      "grad_norm": 2.2642643451690674,
      "learning_rate": 9.520048216390925e-05,
      "loss": 1.4595,
      "step": 82500
    },
    {
      "epoch": 1.092205861066151,
      "grad_norm": 2.565683603286743,
      "learning_rate": 9.516743274210854e-05,
      "loss": 1.4696,
      "step": 83000
    },
    {
      "epoch": 1.0987854144460674,
      "grad_norm": 2.6461660861968994,
      "learning_rate": 9.513431708900164e-05,
      "loss": 1.453,
      "step": 83500
    },
    {
      "epoch": 1.105364967825984,
      "grad_norm": 2.382054567337036,
      "learning_rate": 9.510120143589472e-05,
      "loss": 1.4561,
      "step": 84000
    },
    {
      "epoch": 1.1119445212059005,
      "grad_norm": 2.384202003479004,
      "learning_rate": 9.50680857827878e-05,
      "loss": 1.4631,
      "step": 84500
    },
    {
      "epoch": 1.1185240745858172,
      "grad_norm": 2.4374611377716064,
      "learning_rate": 9.50349701296809e-05,
      "loss": 1.4521,
      "step": 85000
    },
    {
      "epoch": 1.1251036279657336,
      "grad_norm": 2.6281235218048096,
      "learning_rate": 9.500192070788021e-05,
      "loss": 1.4396,
      "step": 85500
    },
    {
      "epoch": 1.1316831813456503,
      "grad_norm": 2.457064151763916,
      "learning_rate": 9.49688050547733e-05,
      "loss": 1.4548,
      "step": 86000
    },
    {
      "epoch": 1.1382627347255667,
      "grad_norm": 2.361156463623047,
      "learning_rate": 9.493568940166638e-05,
      "loss": 1.4533,
      "step": 86500
    },
    {
      "epoch": 1.1448422881054834,
      "grad_norm": 2.2817726135253906,
      "learning_rate": 9.490257374855948e-05,
      "loss": 1.4446,
      "step": 87000
    },
    {
      "epoch": 1.1514218414854,
      "grad_norm": 2.3340413570404053,
      "learning_rate": 9.486952432675877e-05,
      "loss": 1.4459,
      "step": 87500
    },
    {
      "epoch": 1.1580013948653165,
      "grad_norm": 2.4980716705322266,
      "learning_rate": 9.483640867365187e-05,
      "loss": 1.4439,
      "step": 88000
    },
    {
      "epoch": 1.1645809482452332,
      "grad_norm": 2.4675135612487793,
      "learning_rate": 9.480329302054496e-05,
      "loss": 1.4497,
      "step": 88500
    },
    {
      "epoch": 1.1711605016251496,
      "grad_norm": 2.5558524131774902,
      "learning_rate": 9.477017736743804e-05,
      "loss": 1.4461,
      "step": 89000
    },
    {
      "epoch": 1.1777400550050663,
      "grad_norm": 2.472127914428711,
      "learning_rate": 9.473706171433113e-05,
      "loss": 1.4448,
      "step": 89500
    },
    {
      "epoch": 1.1843196083849827,
      "grad_norm": 2.3717501163482666,
      "learning_rate": 9.470401229253044e-05,
      "loss": 1.4324,
      "step": 90000
    },
    {
      "epoch": 1.1908991617648994,
      "grad_norm": 2.469369888305664,
      "learning_rate": 9.467089663942353e-05,
      "loss": 1.4448,
      "step": 90500
    },
    {
      "epoch": 1.197478715144816,
      "grad_norm": 2.335817813873291,
      "learning_rate": 9.463778098631661e-05,
      "loss": 1.4411,
      "step": 91000
    },
    {
      "epoch": 1.2040582685247325,
      "grad_norm": 2.6975557804107666,
      "learning_rate": 9.460466533320971e-05,
      "loss": 1.436,
      "step": 91500
    },
    {
      "epoch": 1.2106378219046492,
      "grad_norm": 2.4308981895446777,
      "learning_rate": 9.45715496801028e-05,
      "loss": 1.4245,
      "step": 92000
    },
    {
      "epoch": 1.2172173752845656,
      "grad_norm": 2.6690714359283447,
      "learning_rate": 9.45385002583021e-05,
      "loss": 1.4353,
      "step": 92500
    },
    {
      "epoch": 1.2237969286644823,
      "grad_norm": 2.493798017501831,
      "learning_rate": 9.450538460519519e-05,
      "loss": 1.4292,
      "step": 93000
    },
    {
      "epoch": 1.2303764820443988,
      "grad_norm": 2.5376861095428467,
      "learning_rate": 9.447226895208829e-05,
      "loss": 1.4307,
      "step": 93500
    },
    {
      "epoch": 1.2369560354243154,
      "grad_norm": 2.642374277114868,
      "learning_rate": 9.443915329898136e-05,
      "loss": 1.4255,
      "step": 94000
    },
    {
      "epoch": 1.2435355888042319,
      "grad_norm": 2.7117414474487305,
      "learning_rate": 9.440603764587446e-05,
      "loss": 1.4233,
      "step": 94500
    },
    {
      "epoch": 1.2501151421841485,
      "grad_norm": 2.6943211555480957,
      "learning_rate": 9.437298822407375e-05,
      "loss": 1.4314,
      "step": 95000
    },
    {
      "epoch": 1.256694695564065,
      "grad_norm": 2.3873891830444336,
      "learning_rate": 9.433987257096685e-05,
      "loss": 1.4344,
      "step": 95500
    },
    {
      "epoch": 1.2632742489439817,
      "grad_norm": 2.4404735565185547,
      "learning_rate": 9.430675691785993e-05,
      "loss": 1.4246,
      "step": 96000
    },
    {
      "epoch": 1.2698538023238983,
      "grad_norm": 2.4862170219421387,
      "learning_rate": 9.427364126475303e-05,
      "loss": 1.4229,
      "step": 96500
    },
    {
      "epoch": 1.2764333557038148,
      "grad_norm": 2.6861891746520996,
      "learning_rate": 9.424059184295232e-05,
      "loss": 1.418,
      "step": 97000
    },
    {
      "epoch": 1.2830129090837314,
      "grad_norm": 2.414257287979126,
      "learning_rate": 9.420747618984542e-05,
      "loss": 1.4135,
      "step": 97500
    },
    {
      "epoch": 1.289592462463648,
      "grad_norm": 2.4285991191864014,
      "learning_rate": 9.417436053673851e-05,
      "loss": 1.4149,
      "step": 98000
    },
    {
      "epoch": 1.2961720158435646,
      "grad_norm": 2.1458792686462402,
      "learning_rate": 9.41412448836316e-05,
      "loss": 1.4246,
      "step": 98500
    },
    {
      "epoch": 1.302751569223481,
      "grad_norm": 2.3326683044433594,
      "learning_rate": 9.410812923052469e-05,
      "loss": 1.4209,
      "step": 99000
    },
    {
      "epoch": 1.3093311226033977,
      "grad_norm": 2.713840961456299,
      "learning_rate": 9.407507980872398e-05,
      "loss": 1.4124,
      "step": 99500
    },
    {
      "epoch": 1.3159106759833143,
      "grad_norm": 2.3583970069885254,
      "learning_rate": 9.404196415561708e-05,
      "loss": 1.4085,
      "step": 100000
    },
    {
      "epoch": 1.3224902293632308,
      "grad_norm": 2.4967503547668457,
      "learning_rate": 9.400891473381639e-05,
      "loss": 1.414,
      "step": 100500
    },
    {
      "epoch": 1.3290697827431475,
      "grad_norm": 2.6298279762268066,
      "learning_rate": 9.397579908070947e-05,
      "loss": 1.4117,
      "step": 101000
    },
    {
      "epoch": 1.335649336123064,
      "grad_norm": 2.4570376873016357,
      "learning_rate": 9.394268342760256e-05,
      "loss": 1.4216,
      "step": 101500
    },
    {
      "epoch": 1.3422288895029806,
      "grad_norm": 2.7450883388519287,
      "learning_rate": 9.390956777449566e-05,
      "loss": 1.4163,
      "step": 102000
    },
    {
      "epoch": 1.348808442882897,
      "grad_norm": 2.127713441848755,
      "learning_rate": 9.387645212138874e-05,
      "loss": 1.4099,
      "step": 102500
    },
    {
      "epoch": 1.3553879962628137,
      "grad_norm": 2.3652710914611816,
      "learning_rate": 9.384333646828183e-05,
      "loss": 1.3933,
      "step": 103000
    },
    {
      "epoch": 1.3619675496427304,
      "grad_norm": 2.6360085010528564,
      "learning_rate": 9.381022081517493e-05,
      "loss": 1.4026,
      "step": 103500
    },
    {
      "epoch": 1.3685471030226468,
      "grad_norm": 2.5368905067443848,
      "learning_rate": 9.377710516206801e-05,
      "loss": 1.4199,
      "step": 104000
    },
    {
      "epoch": 1.3751266564025633,
      "grad_norm": 2.4265260696411133,
      "learning_rate": 9.374398950896111e-05,
      "loss": 1.3974,
      "step": 104500
    },
    {
      "epoch": 1.38170620978248,
      "grad_norm": 2.455392837524414,
      "learning_rate": 9.371087385585418e-05,
      "loss": 1.4085,
      "step": 105000
    },
    {
      "epoch": 1.3882857631623966,
      "grad_norm": 2.3982129096984863,
      "learning_rate": 9.367775820274728e-05,
      "loss": 1.3964,
      "step": 105500
    },
    {
      "epoch": 1.394865316542313,
      "grad_norm": 2.4132423400878906,
      "learning_rate": 9.364470878094657e-05,
      "loss": 1.4078,
      "step": 106000
    },
    {
      "epoch": 1.4014448699222297,
      "grad_norm": 2.558497190475464,
      "learning_rate": 9.361159312783967e-05,
      "loss": 1.3943,
      "step": 106500
    },
    {
      "epoch": 1.4080244233021464,
      "grad_norm": 2.499019145965576,
      "learning_rate": 9.357847747473276e-05,
      "loss": 1.4048,
      "step": 107000
    },
    {
      "epoch": 1.4146039766820628,
      "grad_norm": 2.501918315887451,
      "learning_rate": 9.354536182162586e-05,
      "loss": 1.3873,
      "step": 107500
    },
    {
      "epoch": 1.4211835300619793,
      "grad_norm": 2.671891927719116,
      "learning_rate": 9.351224616851894e-05,
      "loss": 1.4003,
      "step": 108000
    },
    {
      "epoch": 1.427763083441896,
      "grad_norm": 2.769195795059204,
      "learning_rate": 9.347926297802447e-05,
      "loss": 1.39,
      "step": 108500
    },
    {
      "epoch": 1.4343426368218126,
      "grad_norm": 2.4446723461151123,
      "learning_rate": 9.344614732491754e-05,
      "loss": 1.3967,
      "step": 109000
    },
    {
      "epoch": 1.440922190201729,
      "grad_norm": 2.5340118408203125,
      "learning_rate": 9.341303167181064e-05,
      "loss": 1.3942,
      "step": 109500
    },
    {
      "epoch": 1.4475017435816457,
      "grad_norm": 2.54831600189209,
      "learning_rate": 9.337998225000993e-05,
      "loss": 1.3952,
      "step": 110000
    },
    {
      "epoch": 1.4540812969615622,
      "grad_norm": 2.5750696659088135,
      "learning_rate": 9.334686659690303e-05,
      "loss": 1.3842,
      "step": 110500
    },
    {
      "epoch": 1.4606608503414789,
      "grad_norm": 2.3987033367156982,
      "learning_rate": 9.331375094379613e-05,
      "loss": 1.3843,
      "step": 111000
    },
    {
      "epoch": 1.4672404037213953,
      "grad_norm": 2.4515678882598877,
      "learning_rate": 9.328063529068921e-05,
      "loss": 1.3899,
      "step": 111500
    },
    {
      "epoch": 1.473819957101312,
      "grad_norm": 2.5773353576660156,
      "learning_rate": 9.32475196375823e-05,
      "loss": 1.3954,
      "step": 112000
    },
    {
      "epoch": 1.4803995104812286,
      "grad_norm": 2.357548236846924,
      "learning_rate": 9.321440398447538e-05,
      "loss": 1.3905,
      "step": 112500
    },
    {
      "epoch": 1.486979063861145,
      "grad_norm": 2.602388381958008,
      "learning_rate": 9.318128833136848e-05,
      "loss": 1.3886,
      "step": 113000
    },
    {
      "epoch": 1.4935586172410618,
      "grad_norm": 3.421190023422241,
      "learning_rate": 9.314817267826157e-05,
      "loss": 1.3724,
      "step": 113500
    },
    {
      "epoch": 1.5001381706209782,
      "grad_norm": 2.5649163722991943,
      "learning_rate": 9.311505702515465e-05,
      "loss": 1.3914,
      "step": 114000
    },
    {
      "epoch": 1.5067177240008949,
      "grad_norm": 2.367245674133301,
      "learning_rate": 9.308194137204775e-05,
      "loss": 1.3795,
      "step": 114500
    },
    {
      "epoch": 1.5132972773808113,
      "grad_norm": 2.7248499393463135,
      "learning_rate": 9.304882571894083e-05,
      "loss": 1.3793,
      "step": 115000
    },
    {
      "epoch": 1.519876830760728,
      "grad_norm": 2.872533082962036,
      "learning_rate": 9.301571006583393e-05,
      "loss": 1.3819,
      "step": 115500
    },
    {
      "epoch": 1.5264563841406447,
      "grad_norm": 2.173344135284424,
      "learning_rate": 9.298266064403322e-05,
      "loss": 1.3885,
      "step": 116000
    },
    {
      "epoch": 1.533035937520561,
      "grad_norm": 2.8060801029205322,
      "learning_rate": 9.294954499092632e-05,
      "loss": 1.3837,
      "step": 116500
    },
    {
      "epoch": 1.5396154909004776,
      "grad_norm": 2.6260898113250732,
      "learning_rate": 9.29164293378194e-05,
      "loss": 1.3752,
      "step": 117000
    },
    {
      "epoch": 1.5461950442803942,
      "grad_norm": 2.7163803577423096,
      "learning_rate": 9.28833136847125e-05,
      "loss": 1.3765,
      "step": 117500
    },
    {
      "epoch": 1.552774597660311,
      "grad_norm": 2.704744815826416,
      "learning_rate": 9.285019803160558e-05,
      "loss": 1.3823,
      "step": 118000
    },
    {
      "epoch": 1.5593541510402273,
      "grad_norm": 2.4068174362182617,
      "learning_rate": 9.281708237849868e-05,
      "loss": 1.3749,
      "step": 118500
    },
    {
      "epoch": 1.565933704420144,
      "grad_norm": 2.439023733139038,
      "learning_rate": 9.278396672539176e-05,
      "loss": 1.389,
      "step": 119000
    },
    {
      "epoch": 1.5725132578000607,
      "grad_norm": 2.718893527984619,
      "learning_rate": 9.275085107228485e-05,
      "loss": 1.3703,
      "step": 119500
    },
    {
      "epoch": 1.5790928111799771,
      "grad_norm": 2.5925958156585693,
      "learning_rate": 9.271773541917795e-05,
      "loss": 1.3796,
      "step": 120000
    },
    {
      "epoch": 1.5856723645598936,
      "grad_norm": 2.2720694541931152,
      "learning_rate": 9.268468599737724e-05,
      "loss": 1.3623,
      "step": 120500
    },
    {
      "epoch": 1.5922519179398102,
      "grad_norm": 2.560147285461426,
      "learning_rate": 9.265157034427034e-05,
      "loss": 1.3837,
      "step": 121000
    },
    {
      "epoch": 1.598831471319727,
      "grad_norm": 2.442469596862793,
      "learning_rate": 9.261845469116342e-05,
      "loss": 1.3777,
      "step": 121500
    },
    {
      "epoch": 1.6054110246996434,
      "grad_norm": 2.4116127490997314,
      "learning_rate": 9.258533903805652e-05,
      "loss": 1.3575,
      "step": 122000
    },
    {
      "epoch": 1.6119905780795598,
      "grad_norm": 2.5328073501586914,
      "learning_rate": 9.25522233849496e-05,
      "loss": 1.3751,
      "step": 122500
    },
    {
      "epoch": 1.6185701314594765,
      "grad_norm": 2.2568647861480713,
      "learning_rate": 9.251917396314891e-05,
      "loss": 1.3754,
      "step": 123000
    },
    {
      "epoch": 1.6251496848393931,
      "grad_norm": 2.4399757385253906,
      "learning_rate": 9.248605831004198e-05,
      "loss": 1.363,
      "step": 123500
    },
    {
      "epoch": 1.6317292382193096,
      "grad_norm": 2.5836904048919678,
      "learning_rate": 9.245294265693508e-05,
      "loss": 1.3692,
      "step": 124000
    },
    {
      "epoch": 1.6383087915992263,
      "grad_norm": 2.796750545501709,
      "learning_rate": 9.241982700382817e-05,
      "loss": 1.3646,
      "step": 124500
    },
    {
      "epoch": 1.644888344979143,
      "grad_norm": 2.5795059204101562,
      "learning_rate": 9.238671135072127e-05,
      "loss": 1.3626,
      "step": 125000
    },
    {
      "epoch": 1.6514678983590594,
      "grad_norm": 2.656078338623047,
      "learning_rate": 9.235372816022678e-05,
      "loss": 1.3666,
      "step": 125500
    },
    {
      "epoch": 1.6580474517389758,
      "grad_norm": 2.371997833251953,
      "learning_rate": 9.232061250711988e-05,
      "loss": 1.3632,
      "step": 126000
    },
    {
      "epoch": 1.6646270051188925,
      "grad_norm": 2.466104745864868,
      "learning_rate": 9.228749685401296e-05,
      "loss": 1.3583,
      "step": 126500
    },
    {
      "epoch": 1.6712065584988092,
      "grad_norm": 2.350619316101074,
      "learning_rate": 9.225438120090605e-05,
      "loss": 1.3555,
      "step": 127000
    },
    {
      "epoch": 1.6777861118787256,
      "grad_norm": 2.272038459777832,
      "learning_rate": 9.222126554779915e-05,
      "loss": 1.3632,
      "step": 127500
    },
    {
      "epoch": 1.6843656652586423,
      "grad_norm": 2.3408687114715576,
      "learning_rate": 9.218821612599844e-05,
      "loss": 1.352,
      "step": 128000
    },
    {
      "epoch": 1.690945218638559,
      "grad_norm": 2.232896327972412,
      "learning_rate": 9.215510047289154e-05,
      "loss": 1.3441,
      "step": 128500
    },
    {
      "epoch": 1.6975247720184754,
      "grad_norm": 2.5284006595611572,
      "learning_rate": 9.212198481978462e-05,
      "loss": 1.3546,
      "step": 129000
    },
    {
      "epoch": 1.7041043253983919,
      "grad_norm": 2.483107805252075,
      "learning_rate": 9.208886916667771e-05,
      "loss": 1.36,
      "step": 129500
    },
    {
      "epoch": 1.7106838787783085,
      "grad_norm": 2.5915122032165527,
      "learning_rate": 9.205575351357079e-05,
      "loss": 1.3534,
      "step": 130000
    },
    {
      "epoch": 1.7172634321582252,
      "grad_norm": 2.455352783203125,
      "learning_rate": 9.202263786046389e-05,
      "loss": 1.3536,
      "step": 130500
    },
    {
      "epoch": 1.7238429855381416,
      "grad_norm": 2.3403701782226562,
      "learning_rate": 9.198952220735698e-05,
      "loss": 1.3573,
      "step": 131000
    },
    {
      "epoch": 1.730422538918058,
      "grad_norm": 2.424572467803955,
      "learning_rate": 9.195640655425006e-05,
      "loss": 1.3618,
      "step": 131500
    },
    {
      "epoch": 1.737002092297975,
      "grad_norm": 2.435964345932007,
      "learning_rate": 9.192335713244937e-05,
      "loss": 1.3539,
      "step": 132000
    },
    {
      "epoch": 1.7435816456778914,
      "grad_norm": 2.3409171104431152,
      "learning_rate": 9.189024147934247e-05,
      "loss": 1.3519,
      "step": 132500
    },
    {
      "epoch": 1.7501611990578079,
      "grad_norm": 2.6631362438201904,
      "learning_rate": 9.185719205754176e-05,
      "loss": 1.3545,
      "step": 133000
    },
    {
      "epoch": 1.7567407524377245,
      "grad_norm": 2.6280205249786377,
      "learning_rate": 9.182407640443486e-05,
      "loss": 1.3391,
      "step": 133500
    },
    {
      "epoch": 1.7633203058176412,
      "grad_norm": 2.430891275405884,
      "learning_rate": 9.179096075132794e-05,
      "loss": 1.3559,
      "step": 134000
    },
    {
      "epoch": 1.7698998591975577,
      "grad_norm": 2.2667598724365234,
      "learning_rate": 9.175784509822103e-05,
      "loss": 1.3465,
      "step": 134500
    },
    {
      "epoch": 1.776479412577474,
      "grad_norm": 2.2896816730499268,
      "learning_rate": 9.172472944511413e-05,
      "loss": 1.3452,
      "step": 135000
    },
    {
      "epoch": 1.7830589659573908,
      "grad_norm": 2.7149407863616943,
      "learning_rate": 9.169161379200721e-05,
      "loss": 1.3374,
      "step": 135500
    },
    {
      "epoch": 1.7896385193373074,
      "grad_norm": 2.6321165561676025,
      "learning_rate": 9.16584981389003e-05,
      "loss": 1.3552,
      "step": 136000
    },
    {
      "epoch": 1.796218072717224,
      "grad_norm": 2.486288547515869,
      "learning_rate": 9.162538248579338e-05,
      "loss": 1.3626,
      "step": 136500
    },
    {
      "epoch": 1.8027976260971406,
      "grad_norm": 2.6107242107391357,
      "learning_rate": 9.159226683268648e-05,
      "loss": 1.346,
      "step": 137000
    },
    {
      "epoch": 1.8093771794770572,
      "grad_norm": 2.3349876403808594,
      "learning_rate": 9.155928364219199e-05,
      "loss": 1.3436,
      "step": 137500
    },
    {
      "epoch": 1.8159567328569737,
      "grad_norm": 2.6851255893707275,
      "learning_rate": 9.152616798908509e-05,
      "loss": 1.3486,
      "step": 138000
    },
    {
      "epoch": 1.8225362862368901,
      "grad_norm": 2.611515998840332,
      "learning_rate": 9.149305233597818e-05,
      "loss": 1.3345,
      "step": 138500
    },
    {
      "epoch": 1.8291158396168068,
      "grad_norm": 2.5223236083984375,
      "learning_rate": 9.145993668287126e-05,
      "loss": 1.3443,
      "step": 139000
    },
    {
      "epoch": 1.8356953929967235,
      "grad_norm": 2.417102098464966,
      "learning_rate": 9.142682102976436e-05,
      "loss": 1.3348,
      "step": 139500
    },
    {
      "epoch": 1.84227494637664,
      "grad_norm": 2.637437582015991,
      "learning_rate": 9.139370537665744e-05,
      "loss": 1.3352,
      "step": 140000
    },
    {
      "epoch": 1.8488544997565566,
      "grad_norm": 2.6592462062835693,
      "learning_rate": 9.136058972355053e-05,
      "loss": 1.3435,
      "step": 140500
    },
    {
      "epoch": 1.8554340531364732,
      "grad_norm": 2.4774460792541504,
      "learning_rate": 9.132754030174983e-05,
      "loss": 1.3312,
      "step": 141000
    },
    {
      "epoch": 1.8620136065163897,
      "grad_norm": 2.2149710655212402,
      "learning_rate": 9.129442464864293e-05,
      "loss": 1.3358,
      "step": 141500
    },
    {
      "epoch": 1.8685931598963061,
      "grad_norm": 2.4962191581726074,
      "learning_rate": 9.1261308995536e-05,
      "loss": 1.3296,
      "step": 142000
    },
    {
      "epoch": 1.8751727132762228,
      "grad_norm": 2.394467830657959,
      "learning_rate": 9.12281933424291e-05,
      "loss": 1.3275,
      "step": 142500
    },
    {
      "epoch": 1.8817522666561395,
      "grad_norm": 2.577522039413452,
      "learning_rate": 9.119514392062841e-05,
      "loss": 1.3444,
      "step": 143000
    },
    {
      "epoch": 1.888331820036056,
      "grad_norm": 2.5096583366394043,
      "learning_rate": 9.11620282675215e-05,
      "loss": 1.33,
      "step": 143500
    },
    {
      "epoch": 1.8949113734159724,
      "grad_norm": 2.4326233863830566,
      "learning_rate": 9.112891261441458e-05,
      "loss": 1.3246,
      "step": 144000
    },
    {
      "epoch": 1.901490926795889,
      "grad_norm": 2.4166879653930664,
      "learning_rate": 9.109579696130768e-05,
      "loss": 1.3361,
      "step": 144500
    },
    {
      "epoch": 1.9080704801758057,
      "grad_norm": 2.3146395683288574,
      "learning_rate": 9.106274753950697e-05,
      "loss": 1.3308,
      "step": 145000
    },
    {
      "epoch": 1.9146500335557222,
      "grad_norm": 2.507652997970581,
      "learning_rate": 9.102963188640007e-05,
      "loss": 1.3248,
      "step": 145500
    },
    {
      "epoch": 1.9212295869356388,
      "grad_norm": 2.722282886505127,
      "learning_rate": 9.099651623329317e-05,
      "loss": 1.3205,
      "step": 146000
    },
    {
      "epoch": 1.9278091403155555,
      "grad_norm": 2.4875195026397705,
      "learning_rate": 9.096340058018624e-05,
      "loss": 1.3337,
      "step": 146500
    },
    {
      "epoch": 1.934388693695472,
      "grad_norm": 2.3863484859466553,
      "learning_rate": 9.093028492707934e-05,
      "loss": 1.321,
      "step": 147000
    },
    {
      "epoch": 1.9409682470753884,
      "grad_norm": 2.5089006423950195,
      "learning_rate": 9.089716927397242e-05,
      "loss": 1.3325,
      "step": 147500
    },
    {
      "epoch": 1.947547800455305,
      "grad_norm": 2.549931287765503,
      "learning_rate": 9.086411985217173e-05,
      "loss": 1.3254,
      "step": 148000
    },
    {
      "epoch": 1.9541273538352217,
      "grad_norm": 2.538729190826416,
      "learning_rate": 9.083100419906481e-05,
      "loss": 1.3368,
      "step": 148500
    },
    {
      "epoch": 1.9607069072151382,
      "grad_norm": 2.547438383102417,
      "learning_rate": 9.079788854595791e-05,
      "loss": 1.3241,
      "step": 149000
    },
    {
      "epoch": 1.9672864605950549,
      "grad_norm": 2.2756104469299316,
      "learning_rate": 9.0764772892851e-05,
      "loss": 1.3267,
      "step": 149500
    },
    {
      "epoch": 1.9738660139749715,
      "grad_norm": 2.2641665935516357,
      "learning_rate": 9.073165723974408e-05,
      "loss": 1.3293,
      "step": 150000
    },
    {
      "epoch": 1.980445567354888,
      "grad_norm": 2.3214304447174072,
      "learning_rate": 9.069854158663718e-05,
      "loss": 1.3287,
      "step": 150500
    },
    {
      "epoch": 1.9870251207348044,
      "grad_norm": 2.400092840194702,
      "learning_rate": 9.066549216483647e-05,
      "loss": 1.3191,
      "step": 151000
    },
    {
      "epoch": 1.993604674114721,
      "grad_norm": 3.025855302810669,
      "learning_rate": 9.063237651172957e-05,
      "loss": 1.3328,
      "step": 151500
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.2601280212402344,
      "eval_runtime": 49.446,
      "eval_samples_per_second": 2022.407,
      "eval_steps_per_second": 15.815,
      "step": 151986
    },
    {
      "epoch": 2.0001842274946378,
      "grad_norm": 2.720566749572754,
      "learning_rate": 9.059926085862266e-05,
      "loss": 1.3229,
      "step": 152000
    },
    {
      "epoch": 2.006763780874554,
      "grad_norm": 2.248600721359253,
      "learning_rate": 9.056614520551576e-05,
      "loss": 1.3134,
      "step": 152500
    },
    {
      "epoch": 2.0133433342544707,
      "grad_norm": 2.4969754219055176,
      "learning_rate": 9.053302955240883e-05,
      "loss": 1.3148,
      "step": 153000
    },
    {
      "epoch": 2.0199228876343875,
      "grad_norm": 2.642563819885254,
      "learning_rate": 9.049991389930193e-05,
      "loss": 1.3158,
      "step": 153500
    },
    {
      "epoch": 2.026502441014304,
      "grad_norm": 2.6636054515838623,
      "learning_rate": 9.046679824619501e-05,
      "loss": 1.3301,
      "step": 154000
    },
    {
      "epoch": 2.0330819943942204,
      "grad_norm": 2.3123459815979004,
      "learning_rate": 9.043368259308811e-05,
      "loss": 1.321,
      "step": 154500
    },
    {
      "epoch": 2.039661547774137,
      "grad_norm": 2.5888679027557373,
      "learning_rate": 9.04005669399812e-05,
      "loss": 1.3085,
      "step": 155000
    },
    {
      "epoch": 2.0462411011540538,
      "grad_norm": 2.6440255641937256,
      "learning_rate": 9.03675175181805e-05,
      "loss": 1.3075,
      "step": 155500
    },
    {
      "epoch": 2.0528206545339702,
      "grad_norm": 2.5016331672668457,
      "learning_rate": 9.033440186507359e-05,
      "loss": 1.3046,
      "step": 156000
    },
    {
      "epoch": 2.0594002079138867,
      "grad_norm": 2.566767930984497,
      "learning_rate": 9.030128621196667e-05,
      "loss": 1.315,
      "step": 156500
    },
    {
      "epoch": 2.0659797612938036,
      "grad_norm": 2.6419522762298584,
      "learning_rate": 9.026817055885977e-05,
      "loss": 1.3126,
      "step": 157000
    },
    {
      "epoch": 2.07255931467372,
      "grad_norm": 2.6065409183502197,
      "learning_rate": 9.023512113705906e-05,
      "loss": 1.3098,
      "step": 157500
    },
    {
      "epoch": 2.0791388680536365,
      "grad_norm": 2.776097297668457,
      "learning_rate": 9.020200548395216e-05,
      "loss": 1.3149,
      "step": 158000
    },
    {
      "epoch": 2.085718421433553,
      "grad_norm": 2.5781149864196777,
      "learning_rate": 9.016895606215147e-05,
      "loss": 1.3122,
      "step": 158500
    },
    {
      "epoch": 2.09229797481347,
      "grad_norm": 2.402381420135498,
      "learning_rate": 9.013584040904455e-05,
      "loss": 1.3149,
      "step": 159000
    },
    {
      "epoch": 2.0988775281933862,
      "grad_norm": 2.06514048576355,
      "learning_rate": 9.010272475593764e-05,
      "loss": 1.3067,
      "step": 159500
    },
    {
      "epoch": 2.1054570815733027,
      "grad_norm": 2.4490790367126465,
      "learning_rate": 9.006967533413695e-05,
      "loss": 1.3139,
      "step": 160000
    },
    {
      "epoch": 2.1120366349532196,
      "grad_norm": 2.627497911453247,
      "learning_rate": 9.003655968103003e-05,
      "loss": 1.3081,
      "step": 160500
    },
    {
      "epoch": 2.118616188333136,
      "grad_norm": 2.440034866333008,
      "learning_rate": 9.000344402792313e-05,
      "loss": 1.2914,
      "step": 161000
    },
    {
      "epoch": 2.1251957417130525,
      "grad_norm": 2.4191529750823975,
      "learning_rate": 8.997032837481621e-05,
      "loss": 1.3127,
      "step": 161500
    },
    {
      "epoch": 2.131775295092969,
      "grad_norm": 2.5034234523773193,
      "learning_rate": 8.99372127217093e-05,
      "loss": 1.3081,
      "step": 162000
    },
    {
      "epoch": 2.138354848472886,
      "grad_norm": 2.5589759349823,
      "learning_rate": 8.99040970686024e-05,
      "loss": 1.3056,
      "step": 162500
    },
    {
      "epoch": 2.1449344018528023,
      "grad_norm": 2.4576187133789062,
      "learning_rate": 8.98710476468017e-05,
      "loss": 1.3078,
      "step": 163000
    },
    {
      "epoch": 2.1515139552327187,
      "grad_norm": 2.546689748764038,
      "learning_rate": 8.983793199369479e-05,
      "loss": 1.3046,
      "step": 163500
    },
    {
      "epoch": 2.1580935086126356,
      "grad_norm": 2.8249905109405518,
      "learning_rate": 8.980481634058787e-05,
      "loss": 1.2987,
      "step": 164000
    },
    {
      "epoch": 2.164673061992552,
      "grad_norm": 2.437106132507324,
      "learning_rate": 8.977170068748097e-05,
      "loss": 1.3101,
      "step": 164500
    },
    {
      "epoch": 2.1712526153724685,
      "grad_norm": 2.4512147903442383,
      "learning_rate": 8.973858503437405e-05,
      "loss": 1.298,
      "step": 165000
    },
    {
      "epoch": 2.177832168752385,
      "grad_norm": 2.930427074432373,
      "learning_rate": 8.970546938126714e-05,
      "loss": 1.3008,
      "step": 165500
    },
    {
      "epoch": 2.184411722132302,
      "grad_norm": 2.6481809616088867,
      "learning_rate": 8.967235372816023e-05,
      "loss": 1.3025,
      "step": 166000
    },
    {
      "epoch": 2.1909912755122183,
      "grad_norm": 2.6559364795684814,
      "learning_rate": 8.963923807505332e-05,
      "loss": 1.2997,
      "step": 166500
    },
    {
      "epoch": 2.1975708288921347,
      "grad_norm": 2.5795600414276123,
      "learning_rate": 8.960618865325262e-05,
      "loss": 1.2982,
      "step": 167000
    },
    {
      "epoch": 2.204150382272051,
      "grad_norm": 2.590757131576538,
      "learning_rate": 8.957307300014571e-05,
      "loss": 1.2925,
      "step": 167500
    },
    {
      "epoch": 2.210729935651968,
      "grad_norm": 2.305295705795288,
      "learning_rate": 8.95399573470388e-05,
      "loss": 1.2907,
      "step": 168000
    },
    {
      "epoch": 2.2173094890318845,
      "grad_norm": 2.5161664485931396,
      "learning_rate": 8.950684169393188e-05,
      "loss": 1.3037,
      "step": 168500
    },
    {
      "epoch": 2.223889042411801,
      "grad_norm": 2.3446054458618164,
      "learning_rate": 8.947372604082498e-05,
      "loss": 1.2979,
      "step": 169000
    },
    {
      "epoch": 2.230468595791718,
      "grad_norm": 2.581738233566284,
      "learning_rate": 8.944067661902429e-05,
      "loss": 1.3042,
      "step": 169500
    },
    {
      "epoch": 2.2370481491716343,
      "grad_norm": 2.339862823486328,
      "learning_rate": 8.940756096591737e-05,
      "loss": 1.2999,
      "step": 170000
    },
    {
      "epoch": 2.2436277025515508,
      "grad_norm": 2.405687093734741,
      "learning_rate": 8.937444531281046e-05,
      "loss": 1.2982,
      "step": 170500
    },
    {
      "epoch": 2.250207255931467,
      "grad_norm": 2.3780815601348877,
      "learning_rate": 8.934132965970356e-05,
      "loss": 1.2848,
      "step": 171000
    },
    {
      "epoch": 2.256786809311384,
      "grad_norm": 2.6442179679870605,
      "learning_rate": 8.930821400659664e-05,
      "loss": 1.2997,
      "step": 171500
    },
    {
      "epoch": 2.2633663626913005,
      "grad_norm": 2.678795337677002,
      "learning_rate": 8.927509835348973e-05,
      "loss": 1.2964,
      "step": 172000
    },
    {
      "epoch": 2.269945916071217,
      "grad_norm": 2.5761940479278564,
      "learning_rate": 8.924198270038281e-05,
      "loss": 1.2999,
      "step": 172500
    },
    {
      "epoch": 2.2765254694511334,
      "grad_norm": 2.4614694118499756,
      "learning_rate": 8.920886704727591e-05,
      "loss": 1.298,
      "step": 173000
    },
    {
      "epoch": 2.2831050228310503,
      "grad_norm": 2.1049790382385254,
      "learning_rate": 8.9175751394169e-05,
      "loss": 1.2811,
      "step": 173500
    },
    {
      "epoch": 2.2896845762109668,
      "grad_norm": 2.507854461669922,
      "learning_rate": 8.914263574106208e-05,
      "loss": 1.2881,
      "step": 174000
    },
    {
      "epoch": 2.296264129590883,
      "grad_norm": 2.407914638519287,
      "learning_rate": 8.910952008795518e-05,
      "loss": 1.2841,
      "step": 174500
    },
    {
      "epoch": 2.3028436829708,
      "grad_norm": 2.7808644771575928,
      "learning_rate": 8.907640443484827e-05,
      "loss": 1.2973,
      "step": 175000
    },
    {
      "epoch": 2.3094232363507166,
      "grad_norm": 2.5101382732391357,
      "learning_rate": 8.904335501304757e-05,
      "loss": 1.2861,
      "step": 175500
    },
    {
      "epoch": 2.316002789730633,
      "grad_norm": 2.5759520530700684,
      "learning_rate": 8.901023935994066e-05,
      "loss": 1.2958,
      "step": 176000
    },
    {
      "epoch": 2.3225823431105495,
      "grad_norm": 2.5106101036071777,
      "learning_rate": 8.897712370683376e-05,
      "loss": 1.2854,
      "step": 176500
    },
    {
      "epoch": 2.3291618964904663,
      "grad_norm": 2.3938381671905518,
      "learning_rate": 8.894407428503305e-05,
      "loss": 1.2848,
      "step": 177000
    },
    {
      "epoch": 2.335741449870383,
      "grad_norm": 2.5092720985412598,
      "learning_rate": 8.891095863192615e-05,
      "loss": 1.2829,
      "step": 177500
    },
    {
      "epoch": 2.3423210032502992,
      "grad_norm": 2.4083592891693115,
      "learning_rate": 8.887784297881923e-05,
      "loss": 1.2859,
      "step": 178000
    },
    {
      "epoch": 2.348900556630216,
      "grad_norm": 2.545307159423828,
      "learning_rate": 8.884472732571232e-05,
      "loss": 1.2838,
      "step": 178500
    },
    {
      "epoch": 2.3554801100101326,
      "grad_norm": 2.631819725036621,
      "learning_rate": 8.881161167260542e-05,
      "loss": 1.2904,
      "step": 179000
    },
    {
      "epoch": 2.362059663390049,
      "grad_norm": 2.4089365005493164,
      "learning_rate": 8.87784960194985e-05,
      "loss": 1.2904,
      "step": 179500
    },
    {
      "epoch": 2.3686392167699655,
      "grad_norm": 2.461505889892578,
      "learning_rate": 8.87453803663916e-05,
      "loss": 1.2857,
      "step": 180000
    },
    {
      "epoch": 2.3752187701498824,
      "grad_norm": 2.631765604019165,
      "learning_rate": 8.871226471328467e-05,
      "loss": 1.2868,
      "step": 180500
    },
    {
      "epoch": 2.381798323529799,
      "grad_norm": 2.537003517150879,
      "learning_rate": 8.86792815227902e-05,
      "loss": 1.2929,
      "step": 181000
    },
    {
      "epoch": 2.3883778769097153,
      "grad_norm": 2.326262950897217,
      "learning_rate": 8.864629833229572e-05,
      "loss": 1.2728,
      "step": 181500
    },
    {
      "epoch": 2.394957430289632,
      "grad_norm": 2.5656440258026123,
      "learning_rate": 8.86131826791888e-05,
      "loss": 1.283,
      "step": 182000
    },
    {
      "epoch": 2.4015369836695486,
      "grad_norm": 2.5484371185302734,
      "learning_rate": 8.858006702608189e-05,
      "loss": 1.2815,
      "step": 182500
    },
    {
      "epoch": 2.408116537049465,
      "grad_norm": 2.4767537117004395,
      "learning_rate": 8.854695137297499e-05,
      "loss": 1.2786,
      "step": 183000
    },
    {
      "epoch": 2.4146960904293815,
      "grad_norm": 2.1993043422698975,
      "learning_rate": 8.851383571986808e-05,
      "loss": 1.2836,
      "step": 183500
    },
    {
      "epoch": 2.4212756438092984,
      "grad_norm": 2.5507354736328125,
      "learning_rate": 8.848072006676116e-05,
      "loss": 1.2893,
      "step": 184000
    },
    {
      "epoch": 2.427855197189215,
      "grad_norm": 2.4121358394622803,
      "learning_rate": 8.844760441365425e-05,
      "loss": 1.2882,
      "step": 184500
    },
    {
      "epoch": 2.4344347505691313,
      "grad_norm": 2.496734142303467,
      "learning_rate": 8.841448876054735e-05,
      "loss": 1.2827,
      "step": 185000
    },
    {
      "epoch": 2.441014303949048,
      "grad_norm": 2.454406976699829,
      "learning_rate": 8.838137310744043e-05,
      "loss": 1.2819,
      "step": 185500
    },
    {
      "epoch": 2.4475938573289646,
      "grad_norm": 2.441211223602295,
      "learning_rate": 8.834825745433352e-05,
      "loss": 1.2745,
      "step": 186000
    },
    {
      "epoch": 2.454173410708881,
      "grad_norm": 2.4780280590057373,
      "learning_rate": 8.831514180122661e-05,
      "loss": 1.2814,
      "step": 186500
    },
    {
      "epoch": 2.4607529640887975,
      "grad_norm": 2.9312641620635986,
      "learning_rate": 8.82820261481197e-05,
      "loss": 1.2794,
      "step": 187000
    },
    {
      "epoch": 2.4673325174687144,
      "grad_norm": 2.6647372245788574,
      "learning_rate": 8.824891049501279e-05,
      "loss": 1.2772,
      "step": 187500
    },
    {
      "epoch": 2.473912070848631,
      "grad_norm": 2.72605562210083,
      "learning_rate": 8.821592730451831e-05,
      "loss": 1.2788,
      "step": 188000
    },
    {
      "epoch": 2.4804916242285473,
      "grad_norm": 2.202538013458252,
      "learning_rate": 8.81828778827176e-05,
      "loss": 1.2706,
      "step": 188500
    },
    {
      "epoch": 2.4870711776084637,
      "grad_norm": 2.4035844802856445,
      "learning_rate": 8.81498284609169e-05,
      "loss": 1.2725,
      "step": 189000
    },
    {
      "epoch": 2.4936507309883806,
      "grad_norm": 2.473832845687866,
      "learning_rate": 8.811671280780999e-05,
      "loss": 1.2826,
      "step": 189500
    },
    {
      "epoch": 2.500230284368297,
      "grad_norm": 2.388871431350708,
      "learning_rate": 8.808359715470309e-05,
      "loss": 1.2738,
      "step": 190000
    },
    {
      "epoch": 2.5068098377482135,
      "grad_norm": 2.4969701766967773,
      "learning_rate": 8.805048150159619e-05,
      "loss": 1.2775,
      "step": 190500
    },
    {
      "epoch": 2.51338939112813,
      "grad_norm": 2.4958319664001465,
      "learning_rate": 8.801736584848926e-05,
      "loss": 1.2802,
      "step": 191000
    },
    {
      "epoch": 2.519968944508047,
      "grad_norm": 2.2347912788391113,
      "learning_rate": 8.798425019538236e-05,
      "loss": 1.2795,
      "step": 191500
    },
    {
      "epoch": 2.5265484978879633,
      "grad_norm": 2.488762855529785,
      "learning_rate": 8.795113454227544e-05,
      "loss": 1.2659,
      "step": 192000
    },
    {
      "epoch": 2.53312805126788,
      "grad_norm": 2.5741772651672363,
      "learning_rate": 8.791801888916854e-05,
      "loss": 1.277,
      "step": 192500
    },
    {
      "epoch": 2.5397076046477967,
      "grad_norm": 2.448676824569702,
      "learning_rate": 8.788490323606162e-05,
      "loss": 1.2737,
      "step": 193000
    },
    {
      "epoch": 2.546287158027713,
      "grad_norm": 2.4032952785491943,
      "learning_rate": 8.785178758295471e-05,
      "loss": 1.2846,
      "step": 193500
    },
    {
      "epoch": 2.5528667114076296,
      "grad_norm": 2.4913930892944336,
      "learning_rate": 8.781887062376644e-05,
      "loss": 1.2816,
      "step": 194000
    },
    {
      "epoch": 2.559446264787546,
      "grad_norm": 2.64023494720459,
      "learning_rate": 8.778575497065954e-05,
      "loss": 1.2725,
      "step": 194500
    },
    {
      "epoch": 2.566025818167463,
      "grad_norm": 2.646008014678955,
      "learning_rate": 8.775263931755262e-05,
      "loss": 1.2796,
      "step": 195000
    },
    {
      "epoch": 2.5726053715473793,
      "grad_norm": 2.5842981338500977,
      "learning_rate": 8.771952366444571e-05,
      "loss": 1.266,
      "step": 195500
    },
    {
      "epoch": 2.579184924927296,
      "grad_norm": 2.2472314834594727,
      "learning_rate": 8.76864080113388e-05,
      "loss": 1.269,
      "step": 196000
    },
    {
      "epoch": 2.5857644783072127,
      "grad_norm": 2.7989861965179443,
      "learning_rate": 8.76532923582319e-05,
      "loss": 1.2683,
      "step": 196500
    },
    {
      "epoch": 2.592344031687129,
      "grad_norm": 2.3489952087402344,
      "learning_rate": 8.762017670512498e-05,
      "loss": 1.2758,
      "step": 197000
    },
    {
      "epoch": 2.5989235850670456,
      "grad_norm": 2.576188325881958,
      "learning_rate": 8.758706105201807e-05,
      "loss": 1.2785,
      "step": 197500
    },
    {
      "epoch": 2.605503138446962,
      "grad_norm": 2.411442995071411,
      "learning_rate": 8.755394539891117e-05,
      "loss": 1.2701,
      "step": 198000
    },
    {
      "epoch": 2.612082691826879,
      "grad_norm": 2.650944471359253,
      "learning_rate": 8.752082974580425e-05,
      "loss": 1.2634,
      "step": 198500
    },
    {
      "epoch": 2.6186622452067954,
      "grad_norm": 2.2421460151672363,
      "learning_rate": 8.748771409269734e-05,
      "loss": 1.2728,
      "step": 199000
    },
    {
      "epoch": 2.625241798586712,
      "grad_norm": 2.236410617828369,
      "learning_rate": 8.745459843959042e-05,
      "loss": 1.2659,
      "step": 199500
    },
    {
      "epoch": 2.6318213519666287,
      "grad_norm": 2.567091941833496,
      "learning_rate": 8.742148278648352e-05,
      "loss": 1.2633,
      "step": 200000
    },
    {
      "epoch": 2.638400905346545,
      "grad_norm": 2.7983930110931396,
      "learning_rate": 8.738836713337661e-05,
      "loss": 1.264,
      "step": 200500
    },
    {
      "epoch": 2.6449804587264616,
      "grad_norm": 2.439687490463257,
      "learning_rate": 8.73552514802697e-05,
      "loss": 1.2585,
      "step": 201000
    },
    {
      "epoch": 2.651560012106378,
      "grad_norm": 2.2603721618652344,
      "learning_rate": 8.732213582716279e-05,
      "loss": 1.2514,
      "step": 201500
    },
    {
      "epoch": 2.658139565486295,
      "grad_norm": 2.5642313957214355,
      "learning_rate": 8.728902017405588e-05,
      "loss": 1.273,
      "step": 202000
    },
    {
      "epoch": 2.6647191188662114,
      "grad_norm": 2.3625152111053467,
      "learning_rate": 8.725590452094896e-05,
      "loss": 1.2633,
      "step": 202500
    },
    {
      "epoch": 2.671298672246128,
      "grad_norm": 2.5109050273895264,
      "learning_rate": 8.722278886784205e-05,
      "loss": 1.2615,
      "step": 203000
    },
    {
      "epoch": 2.6778782256260447,
      "grad_norm": 2.3490400314331055,
      "learning_rate": 8.718967321473515e-05,
      "loss": 1.2652,
      "step": 203500
    },
    {
      "epoch": 2.684457779005961,
      "grad_norm": 2.8492820262908936,
      "learning_rate": 8.715655756162823e-05,
      "loss": 1.2574,
      "step": 204000
    },
    {
      "epoch": 2.6910373323858776,
      "grad_norm": 2.6140074729919434,
      "learning_rate": 8.712344190852132e-05,
      "loss": 1.2606,
      "step": 204500
    },
    {
      "epoch": 2.697616885765794,
      "grad_norm": 2.262080192565918,
      "learning_rate": 8.709039248672062e-05,
      "loss": 1.2579,
      "step": 205000
    },
    {
      "epoch": 2.704196439145711,
      "grad_norm": 2.4303770065307617,
      "learning_rate": 8.705734306491993e-05,
      "loss": 1.2575,
      "step": 205500
    },
    {
      "epoch": 2.7107759925256274,
      "grad_norm": 2.1607022285461426,
      "learning_rate": 8.702422741181303e-05,
      "loss": 1.2606,
      "step": 206000
    },
    {
      "epoch": 2.717355545905544,
      "grad_norm": 2.2316386699676514,
      "learning_rate": 8.699111175870611e-05,
      "loss": 1.2691,
      "step": 206500
    },
    {
      "epoch": 2.7239350992854607,
      "grad_norm": 2.5012354850769043,
      "learning_rate": 8.695799610559921e-05,
      "loss": 1.2582,
      "step": 207000
    },
    {
      "epoch": 2.730514652665377,
      "grad_norm": 2.263758897781372,
      "learning_rate": 8.692488045249228e-05,
      "loss": 1.2555,
      "step": 207500
    },
    {
      "epoch": 2.7370942060452936,
      "grad_norm": 2.4431962966918945,
      "learning_rate": 8.689176479938538e-05,
      "loss": 1.2525,
      "step": 208000
    },
    {
      "epoch": 2.74367375942521,
      "grad_norm": 2.39505672454834,
      "learning_rate": 8.685864914627847e-05,
      "loss": 1.2563,
      "step": 208500
    },
    {
      "epoch": 2.7502533128051265,
      "grad_norm": 2.411895275115967,
      "learning_rate": 8.682553349317155e-05,
      "loss": 1.2533,
      "step": 209000
    },
    {
      "epoch": 2.7568328661850434,
      "grad_norm": 2.288642644882202,
      "learning_rate": 8.679248407137086e-05,
      "loss": 1.2607,
      "step": 209500
    },
    {
      "epoch": 2.76341241956496,
      "grad_norm": 2.840708017349243,
      "learning_rate": 8.675936841826396e-05,
      "loss": 1.2547,
      "step": 210000
    },
    {
      "epoch": 2.7699919729448768,
      "grad_norm": 2.5602076053619385,
      "learning_rate": 8.672625276515704e-05,
      "loss": 1.2628,
      "step": 210500
    },
    {
      "epoch": 2.776571526324793,
      "grad_norm": 2.3073248863220215,
      "learning_rate": 8.669333580596877e-05,
      "loss": 1.266,
      "step": 211000
    },
    {
      "epoch": 2.7831510797047097,
      "grad_norm": 2.391941785812378,
      "learning_rate": 8.666022015286186e-05,
      "loss": 1.2589,
      "step": 211500
    },
    {
      "epoch": 2.789730633084626,
      "grad_norm": 2.5731241703033447,
      "learning_rate": 8.662710449975496e-05,
      "loss": 1.2611,
      "step": 212000
    },
    {
      "epoch": 2.7963101864645425,
      "grad_norm": 2.4400107860565186,
      "learning_rate": 8.659405507795425e-05,
      "loss": 1.2597,
      "step": 212500
    },
    {
      "epoch": 2.8028897398444594,
      "grad_norm": 2.309630870819092,
      "learning_rate": 8.656093942484735e-05,
      "loss": 1.2642,
      "step": 213000
    },
    {
      "epoch": 2.809469293224376,
      "grad_norm": 2.8115949630737305,
      "learning_rate": 8.652782377174043e-05,
      "loss": 1.26,
      "step": 213500
    },
    {
      "epoch": 2.8160488466042928,
      "grad_norm": 2.5148255825042725,
      "learning_rate": 8.649470811863352e-05,
      "loss": 1.2419,
      "step": 214000
    },
    {
      "epoch": 2.8226283999842092,
      "grad_norm": 2.396822690963745,
      "learning_rate": 8.646159246552661e-05,
      "loss": 1.2485,
      "step": 214500
    },
    {
      "epoch": 2.8292079533641257,
      "grad_norm": 2.434008836746216,
      "learning_rate": 8.64284768124197e-05,
      "loss": 1.245,
      "step": 215000
    },
    {
      "epoch": 2.835787506744042,
      "grad_norm": 2.4131901264190674,
      "learning_rate": 8.63953611593128e-05,
      "loss": 1.2507,
      "step": 215500
    },
    {
      "epoch": 2.8423670601239586,
      "grad_norm": 2.10672664642334,
      "learning_rate": 8.636224550620587e-05,
      "loss": 1.2522,
      "step": 216000
    },
    {
      "epoch": 2.8489466135038755,
      "grad_norm": 2.5935862064361572,
      "learning_rate": 8.632912985309897e-05,
      "loss": 1.2482,
      "step": 216500
    },
    {
      "epoch": 2.855526166883792,
      "grad_norm": 2.3243041038513184,
      "learning_rate": 8.629601419999205e-05,
      "loss": 1.2382,
      "step": 217000
    },
    {
      "epoch": 2.8621057202637084,
      "grad_norm": 2.2856605052948,
      "learning_rate": 8.626289854688514e-05,
      "loss": 1.2613,
      "step": 217500
    },
    {
      "epoch": 2.8686852736436252,
      "grad_norm": 2.3677661418914795,
      "learning_rate": 8.622978289377824e-05,
      "loss": 1.253,
      "step": 218000
    },
    {
      "epoch": 2.8752648270235417,
      "grad_norm": 2.136608600616455,
      "learning_rate": 8.619666724067132e-05,
      "loss": 1.2493,
      "step": 218500
    },
    {
      "epoch": 2.881844380403458,
      "grad_norm": 2.427839517593384,
      "learning_rate": 8.616355158756442e-05,
      "loss": 1.249,
      "step": 219000
    },
    {
      "epoch": 2.8884239337833746,
      "grad_norm": 2.340036630630493,
      "learning_rate": 8.61304359344575e-05,
      "loss": 1.2497,
      "step": 219500
    },
    {
      "epoch": 2.8950034871632915,
      "grad_norm": 2.4987716674804688,
      "learning_rate": 8.60973202813506e-05,
      "loss": 1.2413,
      "step": 220000
    },
    {
      "epoch": 2.901583040543208,
      "grad_norm": 2.4221484661102295,
      "learning_rate": 8.606420462824368e-05,
      "loss": 1.2488,
      "step": 220500
    },
    {
      "epoch": 2.9081625939231244,
      "grad_norm": 2.8365936279296875,
      "learning_rate": 8.603115520644298e-05,
      "loss": 1.2441,
      "step": 221000
    },
    {
      "epoch": 2.9147421473030413,
      "grad_norm": 2.5157461166381836,
      "learning_rate": 8.599803955333607e-05,
      "loss": 1.2334,
      "step": 221500
    },
    {
      "epoch": 2.9213217006829577,
      "grad_norm": 2.4614131450653076,
      "learning_rate": 8.596492390022917e-05,
      "loss": 1.248,
      "step": 222000
    },
    {
      "epoch": 2.927901254062874,
      "grad_norm": 2.308462381362915,
      "learning_rate": 8.593180824712225e-05,
      "loss": 1.2449,
      "step": 222500
    },
    {
      "epoch": 2.9344808074427906,
      "grad_norm": 2.560129165649414,
      "learning_rate": 8.589869259401534e-05,
      "loss": 1.2431,
      "step": 223000
    },
    {
      "epoch": 2.9410603608227075,
      "grad_norm": 2.417508125305176,
      "learning_rate": 8.586557694090844e-05,
      "loss": 1.2415,
      "step": 223500
    },
    {
      "epoch": 2.947639914202624,
      "grad_norm": 2.7131266593933105,
      "learning_rate": 8.583246128780152e-05,
      "loss": 1.247,
      "step": 224000
    },
    {
      "epoch": 2.9542194675825404,
      "grad_norm": 2.2705585956573486,
      "learning_rate": 8.579934563469462e-05,
      "loss": 1.2488,
      "step": 224500
    },
    {
      "epoch": 2.9607990209624573,
      "grad_norm": 2.5807881355285645,
      "learning_rate": 8.57662299815877e-05,
      "loss": 1.2505,
      "step": 225000
    },
    {
      "epoch": 2.9673785743423737,
      "grad_norm": 2.4092020988464355,
      "learning_rate": 8.573318055978701e-05,
      "loss": 1.2446,
      "step": 225500
    },
    {
      "epoch": 2.97395812772229,
      "grad_norm": 2.4314024448394775,
      "learning_rate": 8.570006490668008e-05,
      "loss": 1.2405,
      "step": 226000
    },
    {
      "epoch": 2.9805376811022066,
      "grad_norm": 2.2734267711639404,
      "learning_rate": 8.566694925357318e-05,
      "loss": 1.2386,
      "step": 226500
    },
    {
      "epoch": 2.9871172344821235,
      "grad_norm": 2.3924615383148193,
      "learning_rate": 8.563383360046627e-05,
      "loss": 1.24,
      "step": 227000
    },
    {
      "epoch": 2.99369678786204,
      "grad_norm": 2.283802032470703,
      "learning_rate": 8.560071794735937e-05,
      "loss": 1.2477,
      "step": 227500
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.182247519493103,
      "eval_runtime": 49.7994,
      "eval_samples_per_second": 2008.055,
      "eval_steps_per_second": 15.703,
      "step": 227979
    },
    {
      "epoch": 3.0002763412419564,
      "grad_norm": 2.3742706775665283,
      "learning_rate": 8.556760229425245e-05,
      "loss": 1.2414,
      "step": 228000
    },
    {
      "epoch": 3.006855894621873,
      "grad_norm": 2.449753522872925,
      "learning_rate": 8.553448664114554e-05,
      "loss": 1.2381,
      "step": 228500
    },
    {
      "epoch": 3.0134354480017898,
      "grad_norm": 2.3953804969787598,
      "learning_rate": 8.550143721934484e-05,
      "loss": 1.2312,
      "step": 229000
    },
    {
      "epoch": 3.020015001381706,
      "grad_norm": 2.3364617824554443,
      "learning_rate": 8.546832156623793e-05,
      "loss": 1.2254,
      "step": 229500
    },
    {
      "epoch": 3.0265945547616226,
      "grad_norm": 2.451772689819336,
      "learning_rate": 8.543520591313103e-05,
      "loss": 1.2338,
      "step": 230000
    },
    {
      "epoch": 3.0331741081415395,
      "grad_norm": 2.623823404312134,
      "learning_rate": 8.540215649133032e-05,
      "loss": 1.2344,
      "step": 230500
    },
    {
      "epoch": 3.039753661521456,
      "grad_norm": 2.9754786491394043,
      "learning_rate": 8.536904083822342e-05,
      "loss": 1.2417,
      "step": 231000
    },
    {
      "epoch": 3.0463332149013724,
      "grad_norm": 2.5363271236419678,
      "learning_rate": 8.533599141642272e-05,
      "loss": 1.2325,
      "step": 231500
    },
    {
      "epoch": 3.052912768281289,
      "grad_norm": 2.4842469692230225,
      "learning_rate": 8.53028757633158e-05,
      "loss": 1.2284,
      "step": 232000
    },
    {
      "epoch": 3.0594923216612058,
      "grad_norm": 2.316167116165161,
      "learning_rate": 8.526976011020889e-05,
      "loss": 1.2333,
      "step": 232500
    },
    {
      "epoch": 3.066071875041122,
      "grad_norm": 2.545714855194092,
      "learning_rate": 8.523671068840821e-05,
      "loss": 1.2239,
      "step": 233000
    },
    {
      "epoch": 3.0726514284210387,
      "grad_norm": 2.6290321350097656,
      "learning_rate": 8.520359503530128e-05,
      "loss": 1.2292,
      "step": 233500
    },
    {
      "epoch": 3.0792309818009556,
      "grad_norm": 2.2982194423675537,
      "learning_rate": 8.517047938219438e-05,
      "loss": 1.2332,
      "step": 234000
    },
    {
      "epoch": 3.085810535180872,
      "grad_norm": 2.3809595108032227,
      "learning_rate": 8.513736372908747e-05,
      "loss": 1.2397,
      "step": 234500
    },
    {
      "epoch": 3.0923900885607885,
      "grad_norm": 2.4155828952789307,
      "learning_rate": 8.510424807598057e-05,
      "loss": 1.2284,
      "step": 235000
    },
    {
      "epoch": 3.098969641940705,
      "grad_norm": 2.403456211090088,
      "learning_rate": 8.507113242287365e-05,
      "loss": 1.2298,
      "step": 235500
    },
    {
      "epoch": 3.105549195320622,
      "grad_norm": 2.720700979232788,
      "learning_rate": 8.503801676976674e-05,
      "loss": 1.234,
      "step": 236000
    },
    {
      "epoch": 3.1121287487005382,
      "grad_norm": 2.6121647357940674,
      "learning_rate": 8.500490111665983e-05,
      "loss": 1.2305,
      "step": 236500
    },
    {
      "epoch": 3.1187083020804547,
      "grad_norm": 2.329378128051758,
      "learning_rate": 8.49717854635529e-05,
      "loss": 1.2348,
      "step": 237000
    },
    {
      "epoch": 3.125287855460371,
      "grad_norm": 2.54818058013916,
      "learning_rate": 8.4938669810446e-05,
      "loss": 1.2306,
      "step": 237500
    },
    {
      "epoch": 3.131867408840288,
      "grad_norm": 2.3453893661499023,
      "learning_rate": 8.490555415733909e-05,
      "loss": 1.223,
      "step": 238000
    },
    {
      "epoch": 3.1384469622202045,
      "grad_norm": 2.404945135116577,
      "learning_rate": 8.48725047355384e-05,
      "loss": 1.2257,
      "step": 238500
    },
    {
      "epoch": 3.145026515600121,
      "grad_norm": 2.419325351715088,
      "learning_rate": 8.483938908243148e-05,
      "loss": 1.221,
      "step": 239000
    },
    {
      "epoch": 3.151606068980038,
      "grad_norm": 2.4190547466278076,
      "learning_rate": 8.480627342932458e-05,
      "loss": 1.226,
      "step": 239500
    },
    {
      "epoch": 3.1581856223599543,
      "grad_norm": 2.6312813758850098,
      "learning_rate": 8.477315777621766e-05,
      "loss": 1.2222,
      "step": 240000
    },
    {
      "epoch": 3.1647651757398707,
      "grad_norm": 2.3267881870269775,
      "learning_rate": 8.474004212311075e-05,
      "loss": 1.2218,
      "step": 240500
    },
    {
      "epoch": 3.171344729119787,
      "grad_norm": 2.540269136428833,
      "learning_rate": 8.470692647000385e-05,
      "loss": 1.2288,
      "step": 241000
    },
    {
      "epoch": 3.177924282499704,
      "grad_norm": 2.430459976196289,
      "learning_rate": 8.467381081689693e-05,
      "loss": 1.2244,
      "step": 241500
    },
    {
      "epoch": 3.1845038358796205,
      "grad_norm": 2.6790285110473633,
      "learning_rate": 8.464069516379003e-05,
      "loss": 1.227,
      "step": 242000
    },
    {
      "epoch": 3.191083389259537,
      "grad_norm": 2.3202064037323,
      "learning_rate": 8.46075795106831e-05,
      "loss": 1.2222,
      "step": 242500
    },
    {
      "epoch": 3.197662942639454,
      "grad_norm": 2.5177204608917236,
      "learning_rate": 8.45744638575762e-05,
      "loss": 1.2132,
      "step": 243000
    },
    {
      "epoch": 3.2042424960193703,
      "grad_norm": 2.3209238052368164,
      "learning_rate": 8.454134820446929e-05,
      "loss": 1.2201,
      "step": 243500
    },
    {
      "epoch": 3.2108220493992867,
      "grad_norm": 2.6639628410339355,
      "learning_rate": 8.450823255136239e-05,
      "loss": 1.2274,
      "step": 244000
    },
    {
      "epoch": 3.217401602779203,
      "grad_norm": 5.090656757354736,
      "learning_rate": 8.447518312956168e-05,
      "loss": 1.2252,
      "step": 244500
    },
    {
      "epoch": 3.22398115615912,
      "grad_norm": 2.4853808879852295,
      "learning_rate": 8.444206747645478e-05,
      "loss": 1.2212,
      "step": 245000
    },
    {
      "epoch": 3.2305607095390365,
      "grad_norm": 2.6249215602874756,
      "learning_rate": 8.440901805465408e-05,
      "loss": 1.2224,
      "step": 245500
    },
    {
      "epoch": 3.237140262918953,
      "grad_norm": 2.1645920276641846,
      "learning_rate": 8.437590240154717e-05,
      "loss": 1.2327,
      "step": 246000
    },
    {
      "epoch": 3.24371981629887,
      "grad_norm": 2.532456874847412,
      "learning_rate": 8.434278674844027e-05,
      "loss": 1.2177,
      "step": 246500
    },
    {
      "epoch": 3.2502993696787863,
      "grad_norm": 2.3240582942962646,
      "learning_rate": 8.430967109533334e-05,
      "loss": 1.2158,
      "step": 247000
    },
    {
      "epoch": 3.2568789230587027,
      "grad_norm": 2.5990540981292725,
      "learning_rate": 8.427655544222644e-05,
      "loss": 1.2079,
      "step": 247500
    },
    {
      "epoch": 3.263458476438619,
      "grad_norm": 2.312697172164917,
      "learning_rate": 8.424343978911952e-05,
      "loss": 1.2216,
      "step": 248000
    },
    {
      "epoch": 3.270038029818536,
      "grad_norm": 2.027768611907959,
      "learning_rate": 8.421032413601262e-05,
      "loss": 1.2247,
      "step": 248500
    },
    {
      "epoch": 3.2766175831984525,
      "grad_norm": 2.6787660121917725,
      "learning_rate": 8.417727471421191e-05,
      "loss": 1.2207,
      "step": 249000
    },
    {
      "epoch": 3.283197136578369,
      "grad_norm": 2.3107993602752686,
      "learning_rate": 8.414415906110501e-05,
      "loss": 1.2103,
      "step": 249500
    },
    {
      "epoch": 3.289776689958286,
      "grad_norm": 2.234889030456543,
      "learning_rate": 8.41110434079981e-05,
      "loss": 1.2178,
      "step": 250000
    },
    {
      "epoch": 3.2963562433382023,
      "grad_norm": 2.669450044631958,
      "learning_rate": 8.407792775489118e-05,
      "loss": 1.2137,
      "step": 250500
    },
    {
      "epoch": 3.3029357967181188,
      "grad_norm": 2.428875207901001,
      "learning_rate": 8.404481210178428e-05,
      "loss": 1.2217,
      "step": 251000
    },
    {
      "epoch": 3.309515350098035,
      "grad_norm": 2.3577802181243896,
      "learning_rate": 8.401169644867737e-05,
      "loss": 1.2246,
      "step": 251500
    },
    {
      "epoch": 3.316094903477952,
      "grad_norm": 2.5248327255249023,
      "learning_rate": 8.397858079557045e-05,
      "loss": 1.2211,
      "step": 252000
    },
    {
      "epoch": 3.3226744568578686,
      "grad_norm": 2.4467995166778564,
      "learning_rate": 8.394546514246354e-05,
      "loss": 1.2161,
      "step": 252500
    },
    {
      "epoch": 3.329254010237785,
      "grad_norm": 3.061495780944824,
      "learning_rate": 8.391241572066286e-05,
      "loss": 1.2212,
      "step": 253000
    },
    {
      "epoch": 3.3358335636177014,
      "grad_norm": 2.6141347885131836,
      "learning_rate": 8.387930006755593e-05,
      "loss": 1.2181,
      "step": 253500
    },
    {
      "epoch": 3.3424131169976183,
      "grad_norm": 2.581589937210083,
      "learning_rate": 8.384618441444903e-05,
      "loss": 1.2125,
      "step": 254000
    },
    {
      "epoch": 3.348992670377535,
      "grad_norm": 2.457829236984253,
      "learning_rate": 8.381306876134211e-05,
      "loss": 1.2148,
      "step": 254500
    },
    {
      "epoch": 3.3555722237574512,
      "grad_norm": 3.014432430267334,
      "learning_rate": 8.377995310823521e-05,
      "loss": 1.2211,
      "step": 255000
    },
    {
      "epoch": 3.3621517771373677,
      "grad_norm": 2.3410773277282715,
      "learning_rate": 8.37469036864345e-05,
      "loss": 1.2143,
      "step": 255500
    },
    {
      "epoch": 3.3687313305172846,
      "grad_norm": 2.7952077388763428,
      "learning_rate": 8.37137880333276e-05,
      "loss": 1.2082,
      "step": 256000
    },
    {
      "epoch": 3.375310883897201,
      "grad_norm": 2.5039193630218506,
      "learning_rate": 8.368067238022069e-05,
      "loss": 1.2155,
      "step": 256500
    },
    {
      "epoch": 3.3818904372771175,
      "grad_norm": 2.4991977214813232,
      "learning_rate": 8.364755672711377e-05,
      "loss": 1.2134,
      "step": 257000
    },
    {
      "epoch": 3.3884699906570344,
      "grad_norm": 2.2844314575195312,
      "learning_rate": 8.361444107400687e-05,
      "loss": 1.2146,
      "step": 257500
    },
    {
      "epoch": 3.395049544036951,
      "grad_norm": 2.3899526596069336,
      "learning_rate": 8.358145788351238e-05,
      "loss": 1.2242,
      "step": 258000
    },
    {
      "epoch": 3.4016290974168673,
      "grad_norm": 2.465038537979126,
      "learning_rate": 8.354834223040548e-05,
      "loss": 1.229,
      "step": 258500
    },
    {
      "epoch": 3.4082086507967837,
      "grad_norm": 2.5574257373809814,
      "learning_rate": 8.351522657729857e-05,
      "loss": 1.2155,
      "step": 259000
    },
    {
      "epoch": 3.4147882041767006,
      "grad_norm": 2.416874885559082,
      "learning_rate": 8.348211092419165e-05,
      "loss": 1.2145,
      "step": 259500
    },
    {
      "epoch": 3.421367757556617,
      "grad_norm": 2.22422194480896,
      "learning_rate": 8.344899527108474e-05,
      "loss": 1.2097,
      "step": 260000
    },
    {
      "epoch": 3.4279473109365335,
      "grad_norm": 2.8909623622894287,
      "learning_rate": 8.341587961797783e-05,
      "loss": 1.2121,
      "step": 260500
    },
    {
      "epoch": 3.4345268643164504,
      "grad_norm": 2.3491532802581787,
      "learning_rate": 8.338276396487092e-05,
      "loss": 1.218,
      "step": 261000
    },
    {
      "epoch": 3.441106417696367,
      "grad_norm": 2.7020418643951416,
      "learning_rate": 8.3349648311764e-05,
      "loss": 1.2108,
      "step": 261500
    },
    {
      "epoch": 3.4476859710762833,
      "grad_norm": 2.5041608810424805,
      "learning_rate": 8.331666512126952e-05,
      "loss": 1.2119,
      "step": 262000
    },
    {
      "epoch": 3.4542655244561997,
      "grad_norm": 2.4656827449798584,
      "learning_rate": 8.328354946816262e-05,
      "loss": 1.2092,
      "step": 262500
    },
    {
      "epoch": 3.4608450778361166,
      "grad_norm": 2.527804136276245,
      "learning_rate": 8.32504338150557e-05,
      "loss": 1.2032,
      "step": 263000
    },
    {
      "epoch": 3.467424631216033,
      "grad_norm": 2.539989471435547,
      "learning_rate": 8.32173181619488e-05,
      "loss": 1.2042,
      "step": 263500
    },
    {
      "epoch": 3.4740041845959495,
      "grad_norm": 2.7995474338531494,
      "learning_rate": 8.318420250884188e-05,
      "loss": 1.2084,
      "step": 264000
    },
    {
      "epoch": 3.4805837379758664,
      "grad_norm": 2.281306743621826,
      "learning_rate": 8.315108685573497e-05,
      "loss": 1.208,
      "step": 264500
    },
    {
      "epoch": 3.487163291355783,
      "grad_norm": 2.591644525527954,
      "learning_rate": 8.311797120262807e-05,
      "loss": 1.2113,
      "step": 265000
    },
    {
      "epoch": 3.4937428447356993,
      "grad_norm": 2.5105793476104736,
      "learning_rate": 8.308485554952115e-05,
      "loss": 1.2128,
      "step": 265500
    },
    {
      "epoch": 3.5003223981156157,
      "grad_norm": 2.264354705810547,
      "learning_rate": 8.305173989641424e-05,
      "loss": 1.2105,
      "step": 266000
    },
    {
      "epoch": 3.5069019514955326,
      "grad_norm": 2.388298988342285,
      "learning_rate": 8.301862424330732e-05,
      "loss": 1.2143,
      "step": 266500
    },
    {
      "epoch": 3.513481504875449,
      "grad_norm": 2.0847041606903076,
      "learning_rate": 8.298550859020042e-05,
      "loss": 1.2049,
      "step": 267000
    },
    {
      "epoch": 3.5200610582553655,
      "grad_norm": 2.3262248039245605,
      "learning_rate": 8.295239293709351e-05,
      "loss": 1.2098,
      "step": 267500
    },
    {
      "epoch": 3.5266406116352824,
      "grad_norm": 2.560687303543091,
      "learning_rate": 8.291934351529281e-05,
      "loss": 1.2037,
      "step": 268000
    },
    {
      "epoch": 3.533220165015199,
      "grad_norm": 2.1923577785491943,
      "learning_rate": 8.28862278621859e-05,
      "loss": 1.2125,
      "step": 268500
    },
    {
      "epoch": 3.5397997183951153,
      "grad_norm": 2.6301488876342773,
      "learning_rate": 8.285311220907898e-05,
      "loss": 1.2074,
      "step": 269000
    },
    {
      "epoch": 3.5463792717750318,
      "grad_norm": 2.38020920753479,
      "learning_rate": 8.281999655597208e-05,
      "loss": 1.21,
      "step": 269500
    },
    {
      "epoch": 3.552958825154948,
      "grad_norm": 2.5946176052093506,
      "learning_rate": 8.278688090286517e-05,
      "loss": 1.1989,
      "step": 270000
    },
    {
      "epoch": 3.559538378534865,
      "grad_norm": 2.3871052265167236,
      "learning_rate": 8.275376524975827e-05,
      "loss": 1.2068,
      "step": 270500
    },
    {
      "epoch": 3.5661179319147815,
      "grad_norm": 2.292628526687622,
      "learning_rate": 8.272064959665134e-05,
      "loss": 1.2184,
      "step": 271000
    },
    {
      "epoch": 3.5726974852946984,
      "grad_norm": 2.379471778869629,
      "learning_rate": 8.268753394354444e-05,
      "loss": 1.2026,
      "step": 271500
    },
    {
      "epoch": 3.579277038674615,
      "grad_norm": 2.5694143772125244,
      "learning_rate": 8.265448452174374e-05,
      "loss": 1.213,
      "step": 272000
    },
    {
      "epoch": 3.5858565920545313,
      "grad_norm": 2.2886180877685547,
      "learning_rate": 8.262136886863683e-05,
      "loss": 1.2013,
      "step": 272500
    },
    {
      "epoch": 3.592436145434448,
      "grad_norm": 2.4485023021698,
      "learning_rate": 8.258825321552993e-05,
      "loss": 1.2018,
      "step": 273000
    },
    {
      "epoch": 3.5990156988143642,
      "grad_norm": 2.425260066986084,
      "learning_rate": 8.255513756242301e-05,
      "loss": 1.2005,
      "step": 273500
    },
    {
      "epoch": 3.605595252194281,
      "grad_norm": 2.371230125427246,
      "learning_rate": 8.25220219093161e-05,
      "loss": 1.2075,
      "step": 274000
    },
    {
      "epoch": 3.6121748055741976,
      "grad_norm": 2.9194254875183105,
      "learning_rate": 8.24889724875154e-05,
      "loss": 1.2086,
      "step": 274500
    },
    {
      "epoch": 3.6187543589541145,
      "grad_norm": 2.2789318561553955,
      "learning_rate": 8.24558568344085e-05,
      "loss": 1.1994,
      "step": 275000
    },
    {
      "epoch": 3.625333912334031,
      "grad_norm": 2.322998046875,
      "learning_rate": 8.242274118130157e-05,
      "loss": 1.2022,
      "step": 275500
    },
    {
      "epoch": 3.6319134657139474,
      "grad_norm": 2.412785053253174,
      "learning_rate": 8.238969175950089e-05,
      "loss": 1.1973,
      "step": 276000
    },
    {
      "epoch": 3.638493019093864,
      "grad_norm": 2.2022902965545654,
      "learning_rate": 8.235657610639398e-05,
      "loss": 1.2031,
      "step": 276500
    },
    {
      "epoch": 3.6450725724737802,
      "grad_norm": 2.585684299468994,
      "learning_rate": 8.232346045328706e-05,
      "loss": 1.1952,
      "step": 277000
    },
    {
      "epoch": 3.651652125853697,
      "grad_norm": 2.411311388015747,
      "learning_rate": 8.229034480018015e-05,
      "loss": 1.2083,
      "step": 277500
    },
    {
      "epoch": 3.6582316792336136,
      "grad_norm": 2.386382579803467,
      "learning_rate": 8.225722914707325e-05,
      "loss": 1.1992,
      "step": 278000
    },
    {
      "epoch": 3.66481123261353,
      "grad_norm": 2.320072650909424,
      "learning_rate": 8.222411349396633e-05,
      "loss": 1.1975,
      "step": 278500
    },
    {
      "epoch": 3.671390785993447,
      "grad_norm": 2.480111598968506,
      "learning_rate": 8.219099784085942e-05,
      "loss": 1.2002,
      "step": 279000
    },
    {
      "epoch": 3.6779703393733634,
      "grad_norm": 2.214592933654785,
      "learning_rate": 8.215788218775252e-05,
      "loss": 1.1916,
      "step": 279500
    },
    {
      "epoch": 3.68454989275328,
      "grad_norm": 2.2579143047332764,
      "learning_rate": 8.21247665346456e-05,
      "loss": 1.1977,
      "step": 280000
    },
    {
      "epoch": 3.6911294461331963,
      "grad_norm": 2.2399911880493164,
      "learning_rate": 8.209165088153869e-05,
      "loss": 1.1943,
      "step": 280500
    },
    {
      "epoch": 3.697708999513113,
      "grad_norm": 2.624103546142578,
      "learning_rate": 8.205853522843177e-05,
      "loss": 1.1924,
      "step": 281000
    },
    {
      "epoch": 3.7042885528930296,
      "grad_norm": 2.2250113487243652,
      "learning_rate": 8.202548580663109e-05,
      "loss": 1.1995,
      "step": 281500
    },
    {
      "epoch": 3.710868106272946,
      "grad_norm": 2.2924795150756836,
      "learning_rate": 8.199237015352416e-05,
      "loss": 1.1927,
      "step": 282000
    },
    {
      "epoch": 3.717447659652863,
      "grad_norm": 2.4318103790283203,
      "learning_rate": 8.195925450041726e-05,
      "loss": 1.1956,
      "step": 282500
    },
    {
      "epoch": 3.7240272130327794,
      "grad_norm": 2.467191219329834,
      "learning_rate": 8.192613884731035e-05,
      "loss": 1.1982,
      "step": 283000
    },
    {
      "epoch": 3.730606766412696,
      "grad_norm": 2.3096370697021484,
      "learning_rate": 8.189302319420344e-05,
      "loss": 1.2038,
      "step": 283500
    },
    {
      "epoch": 3.7371863197926123,
      "grad_norm": 2.2882919311523438,
      "learning_rate": 8.185997377240274e-05,
      "loss": 1.1951,
      "step": 284000
    },
    {
      "epoch": 3.743765873172529,
      "grad_norm": 2.4755373001098633,
      "learning_rate": 8.182692435060204e-05,
      "loss": 1.2023,
      "step": 284500
    },
    {
      "epoch": 3.7503454265524456,
      "grad_norm": 2.4108283519744873,
      "learning_rate": 8.179380869749514e-05,
      "loss": 1.194,
      "step": 285000
    },
    {
      "epoch": 3.756924979932362,
      "grad_norm": 2.6548471450805664,
      "learning_rate": 8.176069304438823e-05,
      "loss": 1.2015,
      "step": 285500
    },
    {
      "epoch": 3.763504533312279,
      "grad_norm": 2.4423532485961914,
      "learning_rate": 8.172757739128132e-05,
      "loss": 1.2008,
      "step": 286000
    },
    {
      "epoch": 3.7700840866921954,
      "grad_norm": 2.4098737239837646,
      "learning_rate": 8.16944617381744e-05,
      "loss": 1.1995,
      "step": 286500
    },
    {
      "epoch": 3.776663640072112,
      "grad_norm": 2.4816067218780518,
      "learning_rate": 8.16613460850675e-05,
      "loss": 1.1999,
      "step": 287000
    },
    {
      "epoch": 3.7832431934520283,
      "grad_norm": 2.144608736038208,
      "learning_rate": 8.162823043196058e-05,
      "loss": 1.2114,
      "step": 287500
    },
    {
      "epoch": 3.789822746831945,
      "grad_norm": 2.31813383102417,
      "learning_rate": 8.159511477885368e-05,
      "loss": 1.193,
      "step": 288000
    },
    {
      "epoch": 3.7964023002118616,
      "grad_norm": 2.3753108978271484,
      "learning_rate": 8.156206535705297e-05,
      "loss": 1.1973,
      "step": 288500
    },
    {
      "epoch": 3.802981853591778,
      "grad_norm": 2.5623178482055664,
      "learning_rate": 8.152894970394607e-05,
      "loss": 1.1897,
      "step": 289000
    },
    {
      "epoch": 3.809561406971695,
      "grad_norm": 2.2152416706085205,
      "learning_rate": 8.149583405083915e-05,
      "loss": 1.1963,
      "step": 289500
    },
    {
      "epoch": 3.8161409603516114,
      "grad_norm": 2.8247406482696533,
      "learning_rate": 8.146271839773224e-05,
      "loss": 1.193,
      "step": 290000
    },
    {
      "epoch": 3.822720513731528,
      "grad_norm": 2.4626312255859375,
      "learning_rate": 8.142966897593154e-05,
      "loss": 1.1948,
      "step": 290500
    },
    {
      "epoch": 3.8293000671114443,
      "grad_norm": 2.404778242111206,
      "learning_rate": 8.139655332282463e-05,
      "loss": 1.1994,
      "step": 291000
    },
    {
      "epoch": 3.8358796204913608,
      "grad_norm": 2.276329517364502,
      "learning_rate": 8.136343766971773e-05,
      "loss": 1.1974,
      "step": 291500
    },
    {
      "epoch": 3.8424591738712777,
      "grad_norm": 2.12136173248291,
      "learning_rate": 8.133032201661081e-05,
      "loss": 1.1931,
      "step": 292000
    },
    {
      "epoch": 3.849038727251194,
      "grad_norm": 2.3764119148254395,
      "learning_rate": 8.129720636350391e-05,
      "loss": 1.1982,
      "step": 292500
    },
    {
      "epoch": 3.855618280631111,
      "grad_norm": 2.6676480770111084,
      "learning_rate": 8.126409071039698e-05,
      "loss": 1.1977,
      "step": 293000
    },
    {
      "epoch": 3.8621978340110275,
      "grad_norm": 2.5244405269622803,
      "learning_rate": 8.123097505729008e-05,
      "loss": 1.1892,
      "step": 293500
    },
    {
      "epoch": 3.868777387390944,
      "grad_norm": 2.238534450531006,
      "learning_rate": 8.11979918667956e-05,
      "loss": 1.1886,
      "step": 294000
    },
    {
      "epoch": 3.8753569407708603,
      "grad_norm": 2.3235809803009033,
      "learning_rate": 8.116487621368869e-05,
      "loss": 1.1962,
      "step": 294500
    },
    {
      "epoch": 3.881936494150777,
      "grad_norm": 2.485750913619995,
      "learning_rate": 8.113182679188798e-05,
      "loss": 1.1927,
      "step": 295000
    },
    {
      "epoch": 3.8885160475306937,
      "grad_norm": 2.5291354656219482,
      "learning_rate": 8.109871113878108e-05,
      "loss": 1.1989,
      "step": 295500
    },
    {
      "epoch": 3.89509560091061,
      "grad_norm": 2.2003393173217773,
      "learning_rate": 8.106559548567417e-05,
      "loss": 1.1833,
      "step": 296000
    },
    {
      "epoch": 3.901675154290527,
      "grad_norm": 2.4012579917907715,
      "learning_rate": 8.103247983256727e-05,
      "loss": 1.1872,
      "step": 296500
    },
    {
      "epoch": 3.9082547076704435,
      "grad_norm": 2.4072556495666504,
      "learning_rate": 8.099949664207278e-05,
      "loss": 1.1877,
      "step": 297000
    },
    {
      "epoch": 3.91483426105036,
      "grad_norm": 2.439957618713379,
      "learning_rate": 8.096638098896588e-05,
      "loss": 1.1952,
      "step": 297500
    },
    {
      "epoch": 3.9214138144302764,
      "grad_norm": 2.240027666091919,
      "learning_rate": 8.093326533585895e-05,
      "loss": 1.1881,
      "step": 298000
    },
    {
      "epoch": 3.927993367810193,
      "grad_norm": 2.5832433700561523,
      "learning_rate": 8.090014968275205e-05,
      "loss": 1.1874,
      "step": 298500
    },
    {
      "epoch": 3.9345729211901097,
      "grad_norm": 2.325484037399292,
      "learning_rate": 8.086703402964513e-05,
      "loss": 1.1819,
      "step": 299000
    },
    {
      "epoch": 3.941152474570026,
      "grad_norm": 2.3594062328338623,
      "learning_rate": 8.083391837653823e-05,
      "loss": 1.1888,
      "step": 299500
    },
    {
      "epoch": 3.9477320279499426,
      "grad_norm": 2.6077771186828613,
      "learning_rate": 8.080080272343132e-05,
      "loss": 1.1827,
      "step": 300000
    },
    {
      "epoch": 3.9543115813298595,
      "grad_norm": 2.5689258575439453,
      "learning_rate": 8.07676870703244e-05,
      "loss": 1.1847,
      "step": 300500
    },
    {
      "epoch": 3.960891134709776,
      "grad_norm": 2.5444767475128174,
      "learning_rate": 8.07345714172175e-05,
      "loss": 1.1855,
      "step": 301000
    },
    {
      "epoch": 3.9674706880896924,
      "grad_norm": 2.4563231468200684,
      "learning_rate": 8.070145576411057e-05,
      "loss": 1.1757,
      "step": 301500
    },
    {
      "epoch": 3.974050241469609,
      "grad_norm": 2.902449131011963,
      "learning_rate": 8.066834011100367e-05,
      "loss": 1.1848,
      "step": 302000
    },
    {
      "epoch": 3.9806297948495257,
      "grad_norm": 2.378872871398926,
      "learning_rate": 8.063522445789676e-05,
      "loss": 1.1884,
      "step": 302500
    },
    {
      "epoch": 3.987209348229442,
      "grad_norm": 2.431849241256714,
      "learning_rate": 8.060210880478986e-05,
      "loss": 1.1791,
      "step": 303000
    },
    {
      "epoch": 3.9937889016093586,
      "grad_norm": 2.4756691455841064,
      "learning_rate": 8.056899315168294e-05,
      "loss": 1.1799,
      "step": 303500
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.1265121698379517,
      "eval_runtime": 49.5306,
      "eval_samples_per_second": 2018.955,
      "eval_steps_per_second": 15.788,
      "step": 303972
    },
    {
      "epoch": 4.0003684549892755,
      "grad_norm": 2.562776803970337,
      "learning_rate": 8.053587749857603e-05,
      "loss": 1.1874,
      "step": 304000
    },
    {
      "epoch": 4.006948008369192,
      "grad_norm": 2.405233383178711,
      "learning_rate": 8.050276184546913e-05,
      "loss": 1.1773,
      "step": 304500
    },
    {
      "epoch": 4.013527561749108,
      "grad_norm": 2.424314498901367,
      "learning_rate": 8.046971242366842e-05,
      "loss": 1.1743,
      "step": 305000
    },
    {
      "epoch": 4.020107115129025,
      "grad_norm": 2.4356303215026855,
      "learning_rate": 8.043659677056152e-05,
      "loss": 1.1772,
      "step": 305500
    },
    {
      "epoch": 4.026686668508941,
      "grad_norm": 2.3696489334106445,
      "learning_rate": 8.04034811174546e-05,
      "loss": 1.1762,
      "step": 306000
    },
    {
      "epoch": 4.033266221888858,
      "grad_norm": 2.347644329071045,
      "learning_rate": 8.03703654643477e-05,
      "loss": 1.1781,
      "step": 306500
    },
    {
      "epoch": 4.039845775268775,
      "grad_norm": 2.496910572052002,
      "learning_rate": 8.033724981124077e-05,
      "loss": 1.178,
      "step": 307000
    },
    {
      "epoch": 4.0464253286486915,
      "grad_norm": 2.5871057510375977,
      "learning_rate": 8.030413415813387e-05,
      "loss": 1.1809,
      "step": 307500
    },
    {
      "epoch": 4.053004882028608,
      "grad_norm": 2.7480053901672363,
      "learning_rate": 8.027101850502696e-05,
      "loss": 1.1775,
      "step": 308000
    },
    {
      "epoch": 4.059584435408524,
      "grad_norm": 2.3386242389678955,
      "learning_rate": 8.023796908322626e-05,
      "loss": 1.1872,
      "step": 308500
    },
    {
      "epoch": 4.066163988788441,
      "grad_norm": 2.3449814319610596,
      "learning_rate": 8.020485343011936e-05,
      "loss": 1.1749,
      "step": 309000
    },
    {
      "epoch": 4.072743542168357,
      "grad_norm": 2.3893465995788574,
      "learning_rate": 8.017173777701244e-05,
      "loss": 1.1687,
      "step": 309500
    },
    {
      "epoch": 4.079323095548274,
      "grad_norm": 2.3559954166412354,
      "learning_rate": 8.013862212390553e-05,
      "loss": 1.1752,
      "step": 310000
    },
    {
      "epoch": 4.085902648928191,
      "grad_norm": 2.318516254425049,
      "learning_rate": 8.010550647079862e-05,
      "loss": 1.1683,
      "step": 310500
    },
    {
      "epoch": 4.0924822023081076,
      "grad_norm": 2.654291868209839,
      "learning_rate": 8.007239081769171e-05,
      "loss": 1.1795,
      "step": 311000
    },
    {
      "epoch": 4.099061755688024,
      "grad_norm": 2.2922372817993164,
      "learning_rate": 8.0039341395891e-05,
      "loss": 1.1705,
      "step": 311500
    },
    {
      "epoch": 4.1056413090679404,
      "grad_norm": 2.364032030105591,
      "learning_rate": 8.00062257427841e-05,
      "loss": 1.1749,
      "step": 312000
    },
    {
      "epoch": 4.112220862447857,
      "grad_norm": 2.4406864643096924,
      "learning_rate": 7.997311008967719e-05,
      "loss": 1.1816,
      "step": 312500
    },
    {
      "epoch": 4.118800415827773,
      "grad_norm": 2.3085551261901855,
      "learning_rate": 7.993999443657029e-05,
      "loss": 1.1752,
      "step": 313000
    },
    {
      "epoch": 4.12537996920769,
      "grad_norm": 2.859773635864258,
      "learning_rate": 7.990687878346337e-05,
      "loss": 1.1766,
      "step": 313500
    },
    {
      "epoch": 4.131959522587607,
      "grad_norm": 2.383622407913208,
      "learning_rate": 7.987376313035646e-05,
      "loss": 1.1823,
      "step": 314000
    },
    {
      "epoch": 4.138539075967524,
      "grad_norm": 2.5634286403656006,
      "learning_rate": 7.984064747724956e-05,
      "loss": 1.1743,
      "step": 314500
    },
    {
      "epoch": 4.14511862934744,
      "grad_norm": 2.2166714668273926,
      "learning_rate": 7.980753182414264e-05,
      "loss": 1.166,
      "step": 315000
    },
    {
      "epoch": 4.1516981827273565,
      "grad_norm": 2.4063637256622314,
      "learning_rate": 7.977441617103573e-05,
      "loss": 1.1712,
      "step": 315500
    },
    {
      "epoch": 4.158277736107273,
      "grad_norm": 2.3581361770629883,
      "learning_rate": 7.974130051792881e-05,
      "loss": 1.1706,
      "step": 316000
    },
    {
      "epoch": 4.164857289487189,
      "grad_norm": 2.4012577533721924,
      "learning_rate": 7.970825109612812e-05,
      "loss": 1.1711,
      "step": 316500
    },
    {
      "epoch": 4.171436842867106,
      "grad_norm": 2.3343889713287354,
      "learning_rate": 7.96751354430212e-05,
      "loss": 1.1742,
      "step": 317000
    },
    {
      "epoch": 4.178016396247023,
      "grad_norm": 2.2167980670928955,
      "learning_rate": 7.96420197899143e-05,
      "loss": 1.1794,
      "step": 317500
    },
    {
      "epoch": 4.18459594962694,
      "grad_norm": 2.3222687244415283,
      "learning_rate": 7.960890413680739e-05,
      "loss": 1.1667,
      "step": 318000
    },
    {
      "epoch": 4.191175503006856,
      "grad_norm": 2.511897563934326,
      "learning_rate": 7.957578848370047e-05,
      "loss": 1.1704,
      "step": 318500
    },
    {
      "epoch": 4.1977550563867725,
      "grad_norm": 2.3208396434783936,
      "learning_rate": 7.954267283059357e-05,
      "loss": 1.1664,
      "step": 319000
    },
    {
      "epoch": 4.204334609766689,
      "grad_norm": 2.368762254714966,
      "learning_rate": 7.950955717748666e-05,
      "loss": 1.1755,
      "step": 319500
    },
    {
      "epoch": 4.210914163146605,
      "grad_norm": 2.5871846675872803,
      "learning_rate": 7.947644152437976e-05,
      "loss": 1.1702,
      "step": 320000
    },
    {
      "epoch": 4.217493716526522,
      "grad_norm": 2.3679428100585938,
      "learning_rate": 7.944332587127283e-05,
      "loss": 1.1774,
      "step": 320500
    },
    {
      "epoch": 4.224073269906439,
      "grad_norm": 2.224456548690796,
      "learning_rate": 7.941021021816593e-05,
      "loss": 1.1771,
      "step": 321000
    },
    {
      "epoch": 4.230652823286356,
      "grad_norm": 2.4018735885620117,
      "learning_rate": 7.937716079636522e-05,
      "loss": 1.1656,
      "step": 321500
    },
    {
      "epoch": 4.237232376666272,
      "grad_norm": 2.6412768363952637,
      "learning_rate": 7.934411137456454e-05,
      "loss": 1.1713,
      "step": 322000
    },
    {
      "epoch": 4.2438119300461885,
      "grad_norm": 2.8690853118896484,
      "learning_rate": 7.931099572145762e-05,
      "loss": 1.1696,
      "step": 322500
    },
    {
      "epoch": 4.250391483426105,
      "grad_norm": 2.2740418910980225,
      "learning_rate": 7.927788006835071e-05,
      "loss": 1.1665,
      "step": 323000
    },
    {
      "epoch": 4.256971036806021,
      "grad_norm": 2.7466273307800293,
      "learning_rate": 7.924476441524379e-05,
      "loss": 1.1621,
      "step": 323500
    },
    {
      "epoch": 4.263550590185938,
      "grad_norm": 2.454329013824463,
      "learning_rate": 7.921164876213689e-05,
      "loss": 1.1702,
      "step": 324000
    },
    {
      "epoch": 4.270130143565854,
      "grad_norm": 2.369509696960449,
      "learning_rate": 7.91785993403362e-05,
      "loss": 1.1743,
      "step": 324500
    },
    {
      "epoch": 4.276709696945772,
      "grad_norm": 2.2599847316741943,
      "learning_rate": 7.914548368722928e-05,
      "loss": 1.1679,
      "step": 325000
    },
    {
      "epoch": 4.283289250325688,
      "grad_norm": 2.3831448554992676,
      "learning_rate": 7.911236803412238e-05,
      "loss": 1.1649,
      "step": 325500
    },
    {
      "epoch": 4.2898688037056045,
      "grad_norm": 2.3481438159942627,
      "learning_rate": 7.907925238101547e-05,
      "loss": 1.1728,
      "step": 326000
    },
    {
      "epoch": 4.296448357085521,
      "grad_norm": 10.510360717773438,
      "learning_rate": 7.904613672790855e-05,
      "loss": 1.1637,
      "step": 326500
    },
    {
      "epoch": 4.303027910465437,
      "grad_norm": 2.242670774459839,
      "learning_rate": 7.901302107480164e-05,
      "loss": 1.162,
      "step": 327000
    },
    {
      "epoch": 4.309607463845354,
      "grad_norm": 2.558157205581665,
      "learning_rate": 7.897990542169474e-05,
      "loss": 1.1724,
      "step": 327500
    },
    {
      "epoch": 4.316187017225271,
      "grad_norm": 2.5078272819519043,
      "learning_rate": 7.894678976858782e-05,
      "loss": 1.1714,
      "step": 328000
    },
    {
      "epoch": 4.322766570605188,
      "grad_norm": 2.3764703273773193,
      "learning_rate": 7.89136741154809e-05,
      "loss": 1.1574,
      "step": 328500
    },
    {
      "epoch": 4.329346123985104,
      "grad_norm": 2.4530715942382812,
      "learning_rate": 7.8880558462374e-05,
      "loss": 1.1646,
      "step": 329000
    },
    {
      "epoch": 4.3359256773650205,
      "grad_norm": 2.609919309616089,
      "learning_rate": 7.88475090405733e-05,
      "loss": 1.1595,
      "step": 329500
    },
    {
      "epoch": 4.342505230744937,
      "grad_norm": 2.527956962585449,
      "learning_rate": 7.88144596187726e-05,
      "loss": 1.1702,
      "step": 330000
    },
    {
      "epoch": 4.349084784124853,
      "grad_norm": 2.2943837642669678,
      "learning_rate": 7.87813439656657e-05,
      "loss": 1.1587,
      "step": 330500
    },
    {
      "epoch": 4.35566433750477,
      "grad_norm": 2.4824116230010986,
      "learning_rate": 7.874822831255879e-05,
      "loss": 1.1681,
      "step": 331000
    },
    {
      "epoch": 4.362243890884686,
      "grad_norm": 2.353828191757202,
      "learning_rate": 7.871511265945187e-05,
      "loss": 1.1695,
      "step": 331500
    },
    {
      "epoch": 4.368823444264604,
      "grad_norm": 2.2528090476989746,
      "learning_rate": 7.868199700634497e-05,
      "loss": 1.1667,
      "step": 332000
    },
    {
      "epoch": 4.37540299764452,
      "grad_norm": 2.382769823074341,
      "learning_rate": 7.864894758454426e-05,
      "loss": 1.1725,
      "step": 332500
    },
    {
      "epoch": 4.381982551024437,
      "grad_norm": 2.7050812244415283,
      "learning_rate": 7.861583193143736e-05,
      "loss": 1.1649,
      "step": 333000
    },
    {
      "epoch": 4.388562104404353,
      "grad_norm": 2.3551840782165527,
      "learning_rate": 7.858271627833045e-05,
      "loss": 1.163,
      "step": 333500
    },
    {
      "epoch": 4.3951416577842695,
      "grad_norm": 2.5145230293273926,
      "learning_rate": 7.854960062522353e-05,
      "loss": 1.1652,
      "step": 334000
    },
    {
      "epoch": 4.401721211164186,
      "grad_norm": 2.5143544673919678,
      "learning_rate": 7.851648497211662e-05,
      "loss": 1.1656,
      "step": 334500
    },
    {
      "epoch": 4.408300764544102,
      "grad_norm": 2.6212499141693115,
      "learning_rate": 7.848343555031593e-05,
      "loss": 1.166,
      "step": 335000
    },
    {
      "epoch": 4.41488031792402,
      "grad_norm": 2.636125087738037,
      "learning_rate": 7.845038612851523e-05,
      "loss": 1.1605,
      "step": 335500
    },
    {
      "epoch": 4.421459871303936,
      "grad_norm": 2.4039530754089355,
      "learning_rate": 7.841733670671453e-05,
      "loss": 1.1664,
      "step": 336000
    },
    {
      "epoch": 4.428039424683853,
      "grad_norm": 2.382403612136841,
      "learning_rate": 7.838422105360762e-05,
      "loss": 1.1684,
      "step": 336500
    },
    {
      "epoch": 4.434618978063769,
      "grad_norm": 2.3169827461242676,
      "learning_rate": 7.835110540050071e-05,
      "loss": 1.1589,
      "step": 337000
    },
    {
      "epoch": 4.4411985314436855,
      "grad_norm": 2.4434304237365723,
      "learning_rate": 7.83179897473938e-05,
      "loss": 1.1564,
      "step": 337500
    },
    {
      "epoch": 4.447778084823602,
      "grad_norm": 2.436652421951294,
      "learning_rate": 7.828487409428689e-05,
      "loss": 1.1638,
      "step": 338000
    },
    {
      "epoch": 4.454357638203518,
      "grad_norm": 2.640244245529175,
      "learning_rate": 7.825175844117998e-05,
      "loss": 1.1689,
      "step": 338500
    },
    {
      "epoch": 4.460937191583436,
      "grad_norm": 3.0943877696990967,
      "learning_rate": 7.821864278807307e-05,
      "loss": 1.1717,
      "step": 339000
    },
    {
      "epoch": 4.467516744963352,
      "grad_norm": 2.362135648727417,
      "learning_rate": 7.818552713496617e-05,
      "loss": 1.1617,
      "step": 339500
    },
    {
      "epoch": 4.474096298343269,
      "grad_norm": 2.4063570499420166,
      "learning_rate": 7.815247771316546e-05,
      "loss": 1.1518,
      "step": 340000
    },
    {
      "epoch": 4.480675851723185,
      "grad_norm": 2.3943772315979004,
      "learning_rate": 7.811936206005856e-05,
      "loss": 1.1594,
      "step": 340500
    },
    {
      "epoch": 4.4872554051031015,
      "grad_norm": 2.9817521572113037,
      "learning_rate": 7.808624640695164e-05,
      "loss": 1.1641,
      "step": 341000
    },
    {
      "epoch": 4.493834958483018,
      "grad_norm": 2.3645451068878174,
      "learning_rate": 7.805313075384473e-05,
      "loss": 1.1617,
      "step": 341500
    },
    {
      "epoch": 4.500414511862934,
      "grad_norm": 2.746504306793213,
      "learning_rate": 7.802001510073781e-05,
      "loss": 1.1714,
      "step": 342000
    },
    {
      "epoch": 4.506994065242852,
      "grad_norm": 2.517317295074463,
      "learning_rate": 7.798689944763091e-05,
      "loss": 1.169,
      "step": 342500
    },
    {
      "epoch": 4.513573618622768,
      "grad_norm": 2.462679147720337,
      "learning_rate": 7.7953783794524e-05,
      "loss": 1.1637,
      "step": 343000
    },
    {
      "epoch": 4.520153172002685,
      "grad_norm": 2.7375524044036865,
      "learning_rate": 7.792066814141708e-05,
      "loss": 1.1574,
      "step": 343500
    },
    {
      "epoch": 4.526732725382601,
      "grad_norm": 2.596801519393921,
      "learning_rate": 7.788755248831018e-05,
      "loss": 1.1616,
      "step": 344000
    },
    {
      "epoch": 4.5333122787625175,
      "grad_norm": 2.1644234657287598,
      "learning_rate": 7.785450306650947e-05,
      "loss": 1.1634,
      "step": 344500
    },
    {
      "epoch": 4.539891832142434,
      "grad_norm": 2.3578009605407715,
      "learning_rate": 7.782138741340257e-05,
      "loss": 1.1639,
      "step": 345000
    },
    {
      "epoch": 4.54647138552235,
      "grad_norm": 2.4385814666748047,
      "learning_rate": 7.778827176029566e-05,
      "loss": 1.1484,
      "step": 345500
    },
    {
      "epoch": 4.553050938902267,
      "grad_norm": 2.2542974948883057,
      "learning_rate": 7.775515610718876e-05,
      "loss": 1.1605,
      "step": 346000
    },
    {
      "epoch": 4.559630492282184,
      "grad_norm": 2.4314639568328857,
      "learning_rate": 7.772204045408183e-05,
      "loss": 1.1529,
      "step": 346500
    },
    {
      "epoch": 4.566210045662101,
      "grad_norm": 4.71856164932251,
      "learning_rate": 7.768892480097493e-05,
      "loss": 1.1633,
      "step": 347000
    },
    {
      "epoch": 4.572789599042017,
      "grad_norm": 2.309116840362549,
      "learning_rate": 7.765580914786801e-05,
      "loss": 1.1726,
      "step": 347500
    },
    {
      "epoch": 4.5793691524219335,
      "grad_norm": 2.5582869052886963,
      "learning_rate": 7.762269349476111e-05,
      "loss": 1.1456,
      "step": 348000
    },
    {
      "epoch": 4.58594870580185,
      "grad_norm": 2.224832534790039,
      "learning_rate": 7.75895778416542e-05,
      "loss": 1.1614,
      "step": 348500
    },
    {
      "epoch": 4.592528259181766,
      "grad_norm": 2.610032796859741,
      "learning_rate": 7.75565284198535e-05,
      "loss": 1.1633,
      "step": 349000
    },
    {
      "epoch": 4.599107812561684,
      "grad_norm": 2.302778720855713,
      "learning_rate": 7.75234127667466e-05,
      "loss": 1.1522,
      "step": 349500
    },
    {
      "epoch": 4.6056873659416,
      "grad_norm": 2.219428062438965,
      "learning_rate": 7.749029711363967e-05,
      "loss": 1.1503,
      "step": 350000
    },
    {
      "epoch": 4.612266919321517,
      "grad_norm": 2.6157732009887695,
      "learning_rate": 7.745718146053277e-05,
      "loss": 1.1545,
      "step": 350500
    },
    {
      "epoch": 4.618846472701433,
      "grad_norm": 2.5879452228546143,
      "learning_rate": 7.742406580742586e-05,
      "loss": 1.1585,
      "step": 351000
    },
    {
      "epoch": 4.62542602608135,
      "grad_norm": 2.50014591217041,
      "learning_rate": 7.739095015431894e-05,
      "loss": 1.1629,
      "step": 351500
    },
    {
      "epoch": 4.632005579461266,
      "grad_norm": 2.6435887813568115,
      "learning_rate": 7.735783450121204e-05,
      "loss": 1.1527,
      "step": 352000
    },
    {
      "epoch": 4.6385851328411825,
      "grad_norm": 2.5283265113830566,
      "learning_rate": 7.732471884810513e-05,
      "loss": 1.1509,
      "step": 352500
    },
    {
      "epoch": 4.645164686221099,
      "grad_norm": 2.0337929725646973,
      "learning_rate": 7.729160319499822e-05,
      "loss": 1.1557,
      "step": 353000
    },
    {
      "epoch": 4.651744239601016,
      "grad_norm": 2.4232330322265625,
      "learning_rate": 7.725855377319752e-05,
      "loss": 1.1509,
      "step": 353500
    },
    {
      "epoch": 4.658323792980933,
      "grad_norm": 2.5143604278564453,
      "learning_rate": 7.722543812009062e-05,
      "loss": 1.1572,
      "step": 354000
    },
    {
      "epoch": 4.664903346360849,
      "grad_norm": 2.4410948753356934,
      "learning_rate": 7.71923224669837e-05,
      "loss": 1.1538,
      "step": 354500
    },
    {
      "epoch": 4.671482899740766,
      "grad_norm": 2.382622480392456,
      "learning_rate": 7.7159273045183e-05,
      "loss": 1.1626,
      "step": 355000
    },
    {
      "epoch": 4.678062453120682,
      "grad_norm": 2.334421396255493,
      "learning_rate": 7.712615739207609e-05,
      "loss": 1.153,
      "step": 355500
    },
    {
      "epoch": 4.6846420065005985,
      "grad_norm": 2.6257834434509277,
      "learning_rate": 7.709304173896919e-05,
      "loss": 1.1503,
      "step": 356000
    },
    {
      "epoch": 4.691221559880515,
      "grad_norm": 2.5762500762939453,
      "learning_rate": 7.705992608586226e-05,
      "loss": 1.1547,
      "step": 356500
    },
    {
      "epoch": 4.697801113260432,
      "grad_norm": 2.475600004196167,
      "learning_rate": 7.702681043275536e-05,
      "loss": 1.1502,
      "step": 357000
    },
    {
      "epoch": 4.704380666640349,
      "grad_norm": 2.5387635231018066,
      "learning_rate": 7.699369477964845e-05,
      "loss": 1.1443,
      "step": 357500
    },
    {
      "epoch": 4.710960220020265,
      "grad_norm": 2.6443521976470947,
      "learning_rate": 7.696057912654153e-05,
      "loss": 1.1521,
      "step": 358000
    },
    {
      "epoch": 4.717539773400182,
      "grad_norm": 2.470890760421753,
      "learning_rate": 7.692746347343463e-05,
      "loss": 1.1602,
      "step": 358500
    },
    {
      "epoch": 4.724119326780098,
      "grad_norm": 2.3856937885284424,
      "learning_rate": 7.689434782032771e-05,
      "loss": 1.1465,
      "step": 359000
    },
    {
      "epoch": 4.7306988801600145,
      "grad_norm": 2.68673038482666,
      "learning_rate": 7.686143086113945e-05,
      "loss": 1.1523,
      "step": 359500
    },
    {
      "epoch": 4.737278433539931,
      "grad_norm": 2.401737689971924,
      "learning_rate": 7.682831520803254e-05,
      "loss": 1.1468,
      "step": 360000
    },
    {
      "epoch": 4.743857986919847,
      "grad_norm": 2.3861732482910156,
      "learning_rate": 7.679519955492563e-05,
      "loss": 1.1423,
      "step": 360500
    },
    {
      "epoch": 4.750437540299765,
      "grad_norm": 2.5515754222869873,
      "learning_rate": 7.676215013312493e-05,
      "loss": 1.1426,
      "step": 361000
    },
    {
      "epoch": 4.757017093679681,
      "grad_norm": 2.355903387069702,
      "learning_rate": 7.672903448001802e-05,
      "loss": 1.1478,
      "step": 361500
    },
    {
      "epoch": 4.763596647059598,
      "grad_norm": 2.426964521408081,
      "learning_rate": 7.66959188269111e-05,
      "loss": 1.153,
      "step": 362000
    },
    {
      "epoch": 4.770176200439514,
      "grad_norm": 2.4509127140045166,
      "learning_rate": 7.66628031738042e-05,
      "loss": 1.1595,
      "step": 362500
    },
    {
      "epoch": 4.7767557538194305,
      "grad_norm": 2.7454993724823,
      "learning_rate": 7.662968752069729e-05,
      "loss": 1.1497,
      "step": 363000
    },
    {
      "epoch": 4.783335307199347,
      "grad_norm": 2.535116195678711,
      "learning_rate": 7.659657186759037e-05,
      "loss": 1.156,
      "step": 363500
    },
    {
      "epoch": 4.789914860579264,
      "grad_norm": 1.968102216720581,
      "learning_rate": 7.656345621448346e-05,
      "loss": 1.1519,
      "step": 364000
    },
    {
      "epoch": 4.796494413959181,
      "grad_norm": 2.43784761428833,
      "learning_rate": 7.653034056137656e-05,
      "loss": 1.1398,
      "step": 364500
    },
    {
      "epoch": 4.803073967339097,
      "grad_norm": 2.537285566329956,
      "learning_rate": 7.649722490826964e-05,
      "loss": 1.1439,
      "step": 365000
    },
    {
      "epoch": 4.809653520719014,
      "grad_norm": 2.4327850341796875,
      "learning_rate": 7.646410925516273e-05,
      "loss": 1.1458,
      "step": 365500
    },
    {
      "epoch": 4.81623307409893,
      "grad_norm": 2.534571886062622,
      "learning_rate": 7.643099360205583e-05,
      "loss": 1.1493,
      "step": 366000
    },
    {
      "epoch": 4.8228126274788465,
      "grad_norm": 2.554833173751831,
      "learning_rate": 7.639787794894891e-05,
      "loss": 1.1533,
      "step": 366500
    },
    {
      "epoch": 4.829392180858763,
      "grad_norm": 2.4664499759674072,
      "learning_rate": 7.636476229584201e-05,
      "loss": 1.1405,
      "step": 367000
    },
    {
      "epoch": 4.835971734238679,
      "grad_norm": 2.4986445903778076,
      "learning_rate": 7.633164664273508e-05,
      "loss": 1.1392,
      "step": 367500
    },
    {
      "epoch": 4.842551287618597,
      "grad_norm": 2.282360553741455,
      "learning_rate": 7.629853098962818e-05,
      "loss": 1.1481,
      "step": 368000
    },
    {
      "epoch": 4.849130840998513,
      "grad_norm": 2.2695279121398926,
      "learning_rate": 7.626541533652127e-05,
      "loss": 1.1507,
      "step": 368500
    },
    {
      "epoch": 4.85571039437843,
      "grad_norm": 2.457422971725464,
      "learning_rate": 7.623229968341437e-05,
      "loss": 1.1346,
      "step": 369000
    },
    {
      "epoch": 4.862289947758346,
      "grad_norm": 2.5155982971191406,
      "learning_rate": 7.619925026161366e-05,
      "loss": 1.1442,
      "step": 369500
    },
    {
      "epoch": 4.868869501138263,
      "grad_norm": 2.3263862133026123,
      "learning_rate": 7.616613460850676e-05,
      "loss": 1.1498,
      "step": 370000
    },
    {
      "epoch": 4.875449054518179,
      "grad_norm": 3.092434883117676,
      "learning_rate": 7.613301895539984e-05,
      "loss": 1.1361,
      "step": 370500
    },
    {
      "epoch": 4.882028607898096,
      "grad_norm": 2.1523241996765137,
      "learning_rate": 7.609996953359915e-05,
      "loss": 1.1357,
      "step": 371000
    },
    {
      "epoch": 4.888608161278013,
      "grad_norm": 2.6484737396240234,
      "learning_rate": 7.606685388049223e-05,
      "loss": 1.1368,
      "step": 371500
    },
    {
      "epoch": 4.895187714657929,
      "grad_norm": 2.1311657428741455,
      "learning_rate": 7.603373822738532e-05,
      "loss": 1.1497,
      "step": 372000
    },
    {
      "epoch": 4.901767268037846,
      "grad_norm": 2.3548803329467773,
      "learning_rate": 7.600062257427842e-05,
      "loss": 1.1381,
      "step": 372500
    },
    {
      "epoch": 4.908346821417762,
      "grad_norm": 2.3356587886810303,
      "learning_rate": 7.59675069211715e-05,
      "loss": 1.1481,
      "step": 373000
    },
    {
      "epoch": 4.914926374797679,
      "grad_norm": 2.281709909439087,
      "learning_rate": 7.59343912680646e-05,
      "loss": 1.1424,
      "step": 373500
    },
    {
      "epoch": 4.921505928177595,
      "grad_norm": 2.4379806518554688,
      "learning_rate": 7.590127561495767e-05,
      "loss": 1.1495,
      "step": 374000
    },
    {
      "epoch": 4.9280854815575115,
      "grad_norm": 2.5542099475860596,
      "learning_rate": 7.586822619315699e-05,
      "loss": 1.1558,
      "step": 374500
    },
    {
      "epoch": 4.934665034937429,
      "grad_norm": 2.3247616291046143,
      "learning_rate": 7.583511054005006e-05,
      "loss": 1.1382,
      "step": 375000
    },
    {
      "epoch": 4.941244588317345,
      "grad_norm": 2.440821409225464,
      "learning_rate": 7.580199488694316e-05,
      "loss": 1.1372,
      "step": 375500
    },
    {
      "epoch": 4.947824141697262,
      "grad_norm": 2.44916033744812,
      "learning_rate": 7.576887923383626e-05,
      "loss": 1.1459,
      "step": 376000
    },
    {
      "epoch": 4.954403695077178,
      "grad_norm": 2.139402151107788,
      "learning_rate": 7.573582981203555e-05,
      "loss": 1.1366,
      "step": 376500
    },
    {
      "epoch": 4.960983248457095,
      "grad_norm": 2.3127048015594482,
      "learning_rate": 7.570271415892865e-05,
      "loss": 1.1418,
      "step": 377000
    },
    {
      "epoch": 4.967562801837011,
      "grad_norm": 2.2646732330322266,
      "learning_rate": 7.566959850582174e-05,
      "loss": 1.1448,
      "step": 377500
    },
    {
      "epoch": 4.9741423552169275,
      "grad_norm": 2.2515718936920166,
      "learning_rate": 7.563648285271483e-05,
      "loss": 1.1471,
      "step": 378000
    },
    {
      "epoch": 4.980721908596845,
      "grad_norm": 2.4985926151275635,
      "learning_rate": 7.56033671996079e-05,
      "loss": 1.136,
      "step": 378500
    },
    {
      "epoch": 4.987301461976761,
      "grad_norm": 2.184616804122925,
      "learning_rate": 7.5570251546501e-05,
      "loss": 1.1483,
      "step": 379000
    },
    {
      "epoch": 4.993881015356678,
      "grad_norm": 2.3420846462249756,
      "learning_rate": 7.553713589339409e-05,
      "loss": 1.1507,
      "step": 379500
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.0848976373672485,
      "eval_runtime": 49.9573,
      "eval_samples_per_second": 2001.708,
      "eval_steps_per_second": 15.653,
      "step": 379965
    },
    {
      "epoch": 5.000460568736594,
      "grad_norm": 2.5178568363189697,
      "learning_rate": 7.550402024028719e-05,
      "loss": 1.1389,
      "step": 380000
    },
    {
      "epoch": 5.007040122116511,
      "grad_norm": 2.6647751331329346,
      "learning_rate": 7.547090458718027e-05,
      "loss": 1.1337,
      "step": 380500
    },
    {
      "epoch": 5.013619675496427,
      "grad_norm": 2.3722431659698486,
      "learning_rate": 7.543778893407336e-05,
      "loss": 1.1302,
      "step": 381000
    },
    {
      "epoch": 5.0201992288763435,
      "grad_norm": 2.4553287029266357,
      "learning_rate": 7.540467328096646e-05,
      "loss": 1.1368,
      "step": 381500
    },
    {
      "epoch": 5.026778782256261,
      "grad_norm": 2.212754964828491,
      "learning_rate": 7.537155762785953e-05,
      "loss": 1.1258,
      "step": 382000
    },
    {
      "epoch": 5.033358335636177,
      "grad_norm": 2.560650587081909,
      "learning_rate": 7.533850820605885e-05,
      "loss": 1.1381,
      "step": 382500
    },
    {
      "epoch": 5.039937889016094,
      "grad_norm": 2.872826099395752,
      "learning_rate": 7.530552501556436e-05,
      "loss": 1.131,
      "step": 383000
    },
    {
      "epoch": 5.04651744239601,
      "grad_norm": 2.448777675628662,
      "learning_rate": 7.527240936245746e-05,
      "loss": 1.1311,
      "step": 383500
    },
    {
      "epoch": 5.053096995775927,
      "grad_norm": 2.5181448459625244,
      "learning_rate": 7.523929370935054e-05,
      "loss": 1.1312,
      "step": 384000
    },
    {
      "epoch": 5.059676549155843,
      "grad_norm": 2.4626035690307617,
      "learning_rate": 7.520617805624363e-05,
      "loss": 1.1383,
      "step": 384500
    },
    {
      "epoch": 5.0662561025357595,
      "grad_norm": 2.2409508228302,
      "learning_rate": 7.517306240313671e-05,
      "loss": 1.1332,
      "step": 385000
    },
    {
      "epoch": 5.072835655915677,
      "grad_norm": 2.262699842453003,
      "learning_rate": 7.513994675002981e-05,
      "loss": 1.1228,
      "step": 385500
    },
    {
      "epoch": 5.079415209295593,
      "grad_norm": 2.4333655834198,
      "learning_rate": 7.510683109692289e-05,
      "loss": 1.1335,
      "step": 386000
    },
    {
      "epoch": 5.08599476267551,
      "grad_norm": 2.025927782058716,
      "learning_rate": 7.50737816751222e-05,
      "loss": 1.1264,
      "step": 386500
    },
    {
      "epoch": 5.092574316055426,
      "grad_norm": 2.4924073219299316,
      "learning_rate": 7.504066602201529e-05,
      "loss": 1.1278,
      "step": 387000
    },
    {
      "epoch": 5.099153869435343,
      "grad_norm": 2.4533708095550537,
      "learning_rate": 7.500755036890837e-05,
      "loss": 1.1368,
      "step": 387500
    },
    {
      "epoch": 5.105733422815259,
      "grad_norm": 2.687056303024292,
      "learning_rate": 7.497443471580147e-05,
      "loss": 1.1305,
      "step": 388000
    },
    {
      "epoch": 5.1123129761951756,
      "grad_norm": 2.1889021396636963,
      "learning_rate": 7.494131906269456e-05,
      "loss": 1.1353,
      "step": 388500
    },
    {
      "epoch": 5.118892529575092,
      "grad_norm": 2.4827685356140137,
      "learning_rate": 7.490820340958766e-05,
      "loss": 1.1352,
      "step": 389000
    },
    {
      "epoch": 5.125472082955009,
      "grad_norm": 2.4609262943267822,
      "learning_rate": 7.487508775648073e-05,
      "loss": 1.1279,
      "step": 389500
    },
    {
      "epoch": 5.132051636334926,
      "grad_norm": 2.5409295558929443,
      "learning_rate": 7.484197210337383e-05,
      "loss": 1.1338,
      "step": 390000
    },
    {
      "epoch": 5.138631189714842,
      "grad_norm": 2.4015135765075684,
      "learning_rate": 7.480885645026691e-05,
      "loss": 1.121,
      "step": 390500
    },
    {
      "epoch": 5.145210743094759,
      "grad_norm": 2.4674136638641357,
      "learning_rate": 7.477580702846622e-05,
      "loss": 1.1276,
      "step": 391000
    },
    {
      "epoch": 5.151790296474675,
      "grad_norm": 5.559149265289307,
      "learning_rate": 7.474275760666552e-05,
      "loss": 1.1369,
      "step": 391500
    },
    {
      "epoch": 5.158369849854592,
      "grad_norm": 2.469104290008545,
      "learning_rate": 7.470964195355861e-05,
      "loss": 1.1235,
      "step": 392000
    },
    {
      "epoch": 5.164949403234508,
      "grad_norm": 2.3622822761535645,
      "learning_rate": 7.46765263004517e-05,
      "loss": 1.1233,
      "step": 392500
    },
    {
      "epoch": 5.171528956614425,
      "grad_norm": 2.564804792404175,
      "learning_rate": 7.464341064734479e-05,
      "loss": 1.1216,
      "step": 393000
    },
    {
      "epoch": 5.178108509994342,
      "grad_norm": 2.564910411834717,
      "learning_rate": 7.461029499423788e-05,
      "loss": 1.1356,
      "step": 393500
    },
    {
      "epoch": 5.184688063374258,
      "grad_norm": 2.499943733215332,
      "learning_rate": 7.457717934113096e-05,
      "loss": 1.1364,
      "step": 394000
    },
    {
      "epoch": 5.191267616754175,
      "grad_norm": 2.4597842693328857,
      "learning_rate": 7.454406368802406e-05,
      "loss": 1.1332,
      "step": 394500
    },
    {
      "epoch": 5.197847170134091,
      "grad_norm": 2.4315643310546875,
      "learning_rate": 7.451094803491715e-05,
      "loss": 1.1356,
      "step": 395000
    },
    {
      "epoch": 5.204426723514008,
      "grad_norm": 2.3930745124816895,
      "learning_rate": 7.447783238181025e-05,
      "loss": 1.1261,
      "step": 395500
    },
    {
      "epoch": 5.211006276893924,
      "grad_norm": 2.4039793014526367,
      "learning_rate": 7.444471672870332e-05,
      "loss": 1.1274,
      "step": 396000
    },
    {
      "epoch": 5.217585830273841,
      "grad_norm": 2.4360427856445312,
      "learning_rate": 7.441179976951506e-05,
      "loss": 1.1308,
      "step": 396500
    },
    {
      "epoch": 5.224165383653758,
      "grad_norm": 2.1750683784484863,
      "learning_rate": 7.437875034771437e-05,
      "loss": 1.1339,
      "step": 397000
    },
    {
      "epoch": 5.230744937033674,
      "grad_norm": 2.7809462547302246,
      "learning_rate": 7.434563469460745e-05,
      "loss": 1.1299,
      "step": 397500
    },
    {
      "epoch": 5.237324490413591,
      "grad_norm": 2.5993616580963135,
      "learning_rate": 7.431258527280676e-05,
      "loss": 1.1363,
      "step": 398000
    },
    {
      "epoch": 5.243904043793507,
      "grad_norm": 2.4637367725372314,
      "learning_rate": 7.427946961969984e-05,
      "loss": 1.1386,
      "step": 398500
    },
    {
      "epoch": 5.250483597173424,
      "grad_norm": 2.529304027557373,
      "learning_rate": 7.424635396659293e-05,
      "loss": 1.1316,
      "step": 399000
    },
    {
      "epoch": 5.25706315055334,
      "grad_norm": 2.247370481491089,
      "learning_rate": 7.421323831348603e-05,
      "loss": 1.1313,
      "step": 399500
    },
    {
      "epoch": 5.263642703933257,
      "grad_norm": 2.6360650062561035,
      "learning_rate": 7.418012266037911e-05,
      "loss": 1.122,
      "step": 400000
    },
    {
      "epoch": 5.270222257313174,
      "grad_norm": 2.423912763595581,
      "learning_rate": 7.41470070072722e-05,
      "loss": 1.1288,
      "step": 400500
    },
    {
      "epoch": 5.27680181069309,
      "grad_norm": 2.2336313724517822,
      "learning_rate": 7.411389135416528e-05,
      "loss": 1.1174,
      "step": 401000
    },
    {
      "epoch": 5.283381364073007,
      "grad_norm": 2.3506040573120117,
      "learning_rate": 7.408077570105838e-05,
      "loss": 1.1284,
      "step": 401500
    },
    {
      "epoch": 5.289960917452923,
      "grad_norm": 2.4965455532073975,
      "learning_rate": 7.404766004795147e-05,
      "loss": 1.1314,
      "step": 402000
    },
    {
      "epoch": 5.29654047083284,
      "grad_norm": 2.227682113647461,
      "learning_rate": 7.401454439484455e-05,
      "loss": 1.1187,
      "step": 402500
    },
    {
      "epoch": 5.303120024212756,
      "grad_norm": 2.5321543216705322,
      "learning_rate": 7.398142874173765e-05,
      "loss": 1.1271,
      "step": 403000
    },
    {
      "epoch": 5.309699577592673,
      "grad_norm": 2.3653008937835693,
      "learning_rate": 7.394831308863074e-05,
      "loss": 1.1243,
      "step": 403500
    },
    {
      "epoch": 5.31627913097259,
      "grad_norm": 2.254655122756958,
      "learning_rate": 7.391519743552383e-05,
      "loss": 1.1313,
      "step": 404000
    },
    {
      "epoch": 5.322858684352506,
      "grad_norm": 2.457047700881958,
      "learning_rate": 7.38820817824169e-05,
      "loss": 1.1214,
      "step": 404500
    },
    {
      "epoch": 5.329438237732423,
      "grad_norm": 2.2707748413085938,
      "learning_rate": 7.384896612931e-05,
      "loss": 1.1237,
      "step": 405000
    },
    {
      "epoch": 5.336017791112339,
      "grad_norm": 2.8224291801452637,
      "learning_rate": 7.381585047620309e-05,
      "loss": 1.1198,
      "step": 405500
    },
    {
      "epoch": 5.342597344492256,
      "grad_norm": 2.3803062438964844,
      "learning_rate": 7.378273482309619e-05,
      "loss": 1.1257,
      "step": 406000
    },
    {
      "epoch": 5.349176897872172,
      "grad_norm": 2.6056747436523438,
      "learning_rate": 7.374961916998928e-05,
      "loss": 1.1243,
      "step": 406500
    },
    {
      "epoch": 5.355756451252089,
      "grad_norm": 2.463562488555908,
      "learning_rate": 7.371656974818858e-05,
      "loss": 1.1371,
      "step": 407000
    },
    {
      "epoch": 5.362336004632006,
      "grad_norm": 2.2453246116638184,
      "learning_rate": 7.368345409508168e-05,
      "loss": 1.1307,
      "step": 407500
    },
    {
      "epoch": 5.368915558011922,
      "grad_norm": 2.981584072113037,
      "learning_rate": 7.365033844197475e-05,
      "loss": 1.1233,
      "step": 408000
    },
    {
      "epoch": 5.375495111391839,
      "grad_norm": 2.5733072757720947,
      "learning_rate": 7.361728902017407e-05,
      "loss": 1.1219,
      "step": 408500
    },
    {
      "epoch": 5.382074664771755,
      "grad_norm": 2.479353666305542,
      "learning_rate": 7.358417336706714e-05,
      "loss": 1.1203,
      "step": 409000
    },
    {
      "epoch": 5.388654218151672,
      "grad_norm": 2.402780055999756,
      "learning_rate": 7.355105771396024e-05,
      "loss": 1.1225,
      "step": 409500
    },
    {
      "epoch": 5.395233771531588,
      "grad_norm": 2.553281307220459,
      "learning_rate": 7.351794206085332e-05,
      "loss": 1.1206,
      "step": 410000
    },
    {
      "epoch": 5.401813324911505,
      "grad_norm": 2.3416848182678223,
      "learning_rate": 7.348482640774642e-05,
      "loss": 1.122,
      "step": 410500
    },
    {
      "epoch": 5.408392878291422,
      "grad_norm": 2.394042730331421,
      "learning_rate": 7.345171075463951e-05,
      "loss": 1.1265,
      "step": 411000
    },
    {
      "epoch": 5.414972431671338,
      "grad_norm": 2.104586362838745,
      "learning_rate": 7.34185951015326e-05,
      "loss": 1.1239,
      "step": 411500
    },
    {
      "epoch": 5.421551985051255,
      "grad_norm": 2.4536426067352295,
      "learning_rate": 7.338547944842569e-05,
      "loss": 1.1274,
      "step": 412000
    },
    {
      "epoch": 5.428131538431171,
      "grad_norm": 2.7629101276397705,
      "learning_rate": 7.335236379531878e-05,
      "loss": 1.1224,
      "step": 412500
    },
    {
      "epoch": 5.434711091811088,
      "grad_norm": 2.1670525074005127,
      "learning_rate": 7.331924814221186e-05,
      "loss": 1.1189,
      "step": 413000
    },
    {
      "epoch": 5.441290645191004,
      "grad_norm": 2.3601973056793213,
      "learning_rate": 7.328613248910495e-05,
      "loss": 1.1206,
      "step": 413500
    },
    {
      "epoch": 5.4478701985709215,
      "grad_norm": 2.533322811126709,
      "learning_rate": 7.325301683599805e-05,
      "loss": 1.1223,
      "step": 414000
    },
    {
      "epoch": 5.454449751950838,
      "grad_norm": 2.5271947383880615,
      "learning_rate": 7.321996741419734e-05,
      "loss": 1.1248,
      "step": 414500
    },
    {
      "epoch": 5.461029305330754,
      "grad_norm": 3.428253650665283,
      "learning_rate": 7.318685176109044e-05,
      "loss": 1.1248,
      "step": 415000
    },
    {
      "epoch": 5.467608858710671,
      "grad_norm": 2.401284694671631,
      "learning_rate": 7.315373610798352e-05,
      "loss": 1.1254,
      "step": 415500
    },
    {
      "epoch": 5.474188412090587,
      "grad_norm": 2.1516873836517334,
      "learning_rate": 7.312062045487661e-05,
      "loss": 1.1303,
      "step": 416000
    },
    {
      "epoch": 5.480767965470504,
      "grad_norm": 2.3103177547454834,
      "learning_rate": 7.308750480176971e-05,
      "loss": 1.1157,
      "step": 416500
    },
    {
      "epoch": 5.48734751885042,
      "grad_norm": 2.3527286052703857,
      "learning_rate": 7.305438914866279e-05,
      "loss": 1.122,
      "step": 417000
    },
    {
      "epoch": 5.493927072230337,
      "grad_norm": 2.4503815174102783,
      "learning_rate": 7.302127349555589e-05,
      "loss": 1.1161,
      "step": 417500
    },
    {
      "epoch": 5.500506625610254,
      "grad_norm": 2.1717066764831543,
      "learning_rate": 7.298815784244896e-05,
      "loss": 1.1178,
      "step": 418000
    },
    {
      "epoch": 5.50708617899017,
      "grad_norm": 2.392498254776001,
      "learning_rate": 7.295504218934206e-05,
      "loss": 1.1188,
      "step": 418500
    },
    {
      "epoch": 5.513665732370087,
      "grad_norm": 2.6528284549713135,
      "learning_rate": 7.292192653623515e-05,
      "loss": 1.1249,
      "step": 419000
    },
    {
      "epoch": 5.520245285750003,
      "grad_norm": 2.3781838417053223,
      "learning_rate": 7.288887711443445e-05,
      "loss": 1.1131,
      "step": 419500
    },
    {
      "epoch": 5.52682483912992,
      "grad_norm": 2.505140781402588,
      "learning_rate": 7.285576146132754e-05,
      "loss": 1.1102,
      "step": 420000
    },
    {
      "epoch": 5.533404392509836,
      "grad_norm": 2.034761905670166,
      "learning_rate": 7.282264580822064e-05,
      "loss": 1.1193,
      "step": 420500
    },
    {
      "epoch": 5.539983945889753,
      "grad_norm": 2.2184972763061523,
      "learning_rate": 7.278953015511372e-05,
      "loss": 1.1126,
      "step": 421000
    },
    {
      "epoch": 5.54656349926967,
      "grad_norm": 2.4671216011047363,
      "learning_rate": 7.275648073331303e-05,
      "loss": 1.1122,
      "step": 421500
    },
    {
      "epoch": 5.553143052649586,
      "grad_norm": 2.3921215534210205,
      "learning_rate": 7.272336508020611e-05,
      "loss": 1.1156,
      "step": 422000
    },
    {
      "epoch": 5.559722606029503,
      "grad_norm": 2.705256223678589,
      "learning_rate": 7.26902494270992e-05,
      "loss": 1.1195,
      "step": 422500
    },
    {
      "epoch": 5.566302159409419,
      "grad_norm": 2.590806722640991,
      "learning_rate": 7.26571337739923e-05,
      "loss": 1.1137,
      "step": 423000
    },
    {
      "epoch": 5.572881712789336,
      "grad_norm": 2.29480242729187,
      "learning_rate": 7.26240843521916e-05,
      "loss": 1.1107,
      "step": 423500
    },
    {
      "epoch": 5.579461266169252,
      "grad_norm": 2.218973398208618,
      "learning_rate": 7.259096869908469e-05,
      "loss": 1.1219,
      "step": 424000
    },
    {
      "epoch": 5.586040819549169,
      "grad_norm": 2.2722394466400146,
      "learning_rate": 7.255785304597777e-05,
      "loss": 1.1188,
      "step": 424500
    },
    {
      "epoch": 5.592620372929085,
      "grad_norm": 2.426806926727295,
      "learning_rate": 7.252473739287087e-05,
      "loss": 1.1115,
      "step": 425000
    },
    {
      "epoch": 5.599199926309002,
      "grad_norm": 2.203651189804077,
      "learning_rate": 7.249168797107016e-05,
      "loss": 1.1193,
      "step": 425500
    },
    {
      "epoch": 5.605779479688919,
      "grad_norm": 2.574148654937744,
      "learning_rate": 7.245857231796326e-05,
      "loss": 1.1068,
      "step": 426000
    },
    {
      "epoch": 5.612359033068835,
      "grad_norm": 2.4552841186523438,
      "learning_rate": 7.242552289616255e-05,
      "loss": 1.1188,
      "step": 426500
    },
    {
      "epoch": 5.618938586448752,
      "grad_norm": 2.4836225509643555,
      "learning_rate": 7.239240724305565e-05,
      "loss": 1.1148,
      "step": 427000
    },
    {
      "epoch": 5.625518139828668,
      "grad_norm": 2.1956357955932617,
      "learning_rate": 7.235929158994874e-05,
      "loss": 1.1171,
      "step": 427500
    },
    {
      "epoch": 5.632097693208585,
      "grad_norm": 2.5563626289367676,
      "learning_rate": 7.232617593684184e-05,
      "loss": 1.1147,
      "step": 428000
    },
    {
      "epoch": 5.638677246588502,
      "grad_norm": 2.368164300918579,
      "learning_rate": 7.229306028373492e-05,
      "loss": 1.1161,
      "step": 428500
    },
    {
      "epoch": 5.6452567999684184,
      "grad_norm": 2.45847749710083,
      "learning_rate": 7.2259944630628e-05,
      "loss": 1.112,
      "step": 429000
    },
    {
      "epoch": 5.651836353348335,
      "grad_norm": 2.7871556282043457,
      "learning_rate": 7.222689520882731e-05,
      "loss": 1.1134,
      "step": 429500
    },
    {
      "epoch": 5.658415906728251,
      "grad_norm": 2.403024911880493,
      "learning_rate": 7.21937795557204e-05,
      "loss": 1.1151,
      "step": 430000
    },
    {
      "epoch": 5.664995460108168,
      "grad_norm": 2.3245487213134766,
      "learning_rate": 7.21606639026135e-05,
      "loss": 1.1181,
      "step": 430500
    },
    {
      "epoch": 5.671575013488084,
      "grad_norm": 2.169771671295166,
      "learning_rate": 7.212754824950658e-05,
      "loss": 1.1046,
      "step": 431000
    },
    {
      "epoch": 5.678154566868001,
      "grad_norm": 2.721892833709717,
      "learning_rate": 7.209443259639968e-05,
      "loss": 1.1133,
      "step": 431500
    },
    {
      "epoch": 5.684734120247917,
      "grad_norm": 2.1072070598602295,
      "learning_rate": 7.206138317459897e-05,
      "loss": 1.1208,
      "step": 432000
    },
    {
      "epoch": 5.6913136736278345,
      "grad_norm": 2.6502788066864014,
      "learning_rate": 7.202826752149207e-05,
      "loss": 1.1104,
      "step": 432500
    },
    {
      "epoch": 5.697893227007751,
      "grad_norm": 2.598518133163452,
      "learning_rate": 7.199515186838514e-05,
      "loss": 1.1094,
      "step": 433000
    },
    {
      "epoch": 5.704472780387667,
      "grad_norm": 2.5360429286956787,
      "learning_rate": 7.196203621527824e-05,
      "loss": 1.1149,
      "step": 433500
    },
    {
      "epoch": 5.711052333767584,
      "grad_norm": 2.527641534805298,
      "learning_rate": 7.192892056217133e-05,
      "loss": 1.1091,
      "step": 434000
    },
    {
      "epoch": 5.7176318871475,
      "grad_norm": 2.4058635234832764,
      "learning_rate": 7.189580490906442e-05,
      "loss": 1.1145,
      "step": 434500
    },
    {
      "epoch": 5.724211440527417,
      "grad_norm": 2.24342679977417,
      "learning_rate": 7.186268925595751e-05,
      "loss": 1.1105,
      "step": 435000
    },
    {
      "epoch": 5.730790993907334,
      "grad_norm": 2.1084721088409424,
      "learning_rate": 7.182963983415681e-05,
      "loss": 1.114,
      "step": 435500
    },
    {
      "epoch": 5.7373705472872505,
      "grad_norm": 2.511122465133667,
      "learning_rate": 7.179652418104991e-05,
      "loss": 1.1079,
      "step": 436000
    },
    {
      "epoch": 5.743950100667167,
      "grad_norm": 2.4917514324188232,
      "learning_rate": 7.176340852794298e-05,
      "loss": 1.1086,
      "step": 436500
    },
    {
      "epoch": 5.750529654047083,
      "grad_norm": 2.7593486309051514,
      "learning_rate": 7.173029287483608e-05,
      "loss": 1.1158,
      "step": 437000
    },
    {
      "epoch": 5.757109207427,
      "grad_norm": 2.1524524688720703,
      "learning_rate": 7.169717722172917e-05,
      "loss": 1.1097,
      "step": 437500
    },
    {
      "epoch": 5.763688760806916,
      "grad_norm": 2.399083137512207,
      "learning_rate": 7.166406156862227e-05,
      "loss": 1.1196,
      "step": 438000
    },
    {
      "epoch": 5.770268314186833,
      "grad_norm": 2.5943987369537354,
      "learning_rate": 7.163094591551534e-05,
      "loss": 1.1194,
      "step": 438500
    },
    {
      "epoch": 5.776847867566749,
      "grad_norm": 2.167064905166626,
      "learning_rate": 7.159789649371466e-05,
      "loss": 1.1171,
      "step": 439000
    },
    {
      "epoch": 5.7834274209466665,
      "grad_norm": 2.223388433456421,
      "learning_rate": 7.156478084060774e-05,
      "loss": 1.1156,
      "step": 439500
    },
    {
      "epoch": 5.790006974326583,
      "grad_norm": 2.312055826187134,
      "learning_rate": 7.153173141880705e-05,
      "loss": 1.1105,
      "step": 440000
    },
    {
      "epoch": 5.796586527706499,
      "grad_norm": 2.5245368480682373,
      "learning_rate": 7.149861576570013e-05,
      "loss": 1.113,
      "step": 440500
    },
    {
      "epoch": 5.803166081086416,
      "grad_norm": 2.3415069580078125,
      "learning_rate": 7.146550011259322e-05,
      "loss": 1.1119,
      "step": 441000
    },
    {
      "epoch": 5.809745634466332,
      "grad_norm": 2.433417797088623,
      "learning_rate": 7.143238445948632e-05,
      "loss": 1.1107,
      "step": 441500
    },
    {
      "epoch": 5.816325187846249,
      "grad_norm": 2.27118182182312,
      "learning_rate": 7.13992688063794e-05,
      "loss": 1.1178,
      "step": 442000
    },
    {
      "epoch": 5.822904741226165,
      "grad_norm": 2.2616677284240723,
      "learning_rate": 7.13661531532725e-05,
      "loss": 1.114,
      "step": 442500
    },
    {
      "epoch": 5.8294842946060825,
      "grad_norm": 2.402179718017578,
      "learning_rate": 7.133303750016557e-05,
      "loss": 1.106,
      "step": 443000
    },
    {
      "epoch": 5.836063847985999,
      "grad_norm": 2.2446954250335693,
      "learning_rate": 7.129992184705867e-05,
      "loss": 1.1137,
      "step": 443500
    },
    {
      "epoch": 5.842643401365915,
      "grad_norm": 2.477332830429077,
      "learning_rate": 7.126680619395176e-05,
      "loss": 1.1042,
      "step": 444000
    },
    {
      "epoch": 5.849222954745832,
      "grad_norm": 2.406822443008423,
      "learning_rate": 7.123369054084486e-05,
      "loss": 1.1013,
      "step": 444500
    },
    {
      "epoch": 5.855802508125748,
      "grad_norm": 2.5992846488952637,
      "learning_rate": 7.120064111904415e-05,
      "loss": 1.1073,
      "step": 445000
    },
    {
      "epoch": 5.862382061505665,
      "grad_norm": 2.3426406383514404,
      "learning_rate": 7.116752546593725e-05,
      "loss": 1.1048,
      "step": 445500
    },
    {
      "epoch": 5.868961614885581,
      "grad_norm": 2.4713199138641357,
      "learning_rate": 7.113440981283033e-05,
      "loss": 1.1106,
      "step": 446000
    },
    {
      "epoch": 5.875541168265498,
      "grad_norm": 2.10608172416687,
      "learning_rate": 7.110129415972342e-05,
      "loss": 1.1066,
      "step": 446500
    },
    {
      "epoch": 5.882120721645415,
      "grad_norm": 2.5569839477539062,
      "learning_rate": 7.106817850661652e-05,
      "loss": 1.103,
      "step": 447000
    },
    {
      "epoch": 5.888700275025331,
      "grad_norm": 2.543036937713623,
      "learning_rate": 7.10350628535096e-05,
      "loss": 1.1144,
      "step": 447500
    },
    {
      "epoch": 5.895279828405248,
      "grad_norm": 2.255427122116089,
      "learning_rate": 7.100194720040269e-05,
      "loss": 1.1025,
      "step": 448000
    },
    {
      "epoch": 5.901859381785164,
      "grad_norm": 2.3939743041992188,
      "learning_rate": 7.096883154729577e-05,
      "loss": 1.1099,
      "step": 448500
    },
    {
      "epoch": 5.908438935165081,
      "grad_norm": 2.405148983001709,
      "learning_rate": 7.093571589418887e-05,
      "loss": 1.1107,
      "step": 449000
    },
    {
      "epoch": 5.915018488544997,
      "grad_norm": 2.2783570289611816,
      "learning_rate": 7.090260024108196e-05,
      "loss": 1.1101,
      "step": 449500
    },
    {
      "epoch": 5.921598041924915,
      "grad_norm": 2.382747173309326,
      "learning_rate": 7.086948458797504e-05,
      "loss": 1.1098,
      "step": 450000
    },
    {
      "epoch": 5.928177595304831,
      "grad_norm": 2.476557731628418,
      "learning_rate": 7.083636893486814e-05,
      "loss": 1.1042,
      "step": 450500
    },
    {
      "epoch": 5.9347571486847475,
      "grad_norm": 2.3767993450164795,
      "learning_rate": 7.080331951306745e-05,
      "loss": 1.1035,
      "step": 451000
    },
    {
      "epoch": 5.941336702064664,
      "grad_norm": 2.47698712348938,
      "learning_rate": 7.077027009126675e-05,
      "loss": 1.1192,
      "step": 451500
    },
    {
      "epoch": 5.94791625544458,
      "grad_norm": 2.4184205532073975,
      "learning_rate": 7.073722066946604e-05,
      "loss": 1.1089,
      "step": 452000
    },
    {
      "epoch": 5.954495808824497,
      "grad_norm": 2.524925708770752,
      "learning_rate": 7.070410501635914e-05,
      "loss": 1.1041,
      "step": 452500
    },
    {
      "epoch": 5.961075362204413,
      "grad_norm": 2.8864786624908447,
      "learning_rate": 7.067098936325223e-05,
      "loss": 1.1091,
      "step": 453000
    },
    {
      "epoch": 5.96765491558433,
      "grad_norm": 2.37845516204834,
      "learning_rate": 7.063787371014532e-05,
      "loss": 1.1011,
      "step": 453500
    },
    {
      "epoch": 5.974234468964247,
      "grad_norm": 2.3558502197265625,
      "learning_rate": 7.06047580570384e-05,
      "loss": 1.112,
      "step": 454000
    },
    {
      "epoch": 5.9808140223441635,
      "grad_norm": 2.272486925125122,
      "learning_rate": 7.05716424039315e-05,
      "loss": 1.098,
      "step": 454500
    },
    {
      "epoch": 5.98739357572408,
      "grad_norm": 2.39921498298645,
      "learning_rate": 7.053852675082458e-05,
      "loss": 1.1164,
      "step": 455000
    },
    {
      "epoch": 5.993973129103996,
      "grad_norm": 2.374601125717163,
      "learning_rate": 7.050541109771768e-05,
      "loss": 1.103,
      "step": 455500
    },
    {
      "epoch": 6.0,
      "eval_loss": 1.0509576797485352,
      "eval_runtime": 49.7994,
      "eval_samples_per_second": 2008.058,
      "eval_steps_per_second": 15.703,
      "step": 455958
    },
    {
      "epoch": 6.000552682483913,
      "grad_norm": 2.572255849838257,
      "learning_rate": 7.047229544461076e-05,
      "loss": 1.1041,
      "step": 456000
    },
    {
      "epoch": 6.007132235863829,
      "grad_norm": 2.3278539180755615,
      "learning_rate": 7.043917979150385e-05,
      "loss": 1.0995,
      "step": 456500
    },
    {
      "epoch": 6.013711789243746,
      "grad_norm": 2.4753382205963135,
      "learning_rate": 7.040606413839695e-05,
      "loss": 1.0925,
      "step": 457000
    },
    {
      "epoch": 6.020291342623663,
      "grad_norm": 2.270946979522705,
      "learning_rate": 7.037301471659624e-05,
      "loss": 1.0924,
      "step": 457500
    },
    {
      "epoch": 6.0268708960035795,
      "grad_norm": 2.3378427028656006,
      "learning_rate": 7.033989906348934e-05,
      "loss": 1.0927,
      "step": 458000
    },
    {
      "epoch": 6.033450449383496,
      "grad_norm": 2.702892303466797,
      "learning_rate": 7.030678341038242e-05,
      "loss": 1.1007,
      "step": 458500
    },
    {
      "epoch": 6.040030002763412,
      "grad_norm": 2.7419021129608154,
      "learning_rate": 7.027366775727551e-05,
      "loss": 1.0993,
      "step": 459000
    },
    {
      "epoch": 6.046609556143329,
      "grad_norm": 2.535101890563965,
      "learning_rate": 7.02405521041686e-05,
      "loss": 1.0927,
      "step": 459500
    },
    {
      "epoch": 6.053189109523245,
      "grad_norm": 2.527043581008911,
      "learning_rate": 7.020750268236791e-05,
      "loss": 1.0962,
      "step": 460000
    },
    {
      "epoch": 6.059768662903162,
      "grad_norm": 2.539639472961426,
      "learning_rate": 7.017438702926098e-05,
      "loss": 1.1021,
      "step": 460500
    },
    {
      "epoch": 6.066348216283079,
      "grad_norm": 2.6269173622131348,
      "learning_rate": 7.014127137615408e-05,
      "loss": 1.088,
      "step": 461000
    },
    {
      "epoch": 6.0729277696629955,
      "grad_norm": 2.3837976455688477,
      "learning_rate": 7.010815572304717e-05,
      "loss": 1.0972,
      "step": 461500
    },
    {
      "epoch": 6.079507323042912,
      "grad_norm": 2.3524935245513916,
      "learning_rate": 7.007504006994027e-05,
      "loss": 1.0988,
      "step": 462000
    },
    {
      "epoch": 6.086086876422828,
      "grad_norm": 2.2759182453155518,
      "learning_rate": 7.004192441683335e-05,
      "loss": 1.1074,
      "step": 462500
    },
    {
      "epoch": 6.092666429802745,
      "grad_norm": 2.9608991146087646,
      "learning_rate": 7.000887499503266e-05,
      "loss": 1.0987,
      "step": 463000
    },
    {
      "epoch": 6.099245983182661,
      "grad_norm": 2.2377052307128906,
      "learning_rate": 6.997589180453817e-05,
      "loss": 1.0935,
      "step": 463500
    },
    {
      "epoch": 6.105825536562578,
      "grad_norm": 2.3489692211151123,
      "learning_rate": 6.994277615143127e-05,
      "loss": 1.0905,
      "step": 464000
    },
    {
      "epoch": 6.112405089942495,
      "grad_norm": 2.583052396774292,
      "learning_rate": 6.990966049832435e-05,
      "loss": 1.1,
      "step": 464500
    },
    {
      "epoch": 6.1189846433224115,
      "grad_norm": 2.3700785636901855,
      "learning_rate": 6.987654484521744e-05,
      "loss": 1.0889,
      "step": 465000
    },
    {
      "epoch": 6.125564196702328,
      "grad_norm": 2.3431897163391113,
      "learning_rate": 6.984349542341674e-05,
      "loss": 1.0974,
      "step": 465500
    },
    {
      "epoch": 6.132143750082244,
      "grad_norm": 2.5465447902679443,
      "learning_rate": 6.981037977030983e-05,
      "loss": 1.0984,
      "step": 466000
    },
    {
      "epoch": 6.138723303462161,
      "grad_norm": 2.4753026962280273,
      "learning_rate": 6.977726411720293e-05,
      "loss": 1.0892,
      "step": 466500
    },
    {
      "epoch": 6.145302856842077,
      "grad_norm": 2.3025810718536377,
      "learning_rate": 6.974414846409601e-05,
      "loss": 1.1032,
      "step": 467000
    },
    {
      "epoch": 6.151882410221994,
      "grad_norm": 2.5734164714813232,
      "learning_rate": 6.97110328109891e-05,
      "loss": 1.0872,
      "step": 467500
    },
    {
      "epoch": 6.158461963601911,
      "grad_norm": 2.3348467350006104,
      "learning_rate": 6.967791715788218e-05,
      "loss": 1.0933,
      "step": 468000
    },
    {
      "epoch": 6.165041516981828,
      "grad_norm": 2.4170637130737305,
      "learning_rate": 6.964480150477528e-05,
      "loss": 1.0919,
      "step": 468500
    },
    {
      "epoch": 6.171621070361744,
      "grad_norm": 2.467864751815796,
      "learning_rate": 6.961168585166837e-05,
      "loss": 1.1038,
      "step": 469000
    },
    {
      "epoch": 6.1782006237416605,
      "grad_norm": 2.7378945350646973,
      "learning_rate": 6.957857019856145e-05,
      "loss": 1.0996,
      "step": 469500
    },
    {
      "epoch": 6.184780177121577,
      "grad_norm": 2.022998094558716,
      "learning_rate": 6.954552077676076e-05,
      "loss": 1.0822,
      "step": 470000
    },
    {
      "epoch": 6.191359730501493,
      "grad_norm": 2.086763620376587,
      "learning_rate": 6.951240512365386e-05,
      "loss": 1.0933,
      "step": 470500
    },
    {
      "epoch": 6.19793928388141,
      "grad_norm": 2.5449602603912354,
      "learning_rate": 6.947928947054694e-05,
      "loss": 1.0946,
      "step": 471000
    },
    {
      "epoch": 6.204518837261327,
      "grad_norm": 2.2891225814819336,
      "learning_rate": 6.944617381744003e-05,
      "loss": 1.0869,
      "step": 471500
    },
    {
      "epoch": 6.211098390641244,
      "grad_norm": 2.912557601928711,
      "learning_rate": 6.941312439563935e-05,
      "loss": 1.0924,
      "step": 472000
    },
    {
      "epoch": 6.21767794402116,
      "grad_norm": 2.584932804107666,
      "learning_rate": 6.938000874253242e-05,
      "loss": 1.0915,
      "step": 472500
    },
    {
      "epoch": 6.2242574974010765,
      "grad_norm": 2.5754647254943848,
      "learning_rate": 6.934689308942552e-05,
      "loss": 1.09,
      "step": 473000
    },
    {
      "epoch": 6.230837050780993,
      "grad_norm": 2.4054696559906006,
      "learning_rate": 6.93137774363186e-05,
      "loss": 1.0914,
      "step": 473500
    },
    {
      "epoch": 6.237416604160909,
      "grad_norm": 2.5959739685058594,
      "learning_rate": 6.928066178321169e-05,
      "loss": 1.0884,
      "step": 474000
    },
    {
      "epoch": 6.243996157540826,
      "grad_norm": 2.1544737815856934,
      "learning_rate": 6.924754613010479e-05,
      "loss": 1.0795,
      "step": 474500
    },
    {
      "epoch": 6.250575710920742,
      "grad_norm": 2.4174115657806396,
      "learning_rate": 6.921443047699787e-05,
      "loss": 1.0952,
      "step": 475000
    },
    {
      "epoch": 6.25715526430066,
      "grad_norm": 2.537313938140869,
      "learning_rate": 6.918131482389097e-05,
      "loss": 1.0919,
      "step": 475500
    },
    {
      "epoch": 6.263734817680576,
      "grad_norm": 2.2719452381134033,
      "learning_rate": 6.914819917078404e-05,
      "loss": 1.0896,
      "step": 476000
    },
    {
      "epoch": 6.2703143710604925,
      "grad_norm": 2.456516742706299,
      "learning_rate": 6.911508351767714e-05,
      "loss": 1.0908,
      "step": 476500
    },
    {
      "epoch": 6.276893924440409,
      "grad_norm": 2.4016366004943848,
      "learning_rate": 6.908203409587645e-05,
      "loss": 1.0941,
      "step": 477000
    },
    {
      "epoch": 6.283473477820325,
      "grad_norm": 2.3133838176727295,
      "learning_rate": 6.904891844276953e-05,
      "loss": 1.1063,
      "step": 477500
    },
    {
      "epoch": 6.290053031200242,
      "grad_norm": 2.550705909729004,
      "learning_rate": 6.901580278966262e-05,
      "loss": 1.0979,
      "step": 478000
    },
    {
      "epoch": 6.296632584580158,
      "grad_norm": 2.279362201690674,
      "learning_rate": 6.898268713655571e-05,
      "loss": 1.0843,
      "step": 478500
    },
    {
      "epoch": 6.303212137960076,
      "grad_norm": 2.297011137008667,
      "learning_rate": 6.89495714834488e-05,
      "loss": 1.0831,
      "step": 479000
    },
    {
      "epoch": 6.309791691339992,
      "grad_norm": 2.3375699520111084,
      "learning_rate": 6.891645583034189e-05,
      "loss": 1.0867,
      "step": 479500
    },
    {
      "epoch": 6.3163712447199085,
      "grad_norm": 2.640502452850342,
      "learning_rate": 6.888334017723498e-05,
      "loss": 1.0888,
      "step": 480000
    },
    {
      "epoch": 6.322950798099825,
      "grad_norm": 2.512737274169922,
      "learning_rate": 6.885022452412807e-05,
      "loss": 1.082,
      "step": 480500
    },
    {
      "epoch": 6.329530351479741,
      "grad_norm": 2.5664749145507812,
      "learning_rate": 6.881710887102117e-05,
      "loss": 1.0869,
      "step": 481000
    },
    {
      "epoch": 6.336109904859658,
      "grad_norm": 2.2605068683624268,
      "learning_rate": 6.878405944922046e-05,
      "loss": 1.0862,
      "step": 481500
    },
    {
      "epoch": 6.342689458239574,
      "grad_norm": 2.398902177810669,
      "learning_rate": 6.875094379611356e-05,
      "loss": 1.1011,
      "step": 482000
    },
    {
      "epoch": 6.349269011619492,
      "grad_norm": 2.3286592960357666,
      "learning_rate": 6.871789437431285e-05,
      "loss": 1.0894,
      "step": 482500
    },
    {
      "epoch": 6.355848564999408,
      "grad_norm": 2.495807647705078,
      "learning_rate": 6.868477872120595e-05,
      "loss": 1.0955,
      "step": 483000
    },
    {
      "epoch": 6.3624281183793245,
      "grad_norm": 2.2485878467559814,
      "learning_rate": 6.865166306809903e-05,
      "loss": 1.0951,
      "step": 483500
    },
    {
      "epoch": 6.369007671759241,
      "grad_norm": 2.498553514480591,
      "learning_rate": 6.861854741499212e-05,
      "loss": 1.0905,
      "step": 484000
    },
    {
      "epoch": 6.375587225139157,
      "grad_norm": 2.320434093475342,
      "learning_rate": 6.858549799319142e-05,
      "loss": 1.091,
      "step": 484500
    },
    {
      "epoch": 6.382166778519074,
      "grad_norm": 2.1577963829040527,
      "learning_rate": 6.855238234008451e-05,
      "loss": 1.0953,
      "step": 485000
    },
    {
      "epoch": 6.38874633189899,
      "grad_norm": 2.641777992248535,
      "learning_rate": 6.85192666869776e-05,
      "loss": 1.0809,
      "step": 485500
    },
    {
      "epoch": 6.395325885278908,
      "grad_norm": 2.220217227935791,
      "learning_rate": 6.84861510338707e-05,
      "loss": 1.0853,
      "step": 486000
    },
    {
      "epoch": 6.401905438658824,
      "grad_norm": 2.243143320083618,
      "learning_rate": 6.845303538076379e-05,
      "loss": 1.0959,
      "step": 486500
    },
    {
      "epoch": 6.408484992038741,
      "grad_norm": 2.2155230045318604,
      "learning_rate": 6.841991972765686e-05,
      "loss": 1.0889,
      "step": 487000
    },
    {
      "epoch": 6.415064545418657,
      "grad_norm": 2.6029584407806396,
      "learning_rate": 6.838687030585618e-05,
      "loss": 1.078,
      "step": 487500
    },
    {
      "epoch": 6.4216440987985735,
      "grad_norm": 2.5924136638641357,
      "learning_rate": 6.835375465274927e-05,
      "loss": 1.0911,
      "step": 488000
    },
    {
      "epoch": 6.42822365217849,
      "grad_norm": 2.380645751953125,
      "learning_rate": 6.832063899964235e-05,
      "loss": 1.0813,
      "step": 488500
    },
    {
      "epoch": 6.434803205558406,
      "grad_norm": 2.3291826248168945,
      "learning_rate": 6.828758957784166e-05,
      "loss": 1.0928,
      "step": 489000
    },
    {
      "epoch": 6.441382758938324,
      "grad_norm": 2.322659730911255,
      "learning_rate": 6.825454015604096e-05,
      "loss": 1.0922,
      "step": 489500
    },
    {
      "epoch": 6.44796231231824,
      "grad_norm": 2.383101224899292,
      "learning_rate": 6.822142450293405e-05,
      "loss": 1.087,
      "step": 490000
    },
    {
      "epoch": 6.454541865698157,
      "grad_norm": 2.412864923477173,
      "learning_rate": 6.818830884982715e-05,
      "loss": 1.0932,
      "step": 490500
    },
    {
      "epoch": 6.461121419078073,
      "grad_norm": 2.459125518798828,
      "learning_rate": 6.815519319672022e-05,
      "loss": 1.0939,
      "step": 491000
    },
    {
      "epoch": 6.4677009724579895,
      "grad_norm": 2.561758279800415,
      "learning_rate": 6.812207754361332e-05,
      "loss": 1.08,
      "step": 491500
    },
    {
      "epoch": 6.474280525837906,
      "grad_norm": 2.219479560852051,
      "learning_rate": 6.80889618905064e-05,
      "loss": 1.0809,
      "step": 492000
    },
    {
      "epoch": 6.480860079217822,
      "grad_norm": 2.5485053062438965,
      "learning_rate": 6.80558462373995e-05,
      "loss": 1.0933,
      "step": 492500
    },
    {
      "epoch": 6.48743963259774,
      "grad_norm": 2.419666051864624,
      "learning_rate": 6.80227968155988e-05,
      "loss": 1.0836,
      "step": 493000
    },
    {
      "epoch": 6.494019185977656,
      "grad_norm": 2.5085248947143555,
      "learning_rate": 6.798968116249189e-05,
      "loss": 1.0917,
      "step": 493500
    },
    {
      "epoch": 6.500598739357573,
      "grad_norm": 2.5368049144744873,
      "learning_rate": 6.795656550938498e-05,
      "loss": 1.0869,
      "step": 494000
    },
    {
      "epoch": 6.507178292737489,
      "grad_norm": 2.4986824989318848,
      "learning_rate": 6.792344985627806e-05,
      "loss": 1.078,
      "step": 494500
    },
    {
      "epoch": 6.5137578461174055,
      "grad_norm": 2.5896997451782227,
      "learning_rate": 6.789033420317116e-05,
      "loss": 1.0806,
      "step": 495000
    },
    {
      "epoch": 6.520337399497322,
      "grad_norm": 2.4450151920318604,
      "learning_rate": 6.785721855006425e-05,
      "loss": 1.088,
      "step": 495500
    },
    {
      "epoch": 6.526916952877238,
      "grad_norm": 2.269605875015259,
      "learning_rate": 6.782410289695735e-05,
      "loss": 1.0878,
      "step": 496000
    },
    {
      "epoch": 6.533496506257155,
      "grad_norm": 2.2018439769744873,
      "learning_rate": 6.779098724385042e-05,
      "loss": 1.079,
      "step": 496500
    },
    {
      "epoch": 6.540076059637072,
      "grad_norm": 2.448685646057129,
      "learning_rate": 6.775787159074352e-05,
      "loss": 1.0916,
      "step": 497000
    },
    {
      "epoch": 6.546655613016989,
      "grad_norm": 2.5800225734710693,
      "learning_rate": 6.77247559376366e-05,
      "loss": 1.0718,
      "step": 497500
    },
    {
      "epoch": 6.553235166396905,
      "grad_norm": 2.5529823303222656,
      "learning_rate": 6.769164028452969e-05,
      "loss": 1.0835,
      "step": 498000
    },
    {
      "epoch": 6.5598147197768215,
      "grad_norm": 2.427114963531494,
      "learning_rate": 6.765852463142279e-05,
      "loss": 1.0937,
      "step": 498500
    },
    {
      "epoch": 6.566394273156738,
      "grad_norm": 2.63297963142395,
      "learning_rate": 6.762547520962209e-05,
      "loss": 1.0897,
      "step": 499000
    },
    {
      "epoch": 6.572973826536654,
      "grad_norm": 2.293109655380249,
      "learning_rate": 6.75924257878214e-05,
      "loss": 1.0805,
      "step": 499500
    },
    {
      "epoch": 6.579553379916572,
      "grad_norm": 2.4441285133361816,
      "learning_rate": 6.755931013471448e-05,
      "loss": 1.081,
      "step": 500000
    },
    {
      "epoch": 6.586132933296488,
      "grad_norm": 2.4445810317993164,
      "learning_rate": 6.752626071291379e-05,
      "loss": 1.081,
      "step": 500500
    },
    {
      "epoch": 6.592712486676405,
      "grad_norm": 2.708824396133423,
      "learning_rate": 6.749321129111309e-05,
      "loss": 1.085,
      "step": 501000
    },
    {
      "epoch": 6.599292040056321,
      "grad_norm": 2.2858521938323975,
      "learning_rate": 6.746009563800618e-05,
      "loss": 1.0781,
      "step": 501500
    },
    {
      "epoch": 6.6058715934362375,
      "grad_norm": 2.5090255737304688,
      "learning_rate": 6.742697998489926e-05,
      "loss": 1.0757,
      "step": 502000
    },
    {
      "epoch": 6.612451146816154,
      "grad_norm": 2.5447983741760254,
      "learning_rate": 6.739386433179236e-05,
      "loss": 1.0898,
      "step": 502500
    },
    {
      "epoch": 6.61903070019607,
      "grad_norm": 2.6527390480041504,
      "learning_rate": 6.736074867868545e-05,
      "loss": 1.0836,
      "step": 503000
    },
    {
      "epoch": 6.625610253575987,
      "grad_norm": 3.2481555938720703,
      "learning_rate": 6.732763302557853e-05,
      "loss": 1.079,
      "step": 503500
    },
    {
      "epoch": 6.632189806955904,
      "grad_norm": 2.441026210784912,
      "learning_rate": 6.729451737247162e-05,
      "loss": 1.0862,
      "step": 504000
    },
    {
      "epoch": 6.638769360335821,
      "grad_norm": 2.546010971069336,
      "learning_rate": 6.726140171936471e-05,
      "loss": 1.0804,
      "step": 504500
    },
    {
      "epoch": 6.645348913715737,
      "grad_norm": 2.512603521347046,
      "learning_rate": 6.72282860662578e-05,
      "loss": 1.0852,
      "step": 505000
    },
    {
      "epoch": 6.6519284670956536,
      "grad_norm": 2.533485174179077,
      "learning_rate": 6.719517041315089e-05,
      "loss": 1.0774,
      "step": 505500
    },
    {
      "epoch": 6.65850802047557,
      "grad_norm": 2.361898899078369,
      "learning_rate": 6.716205476004398e-05,
      "loss": 1.0848,
      "step": 506000
    },
    {
      "epoch": 6.6650875738554864,
      "grad_norm": 2.2576069831848145,
      "learning_rate": 6.712893910693707e-05,
      "loss": 1.0769,
      "step": 506500
    },
    {
      "epoch": 6.671667127235403,
      "grad_norm": 2.3776776790618896,
      "learning_rate": 6.709582345383017e-05,
      "loss": 1.0722,
      "step": 507000
    },
    {
      "epoch": 6.67824668061532,
      "grad_norm": 2.314288854598999,
      "learning_rate": 6.706270780072324e-05,
      "loss": 1.081,
      "step": 507500
    },
    {
      "epoch": 6.684826233995237,
      "grad_norm": 2.239010810852051,
      "learning_rate": 6.702959214761634e-05,
      "loss": 1.0848,
      "step": 508000
    },
    {
      "epoch": 6.691405787375153,
      "grad_norm": 2.392437219619751,
      "learning_rate": 6.699647649450942e-05,
      "loss": 1.0809,
      "step": 508500
    },
    {
      "epoch": 6.69798534075507,
      "grad_norm": 3.0327091217041016,
      "learning_rate": 6.696336084140252e-05,
      "loss": 1.0771,
      "step": 509000
    },
    {
      "epoch": 6.704564894134986,
      "grad_norm": 2.490891933441162,
      "learning_rate": 6.693024518829561e-05,
      "loss": 1.0773,
      "step": 509500
    },
    {
      "epoch": 6.7111444475149025,
      "grad_norm": 2.372410297393799,
      "learning_rate": 6.689719576649491e-05,
      "loss": 1.0898,
      "step": 510000
    },
    {
      "epoch": 6.717724000894819,
      "grad_norm": 2.726006031036377,
      "learning_rate": 6.6864080113388e-05,
      "loss": 1.0736,
      "step": 510500
    },
    {
      "epoch": 6.724303554274735,
      "grad_norm": 2.265376567840576,
      "learning_rate": 6.68310306915873e-05,
      "loss": 1.0849,
      "step": 511000
    },
    {
      "epoch": 6.730883107654653,
      "grad_norm": 2.567110538482666,
      "learning_rate": 6.67979150384804e-05,
      "loss": 1.0718,
      "step": 511500
    },
    {
      "epoch": 6.737462661034569,
      "grad_norm": 2.4495058059692383,
      "learning_rate": 6.676479938537347e-05,
      "loss": 1.0824,
      "step": 512000
    },
    {
      "epoch": 6.744042214414486,
      "grad_norm": 2.307370662689209,
      "learning_rate": 6.673168373226657e-05,
      "loss": 1.0745,
      "step": 512500
    },
    {
      "epoch": 6.750621767794402,
      "grad_norm": 2.3879647254943848,
      "learning_rate": 6.669856807915966e-05,
      "loss": 1.0807,
      "step": 513000
    },
    {
      "epoch": 6.7572013211743185,
      "grad_norm": 2.15539813041687,
      "learning_rate": 6.666545242605276e-05,
      "loss": 1.0796,
      "step": 513500
    },
    {
      "epoch": 6.763780874554235,
      "grad_norm": 2.483964204788208,
      "learning_rate": 6.663233677294584e-05,
      "loss": 1.0791,
      "step": 514000
    },
    {
      "epoch": 6.770360427934152,
      "grad_norm": 2.197779893875122,
      "learning_rate": 6.659922111983893e-05,
      "loss": 1.0901,
      "step": 514500
    },
    {
      "epoch": 6.776939981314069,
      "grad_norm": 2.4044382572174072,
      "learning_rate": 6.656610546673203e-05,
      "loss": 1.0773,
      "step": 515000
    },
    {
      "epoch": 6.783519534693985,
      "grad_norm": 2.4895262718200684,
      "learning_rate": 6.653298981362511e-05,
      "loss": 1.0752,
      "step": 515500
    },
    {
      "epoch": 6.790099088073902,
      "grad_norm": 2.080456495285034,
      "learning_rate": 6.649994039182442e-05,
      "loss": 1.0827,
      "step": 516000
    },
    {
      "epoch": 6.796678641453818,
      "grad_norm": 2.358907699584961,
      "learning_rate": 6.64668247387175e-05,
      "loss": 1.0809,
      "step": 516500
    },
    {
      "epoch": 6.8032581948337345,
      "grad_norm": 2.445016860961914,
      "learning_rate": 6.643370908561059e-05,
      "loss": 1.0779,
      "step": 517000
    },
    {
      "epoch": 6.809837748213651,
      "grad_norm": 2.210524559020996,
      "learning_rate": 6.640059343250367e-05,
      "loss": 1.0903,
      "step": 517500
    },
    {
      "epoch": 6.816417301593567,
      "grad_norm": 2.1081528663635254,
      "learning_rate": 6.636747777939677e-05,
      "loss": 1.0878,
      "step": 518000
    },
    {
      "epoch": 6.822996854973485,
      "grad_norm": 2.4231786727905273,
      "learning_rate": 6.633436212628986e-05,
      "loss": 1.075,
      "step": 518500
    },
    {
      "epoch": 6.829576408353401,
      "grad_norm": 2.451122999191284,
      "learning_rate": 6.630124647318294e-05,
      "loss": 1.0877,
      "step": 519000
    },
    {
      "epoch": 6.836155961733318,
      "grad_norm": 2.518890619277954,
      "learning_rate": 6.626813082007604e-05,
      "loss": 1.0681,
      "step": 519500
    },
    {
      "epoch": 6.842735515113234,
      "grad_norm": 2.207484483718872,
      "learning_rate": 6.623501516696913e-05,
      "loss": 1.0715,
      "step": 520000
    },
    {
      "epoch": 6.8493150684931505,
      "grad_norm": 2.540482521057129,
      "learning_rate": 6.620189951386223e-05,
      "loss": 1.0741,
      "step": 520500
    },
    {
      "epoch": 6.855894621873067,
      "grad_norm": 2.460353136062622,
      "learning_rate": 6.616885009206152e-05,
      "loss": 1.0757,
      "step": 521000
    },
    {
      "epoch": 6.862474175252984,
      "grad_norm": 2.2806031703948975,
      "learning_rate": 6.613573443895462e-05,
      "loss": 1.0742,
      "step": 521500
    },
    {
      "epoch": 6.869053728632901,
      "grad_norm": 2.173248767852783,
      "learning_rate": 6.61026187858477e-05,
      "loss": 1.0717,
      "step": 522000
    },
    {
      "epoch": 6.875633282012817,
      "grad_norm": 2.379152297973633,
      "learning_rate": 6.606950313274079e-05,
      "loss": 1.0801,
      "step": 522500
    },
    {
      "epoch": 6.882212835392734,
      "grad_norm": 2.4311373233795166,
      "learning_rate": 6.603638747963387e-05,
      "loss": 1.0719,
      "step": 523000
    },
    {
      "epoch": 6.88879238877265,
      "grad_norm": 2.325765371322632,
      "learning_rate": 6.600333805783318e-05,
      "loss": 1.0785,
      "step": 523500
    },
    {
      "epoch": 6.8953719421525665,
      "grad_norm": 2.44193959236145,
      "learning_rate": 6.597022240472626e-05,
      "loss": 1.0728,
      "step": 524000
    },
    {
      "epoch": 6.901951495532483,
      "grad_norm": 2.545572519302368,
      "learning_rate": 6.593717298292558e-05,
      "loss": 1.0769,
      "step": 524500
    },
    {
      "epoch": 6.908531048912399,
      "grad_norm": 2.4871737957000732,
      "learning_rate": 6.590405732981865e-05,
      "loss": 1.0806,
      "step": 525000
    },
    {
      "epoch": 6.915110602292317,
      "grad_norm": 2.382641077041626,
      "learning_rate": 6.587094167671175e-05,
      "loss": 1.082,
      "step": 525500
    },
    {
      "epoch": 6.921690155672233,
      "grad_norm": 2.47381854057312,
      "learning_rate": 6.583789225491106e-05,
      "loss": 1.0745,
      "step": 526000
    },
    {
      "epoch": 6.92826970905215,
      "grad_norm": 2.6030008792877197,
      "learning_rate": 6.580477660180414e-05,
      "loss": 1.0784,
      "step": 526500
    },
    {
      "epoch": 6.934849262432066,
      "grad_norm": 2.0959484577178955,
      "learning_rate": 6.577166094869724e-05,
      "loss": 1.0674,
      "step": 527000
    },
    {
      "epoch": 6.941428815811983,
      "grad_norm": 3.2714591026306152,
      "learning_rate": 6.573854529559032e-05,
      "loss": 1.0766,
      "step": 527500
    },
    {
      "epoch": 6.948008369191899,
      "grad_norm": 2.4616634845733643,
      "learning_rate": 6.570542964248341e-05,
      "loss": 1.0721,
      "step": 528000
    },
    {
      "epoch": 6.9545879225718155,
      "grad_norm": 2.5239531993865967,
      "learning_rate": 6.56723139893765e-05,
      "loss": 1.0704,
      "step": 528500
    },
    {
      "epoch": 6.961167475951733,
      "grad_norm": 2.2316229343414307,
      "learning_rate": 6.56391983362696e-05,
      "loss": 1.0654,
      "step": 529000
    },
    {
      "epoch": 6.967747029331649,
      "grad_norm": 2.4964845180511475,
      "learning_rate": 6.560608268316268e-05,
      "loss": 1.0823,
      "step": 529500
    },
    {
      "epoch": 6.974326582711566,
      "grad_norm": 2.7256014347076416,
      "learning_rate": 6.557303326136198e-05,
      "loss": 1.0859,
      "step": 530000
    },
    {
      "epoch": 6.980906136091482,
      "grad_norm": 2.5326507091522217,
      "learning_rate": 6.553991760825507e-05,
      "loss": 1.0723,
      "step": 530500
    },
    {
      "epoch": 6.987485689471399,
      "grad_norm": 2.3340845108032227,
      "learning_rate": 6.550680195514817e-05,
      "loss": 1.0739,
      "step": 531000
    },
    {
      "epoch": 6.994065242851315,
      "grad_norm": 2.7946436405181885,
      "learning_rate": 6.547368630204125e-05,
      "loss": 1.0725,
      "step": 531500
    },
    {
      "epoch": 7.0,
      "eval_loss": 1.0258064270019531,
      "eval_runtime": 49.5465,
      "eval_samples_per_second": 2018.306,
      "eval_steps_per_second": 15.783,
      "step": 531951
    },
    {
      "epoch": 7.0006447962312315,
      "grad_norm": 2.310835361480713,
      "learning_rate": 6.544057064893434e-05,
      "loss": 1.0824,
      "step": 532000
    },
    {
      "epoch": 7.007224349611149,
      "grad_norm": 2.5041568279266357,
      "learning_rate": 6.540745499582744e-05,
      "loss": 1.0714,
      "step": 532500
    },
    {
      "epoch": 7.013803902991065,
      "grad_norm": 2.4994571208953857,
      "learning_rate": 6.537433934272052e-05,
      "loss": 1.0675,
      "step": 533000
    },
    {
      "epoch": 7.020383456370982,
      "grad_norm": 2.7220044136047363,
      "learning_rate": 6.534122368961361e-05,
      "loss": 1.0678,
      "step": 533500
    },
    {
      "epoch": 7.026963009750898,
      "grad_norm": 2.3451271057128906,
      "learning_rate": 6.53081080365067e-05,
      "loss": 1.0667,
      "step": 534000
    },
    {
      "epoch": 7.033542563130815,
      "grad_norm": 2.457918643951416,
      "learning_rate": 6.5275058614706e-05,
      "loss": 1.0691,
      "step": 534500
    },
    {
      "epoch": 7.040122116510731,
      "grad_norm": 2.1406564712524414,
      "learning_rate": 6.52420091929053e-05,
      "loss": 1.0627,
      "step": 535000
    },
    {
      "epoch": 7.0467016698906475,
      "grad_norm": 2.700234889984131,
      "learning_rate": 6.52088935397984e-05,
      "loss": 1.0653,
      "step": 535500
    },
    {
      "epoch": 7.053281223270565,
      "grad_norm": 2.494871139526367,
      "learning_rate": 6.517577788669147e-05,
      "loss": 1.0649,
      "step": 536000
    },
    {
      "epoch": 7.059860776650481,
      "grad_norm": 2.4258387088775635,
      "learning_rate": 6.514266223358457e-05,
      "loss": 1.0644,
      "step": 536500
    },
    {
      "epoch": 7.066440330030398,
      "grad_norm": 2.676682472229004,
      "learning_rate": 6.510961281178388e-05,
      "loss": 1.0649,
      "step": 537000
    },
    {
      "epoch": 7.073019883410314,
      "grad_norm": 2.2656006813049316,
      "learning_rate": 6.507649715867696e-05,
      "loss": 1.0633,
      "step": 537500
    },
    {
      "epoch": 7.079599436790231,
      "grad_norm": 2.237032890319824,
      "learning_rate": 6.504338150557006e-05,
      "loss": 1.0594,
      "step": 538000
    },
    {
      "epoch": 7.086178990170147,
      "grad_norm": 2.7501096725463867,
      "learning_rate": 6.501026585246315e-05,
      "loss": 1.059,
      "step": 538500
    },
    {
      "epoch": 7.0927585435500635,
      "grad_norm": 2.2614479064941406,
      "learning_rate": 6.497715019935623e-05,
      "loss": 1.0607,
      "step": 539000
    },
    {
      "epoch": 7.09933809692998,
      "grad_norm": 2.4504315853118896,
      "learning_rate": 6.494403454624932e-05,
      "loss": 1.0752,
      "step": 539500
    },
    {
      "epoch": 7.105917650309897,
      "grad_norm": 2.8272173404693604,
      "learning_rate": 6.491091889314242e-05,
      "loss": 1.0611,
      "step": 540000
    },
    {
      "epoch": 7.112497203689814,
      "grad_norm": 2.590184450149536,
      "learning_rate": 6.48778032400355e-05,
      "loss": 1.0557,
      "step": 540500
    },
    {
      "epoch": 7.11907675706973,
      "grad_norm": 2.2140953540802,
      "learning_rate": 6.484468758692859e-05,
      "loss": 1.0599,
      "step": 541000
    },
    {
      "epoch": 7.125656310449647,
      "grad_norm": 2.3198745250701904,
      "learning_rate": 6.481157193382169e-05,
      "loss": 1.0608,
      "step": 541500
    },
    {
      "epoch": 7.132235863829563,
      "grad_norm": 2.5832087993621826,
      "learning_rate": 6.477845628071477e-05,
      "loss": 1.0735,
      "step": 542000
    },
    {
      "epoch": 7.1388154172094795,
      "grad_norm": 2.365379810333252,
      "learning_rate": 6.474534062760787e-05,
      "loss": 1.0643,
      "step": 542500
    },
    {
      "epoch": 7.145394970589396,
      "grad_norm": 2.368838310241699,
      "learning_rate": 6.471229120580716e-05,
      "loss": 1.0612,
      "step": 543000
    },
    {
      "epoch": 7.151974523969313,
      "grad_norm": 2.661954879760742,
      "learning_rate": 6.467917555270026e-05,
      "loss": 1.0618,
      "step": 543500
    },
    {
      "epoch": 7.15855407734923,
      "grad_norm": 2.6768639087677,
      "learning_rate": 6.464605989959335e-05,
      "loss": 1.0659,
      "step": 544000
    },
    {
      "epoch": 7.165133630729146,
      "grad_norm": 2.0346035957336426,
      "learning_rate": 6.461294424648643e-05,
      "loss": 1.0663,
      "step": 544500
    },
    {
      "epoch": 7.171713184109063,
      "grad_norm": 2.407496929168701,
      "learning_rate": 6.457982859337952e-05,
      "loss": 1.0664,
      "step": 545000
    },
    {
      "epoch": 7.178292737488979,
      "grad_norm": 2.6673805713653564,
      "learning_rate": 6.454671294027262e-05,
      "loss": 1.0578,
      "step": 545500
    },
    {
      "epoch": 7.184872290868896,
      "grad_norm": 2.354628086090088,
      "learning_rate": 6.45135972871657e-05,
      "loss": 1.065,
      "step": 546000
    },
    {
      "epoch": 7.191451844248812,
      "grad_norm": 2.650480270385742,
      "learning_rate": 6.448048163405879e-05,
      "loss": 1.0663,
      "step": 546500
    },
    {
      "epoch": 7.198031397628729,
      "grad_norm": 2.5275704860687256,
      "learning_rate": 6.444736598095189e-05,
      "loss": 1.0643,
      "step": 547000
    },
    {
      "epoch": 7.204610951008646,
      "grad_norm": 2.4887468814849854,
      "learning_rate": 6.441431655915118e-05,
      "loss": 1.0604,
      "step": 547500
    },
    {
      "epoch": 7.211190504388562,
      "grad_norm": 2.5077576637268066,
      "learning_rate": 6.438120090604428e-05,
      "loss": 1.0568,
      "step": 548000
    },
    {
      "epoch": 7.217770057768479,
      "grad_norm": 2.223529100418091,
      "learning_rate": 6.434815148424358e-05,
      "loss": 1.0595,
      "step": 548500
    },
    {
      "epoch": 7.224349611148395,
      "grad_norm": 2.444920778274536,
      "learning_rate": 6.431503583113667e-05,
      "loss": 1.0632,
      "step": 549000
    },
    {
      "epoch": 7.230929164528312,
      "grad_norm": 2.5125679969787598,
      "learning_rate": 6.428192017802975e-05,
      "loss": 1.0661,
      "step": 549500
    },
    {
      "epoch": 7.237508717908228,
      "grad_norm": 2.4253201484680176,
      "learning_rate": 6.424880452492285e-05,
      "loss": 1.0569,
      "step": 550000
    },
    {
      "epoch": 7.244088271288145,
      "grad_norm": 2.2175545692443848,
      "learning_rate": 6.421568887181593e-05,
      "loss": 1.0654,
      "step": 550500
    },
    {
      "epoch": 7.250667824668062,
      "grad_norm": 2.482527732849121,
      "learning_rate": 6.418257321870902e-05,
      "loss": 1.0602,
      "step": 551000
    },
    {
      "epoch": 7.257247378047978,
      "grad_norm": 2.8258635997772217,
      "learning_rate": 6.41494575656021e-05,
      "loss": 1.0593,
      "step": 551500
    },
    {
      "epoch": 7.263826931427895,
      "grad_norm": 2.5130629539489746,
      "learning_rate": 6.41163419124952e-05,
      "loss": 1.0626,
      "step": 552000
    },
    {
      "epoch": 7.270406484807811,
      "grad_norm": 2.7338221073150635,
      "learning_rate": 6.40832924906945e-05,
      "loss": 1.0693,
      "step": 552500
    },
    {
      "epoch": 7.276986038187728,
      "grad_norm": 2.4734559059143066,
      "learning_rate": 6.405024306889381e-05,
      "loss": 1.0644,
      "step": 553000
    },
    {
      "epoch": 7.283565591567644,
      "grad_norm": 2.2081286907196045,
      "learning_rate": 6.40171274157869e-05,
      "loss": 1.0569,
      "step": 553500
    },
    {
      "epoch": 7.2901451449475605,
      "grad_norm": 2.2537741661071777,
      "learning_rate": 6.398401176267998e-05,
      "loss": 1.0541,
      "step": 554000
    },
    {
      "epoch": 7.296724698327478,
      "grad_norm": 2.3694307804107666,
      "learning_rate": 6.395089610957308e-05,
      "loss": 1.0602,
      "step": 554500
    },
    {
      "epoch": 7.303304251707394,
      "grad_norm": 2.4441263675689697,
      "learning_rate": 6.391778045646617e-05,
      "loss": 1.0666,
      "step": 555000
    },
    {
      "epoch": 7.309883805087311,
      "grad_norm": 2.3768248558044434,
      "learning_rate": 6.388466480335925e-05,
      "loss": 1.0581,
      "step": 555500
    },
    {
      "epoch": 7.316463358467227,
      "grad_norm": 2.5511276721954346,
      "learning_rate": 6.385161538155856e-05,
      "loss": 1.0605,
      "step": 556000
    },
    {
      "epoch": 7.323042911847144,
      "grad_norm": 2.544208288192749,
      "learning_rate": 6.381849972845166e-05,
      "loss": 1.0612,
      "step": 556500
    },
    {
      "epoch": 7.32962246522706,
      "grad_norm": 2.299095392227173,
      "learning_rate": 6.378538407534473e-05,
      "loss": 1.0558,
      "step": 557000
    },
    {
      "epoch": 7.336202018606977,
      "grad_norm": 2.538801908493042,
      "learning_rate": 6.375226842223783e-05,
      "loss": 1.0558,
      "step": 557500
    },
    {
      "epoch": 7.342781571986894,
      "grad_norm": 2.3207204341888428,
      "learning_rate": 6.371915276913091e-05,
      "loss": 1.0558,
      "step": 558000
    },
    {
      "epoch": 7.34936112536681,
      "grad_norm": 3.116994619369507,
      "learning_rate": 6.3686037116024e-05,
      "loss": 1.051,
      "step": 558500
    },
    {
      "epoch": 7.355940678746727,
      "grad_norm": 2.223259925842285,
      "learning_rate": 6.36529214629171e-05,
      "loss": 1.0573,
      "step": 559000
    },
    {
      "epoch": 7.362520232126643,
      "grad_norm": 2.6338701248168945,
      "learning_rate": 6.361980580981018e-05,
      "loss": 1.0505,
      "step": 559500
    },
    {
      "epoch": 7.36909978550656,
      "grad_norm": 2.4682846069335938,
      "learning_rate": 6.358675638800949e-05,
      "loss": 1.0602,
      "step": 560000
    },
    {
      "epoch": 7.375679338886476,
      "grad_norm": 2.463388442993164,
      "learning_rate": 6.355364073490257e-05,
      "loss": 1.0531,
      "step": 560500
    },
    {
      "epoch": 7.3822588922663925,
      "grad_norm": 2.143686056137085,
      "learning_rate": 6.352052508179567e-05,
      "loss": 1.0523,
      "step": 561000
    },
    {
      "epoch": 7.38883844564631,
      "grad_norm": 3.017230987548828,
      "learning_rate": 6.348740942868876e-05,
      "loss": 1.0642,
      "step": 561500
    },
    {
      "epoch": 7.395417999026226,
      "grad_norm": 2.2688794136047363,
      "learning_rate": 6.345436000688806e-05,
      "loss": 1.0567,
      "step": 562000
    },
    {
      "epoch": 7.401997552406143,
      "grad_norm": 2.32806396484375,
      "learning_rate": 6.342124435378115e-05,
      "loss": 1.0629,
      "step": 562500
    },
    {
      "epoch": 7.408577105786059,
      "grad_norm": 2.54548716545105,
      "learning_rate": 6.338812870067425e-05,
      "loss": 1.0629,
      "step": 563000
    },
    {
      "epoch": 7.415156659165976,
      "grad_norm": 2.4017324447631836,
      "learning_rate": 6.335507927887354e-05,
      "loss": 1.0568,
      "step": 563500
    },
    {
      "epoch": 7.421736212545892,
      "grad_norm": 2.455824851989746,
      "learning_rate": 6.332196362576664e-05,
      "loss": 1.0517,
      "step": 564000
    },
    {
      "epoch": 7.4283157659258086,
      "grad_norm": 2.4017691612243652,
      "learning_rate": 6.328884797265971e-05,
      "loss": 1.0592,
      "step": 564500
    },
    {
      "epoch": 7.434895319305726,
      "grad_norm": 2.1681201457977295,
      "learning_rate": 6.325573231955281e-05,
      "loss": 1.067,
      "step": 565000
    },
    {
      "epoch": 7.441474872685642,
      "grad_norm": 2.5395607948303223,
      "learning_rate": 6.32226166664459e-05,
      "loss": 1.0538,
      "step": 565500
    },
    {
      "epoch": 7.448054426065559,
      "grad_norm": 3.0656349658966064,
      "learning_rate": 6.318950101333899e-05,
      "loss": 1.052,
      "step": 566000
    },
    {
      "epoch": 7.454633979445475,
      "grad_norm": 2.264979839324951,
      "learning_rate": 6.315638536023208e-05,
      "loss": 1.0514,
      "step": 566500
    },
    {
      "epoch": 7.461213532825392,
      "grad_norm": 2.5225789546966553,
      "learning_rate": 6.312326970712516e-05,
      "loss": 1.0556,
      "step": 567000
    },
    {
      "epoch": 7.467793086205308,
      "grad_norm": 2.163227081298828,
      "learning_rate": 6.309015405401826e-05,
      "loss": 1.0542,
      "step": 567500
    },
    {
      "epoch": 7.474372639585225,
      "grad_norm": 2.5094966888427734,
      "learning_rate": 6.305703840091135e-05,
      "loss": 1.0555,
      "step": 568000
    },
    {
      "epoch": 7.480952192965142,
      "grad_norm": 2.540278673171997,
      "learning_rate": 6.302392274780443e-05,
      "loss": 1.0531,
      "step": 568500
    },
    {
      "epoch": 7.487531746345058,
      "grad_norm": 2.3628652095794678,
      "learning_rate": 6.299080709469752e-05,
      "loss": 1.0634,
      "step": 569000
    },
    {
      "epoch": 7.494111299724975,
      "grad_norm": 2.773759603500366,
      "learning_rate": 6.295769144159062e-05,
      "loss": 1.0607,
      "step": 569500
    },
    {
      "epoch": 7.500690853104891,
      "grad_norm": 2.2340056896209717,
      "learning_rate": 6.292464201978992e-05,
      "loss": 1.0571,
      "step": 570000
    },
    {
      "epoch": 7.507270406484808,
      "grad_norm": 2.7302567958831787,
      "learning_rate": 6.289172506060165e-05,
      "loss": 1.063,
      "step": 570500
    },
    {
      "epoch": 7.513849959864724,
      "grad_norm": 2.8184635639190674,
      "learning_rate": 6.285860940749474e-05,
      "loss": 1.0566,
      "step": 571000
    },
    {
      "epoch": 7.520429513244641,
      "grad_norm": 2.373462677001953,
      "learning_rate": 6.282549375438784e-05,
      "loss": 1.0654,
      "step": 571500
    },
    {
      "epoch": 7.527009066624558,
      "grad_norm": 2.3956098556518555,
      "learning_rate": 6.279237810128091e-05,
      "loss": 1.0594,
      "step": 572000
    },
    {
      "epoch": 7.533588620004474,
      "grad_norm": 2.6441850662231445,
      "learning_rate": 6.2759262448174e-05,
      "loss": 1.0529,
      "step": 572500
    },
    {
      "epoch": 7.540168173384391,
      "grad_norm": 2.3253698348999023,
      "learning_rate": 6.272614679506709e-05,
      "loss": 1.0513,
      "step": 573000
    },
    {
      "epoch": 7.546747726764307,
      "grad_norm": 2.5590670108795166,
      "learning_rate": 6.26930973732664e-05,
      "loss": 1.0504,
      "step": 573500
    },
    {
      "epoch": 7.553327280144224,
      "grad_norm": 2.42475962638855,
      "learning_rate": 6.26599817201595e-05,
      "loss": 1.0613,
      "step": 574000
    },
    {
      "epoch": 7.55990683352414,
      "grad_norm": 2.4482078552246094,
      "learning_rate": 6.262686606705258e-05,
      "loss": 1.0474,
      "step": 574500
    },
    {
      "epoch": 7.566486386904057,
      "grad_norm": 2.5138840675354004,
      "learning_rate": 6.259375041394567e-05,
      "loss": 1.0494,
      "step": 575000
    },
    {
      "epoch": 7.573065940283973,
      "grad_norm": 2.4844610691070557,
      "learning_rate": 6.256063476083875e-05,
      "loss": 1.053,
      "step": 575500
    },
    {
      "epoch": 7.57964549366389,
      "grad_norm": 2.5715978145599365,
      "learning_rate": 6.252751910773185e-05,
      "loss": 1.0712,
      "step": 576000
    },
    {
      "epoch": 7.586225047043807,
      "grad_norm": 2.756244659423828,
      "learning_rate": 6.249440345462494e-05,
      "loss": 1.0501,
      "step": 576500
    },
    {
      "epoch": 7.592804600423723,
      "grad_norm": 2.3475284576416016,
      "learning_rate": 6.246128780151802e-05,
      "loss": 1.0517,
      "step": 577000
    },
    {
      "epoch": 7.59938415380364,
      "grad_norm": 2.4819679260253906,
      "learning_rate": 6.242817214841112e-05,
      "loss": 1.0638,
      "step": 577500
    },
    {
      "epoch": 7.605963707183556,
      "grad_norm": 2.5401816368103027,
      "learning_rate": 6.23950564953042e-05,
      "loss": 1.0452,
      "step": 578000
    },
    {
      "epoch": 7.612543260563473,
      "grad_norm": 2.8829545974731445,
      "learning_rate": 6.23619408421973e-05,
      "loss": 1.0535,
      "step": 578500
    },
    {
      "epoch": 7.61912281394339,
      "grad_norm": 2.396291732788086,
      "learning_rate": 6.232882518909038e-05,
      "loss": 1.0594,
      "step": 579000
    },
    {
      "epoch": 7.625702367323306,
      "grad_norm": 2.697875738143921,
      "learning_rate": 6.229570953598347e-05,
      "loss": 1.0556,
      "step": 579500
    },
    {
      "epoch": 7.632281920703223,
      "grad_norm": 2.729161024093628,
      "learning_rate": 6.226259388287656e-05,
      "loss": 1.0513,
      "step": 580000
    },
    {
      "epoch": 7.638861474083139,
      "grad_norm": 2.6092638969421387,
      "learning_rate": 6.222947822976966e-05,
      "loss": 1.0479,
      "step": 580500
    },
    {
      "epoch": 7.645441027463056,
      "grad_norm": 2.7834482192993164,
      "learning_rate": 6.219636257666274e-05,
      "loss": 1.049,
      "step": 581000
    },
    {
      "epoch": 7.652020580842972,
      "grad_norm": 2.7272768020629883,
      "learning_rate": 6.216324692355583e-05,
      "loss": 1.052,
      "step": 581500
    },
    {
      "epoch": 7.658600134222889,
      "grad_norm": 2.684231996536255,
      "learning_rate": 6.213013127044893e-05,
      "loss": 1.056,
      "step": 582000
    },
    {
      "epoch": 7.665179687602805,
      "grad_norm": 2.132575750350952,
      "learning_rate": 6.209701561734201e-05,
      "loss": 1.0527,
      "step": 582500
    },
    {
      "epoch": 7.671759240982722,
      "grad_norm": 2.347851276397705,
      "learning_rate": 6.20638999642351e-05,
      "loss": 1.0542,
      "step": 583000
    },
    {
      "epoch": 7.678338794362639,
      "grad_norm": 2.370231866836548,
      "learning_rate": 6.20308505424344e-05,
      "loss": 1.0545,
      "step": 583500
    },
    {
      "epoch": 7.684918347742555,
      "grad_norm": 2.699695348739624,
      "learning_rate": 6.199773488932749e-05,
      "loss": 1.0456,
      "step": 584000
    },
    {
      "epoch": 7.691497901122472,
      "grad_norm": 2.049654722213745,
      "learning_rate": 6.196461923622057e-05,
      "loss": 1.0582,
      "step": 584500
    },
    {
      "epoch": 7.698077454502388,
      "grad_norm": 2.5530765056610107,
      "learning_rate": 6.193150358311367e-05,
      "loss": 1.0471,
      "step": 585000
    },
    {
      "epoch": 7.704657007882305,
      "grad_norm": 2.527482509613037,
      "learning_rate": 6.189838793000676e-05,
      "loss": 1.0516,
      "step": 585500
    },
    {
      "epoch": 7.711236561262222,
      "grad_norm": 2.0678763389587402,
      "learning_rate": 6.186533850820606e-05,
      "loss": 1.0656,
      "step": 586000
    },
    {
      "epoch": 7.7178161146421385,
      "grad_norm": 2.458075523376465,
      "learning_rate": 6.183222285509915e-05,
      "loss": 1.0469,
      "step": 586500
    },
    {
      "epoch": 7.724395668022055,
      "grad_norm": 2.1199660301208496,
      "learning_rate": 6.179917343329845e-05,
      "loss": 1.0485,
      "step": 587000
    },
    {
      "epoch": 7.730975221401971,
      "grad_norm": 2.457204818725586,
      "learning_rate": 6.176605778019154e-05,
      "loss": 1.0557,
      "step": 587500
    },
    {
      "epoch": 7.737554774781888,
      "grad_norm": 2.2250423431396484,
      "learning_rate": 6.173294212708464e-05,
      "loss": 1.0498,
      "step": 588000
    },
    {
      "epoch": 7.744134328161804,
      "grad_norm": 2.4256367683410645,
      "learning_rate": 6.169982647397772e-05,
      "loss": 1.0487,
      "step": 588500
    },
    {
      "epoch": 7.750713881541721,
      "grad_norm": 2.356797933578491,
      "learning_rate": 6.166671082087081e-05,
      "loss": 1.0487,
      "step": 589000
    },
    {
      "epoch": 7.757293434921637,
      "grad_norm": 2.2846484184265137,
      "learning_rate": 6.163366139907013e-05,
      "loss": 1.0557,
      "step": 589500
    },
    {
      "epoch": 7.7638729883015545,
      "grad_norm": 2.5690367221832275,
      "learning_rate": 6.16005457459632e-05,
      "loss": 1.0528,
      "step": 590000
    },
    {
      "epoch": 7.770452541681471,
      "grad_norm": 2.2939116954803467,
      "learning_rate": 6.15674300928563e-05,
      "loss": 1.0508,
      "step": 590500
    },
    {
      "epoch": 7.777032095061387,
      "grad_norm": 2.311290979385376,
      "learning_rate": 6.153431443974938e-05,
      "loss": 1.0481,
      "step": 591000
    },
    {
      "epoch": 7.783611648441304,
      "grad_norm": 2.535992383956909,
      "learning_rate": 6.150119878664248e-05,
      "loss": 1.0416,
      "step": 591500
    },
    {
      "epoch": 7.79019120182122,
      "grad_norm": 2.611372947692871,
      "learning_rate": 6.146808313353555e-05,
      "loss": 1.0509,
      "step": 592000
    },
    {
      "epoch": 7.796770755201137,
      "grad_norm": 2.5652971267700195,
      "learning_rate": 6.143496748042865e-05,
      "loss": 1.0531,
      "step": 592500
    },
    {
      "epoch": 7.803350308581053,
      "grad_norm": 2.6042380332946777,
      "learning_rate": 6.140185182732175e-05,
      "loss": 1.0568,
      "step": 593000
    },
    {
      "epoch": 7.8099298619609705,
      "grad_norm": 2.143724203109741,
      "learning_rate": 6.136880240552104e-05,
      "loss": 1.0461,
      "step": 593500
    },
    {
      "epoch": 7.816509415340887,
      "grad_norm": 2.60284423828125,
      "learning_rate": 6.133568675241414e-05,
      "loss": 1.0468,
      "step": 594000
    },
    {
      "epoch": 7.823088968720803,
      "grad_norm": 2.137969732284546,
      "learning_rate": 6.130257109930723e-05,
      "loss": 1.0518,
      "step": 594500
    },
    {
      "epoch": 7.82966852210072,
      "grad_norm": 2.2601447105407715,
      "learning_rate": 6.126945544620031e-05,
      "loss": 1.0595,
      "step": 595000
    },
    {
      "epoch": 7.836248075480636,
      "grad_norm": 3.0295658111572266,
      "learning_rate": 6.12363397930934e-05,
      "loss": 1.0505,
      "step": 595500
    },
    {
      "epoch": 7.842827628860553,
      "grad_norm": 2.552248954772949,
      "learning_rate": 6.12032241399865e-05,
      "loss": 1.0467,
      "step": 596000
    },
    {
      "epoch": 7.849407182240469,
      "grad_norm": 2.537022590637207,
      "learning_rate": 6.117010848687958e-05,
      "loss": 1.0506,
      "step": 596500
    },
    {
      "epoch": 7.855986735620386,
      "grad_norm": 2.4681077003479004,
      "learning_rate": 6.113705906507889e-05,
      "loss": 1.0534,
      "step": 597000
    },
    {
      "epoch": 7.862566289000303,
      "grad_norm": 2.710005760192871,
      "learning_rate": 6.110394341197197e-05,
      "loss": 1.0495,
      "step": 597500
    },
    {
      "epoch": 7.869145842380219,
      "grad_norm": 2.4941298961639404,
      "learning_rate": 6.107082775886507e-05,
      "loss": 1.0519,
      "step": 598000
    },
    {
      "epoch": 7.875725395760136,
      "grad_norm": 2.4708616733551025,
      "learning_rate": 6.103771210575815e-05,
      "loss": 1.0515,
      "step": 598500
    },
    {
      "epoch": 7.882304949140052,
      "grad_norm": 2.565251350402832,
      "learning_rate": 6.100459645265124e-05,
      "loss": 1.052,
      "step": 599000
    },
    {
      "epoch": 7.888884502519969,
      "grad_norm": 2.3614630699157715,
      "learning_rate": 6.097154703085054e-05,
      "loss": 1.0488,
      "step": 599500
    },
    {
      "epoch": 7.895464055899885,
      "grad_norm": 2.6892318725585938,
      "learning_rate": 6.093856384035607e-05,
      "loss": 1.048,
      "step": 600000
    },
    {
      "epoch": 7.9020436092798025,
      "grad_norm": 2.356282949447632,
      "learning_rate": 6.090544818724915e-05,
      "loss": 1.0456,
      "step": 600500
    },
    {
      "epoch": 7.908623162659719,
      "grad_norm": 2.434262990951538,
      "learning_rate": 6.087233253414224e-05,
      "loss": 1.0519,
      "step": 601000
    },
    {
      "epoch": 7.915202716039635,
      "grad_norm": 2.488643169403076,
      "learning_rate": 6.083921688103533e-05,
      "loss": 1.0469,
      "step": 601500
    },
    {
      "epoch": 7.921782269419552,
      "grad_norm": 2.490494966506958,
      "learning_rate": 6.0806101227928424e-05,
      "loss": 1.0532,
      "step": 602000
    },
    {
      "epoch": 7.928361822799468,
      "grad_norm": 2.384823799133301,
      "learning_rate": 6.07729855748215e-05,
      "loss": 1.0497,
      "step": 602500
    },
    {
      "epoch": 7.934941376179385,
      "grad_norm": 2.5742852687835693,
      "learning_rate": 6.07398699217146e-05,
      "loss": 1.0438,
      "step": 603000
    },
    {
      "epoch": 7.941520929559301,
      "grad_norm": 2.5779507160186768,
      "learning_rate": 6.0706754268607694e-05,
      "loss": 1.0449,
      "step": 603500
    },
    {
      "epoch": 7.948100482939218,
      "grad_norm": 2.710040807723999,
      "learning_rate": 6.0673638615500786e-05,
      "loss": 1.0448,
      "step": 604000
    },
    {
      "epoch": 7.954680036319135,
      "grad_norm": 2.497838258743286,
      "learning_rate": 6.0640522962393864e-05,
      "loss": 1.0516,
      "step": 604500
    },
    {
      "epoch": 7.9612595896990515,
      "grad_norm": 2.445418119430542,
      "learning_rate": 6.0607407309286956e-05,
      "loss": 1.0464,
      "step": 605000
    },
    {
      "epoch": 7.967839143078968,
      "grad_norm": 2.0252130031585693,
      "learning_rate": 6.057429165618005e-05,
      "loss": 1.0559,
      "step": 605500
    },
    {
      "epoch": 7.974418696458884,
      "grad_norm": 2.3839986324310303,
      "learning_rate": 6.054117600307313e-05,
      "loss": 1.0494,
      "step": 606000
    },
    {
      "epoch": 7.980998249838801,
      "grad_norm": 2.365950345993042,
      "learning_rate": 6.050806034996622e-05,
      "loss": 1.0496,
      "step": 606500
    },
    {
      "epoch": 7.987577803218717,
      "grad_norm": 2.3034281730651855,
      "learning_rate": 6.047501092816553e-05,
      "loss": 1.0389,
      "step": 607000
    },
    {
      "epoch": 7.994157356598635,
      "grad_norm": 2.070444345474243,
      "learning_rate": 6.0441895275058616e-05,
      "loss": 1.0349,
      "step": 607500
    },
    {
      "epoch": 8.0,
      "eval_loss": 0.9924125075340271,
      "eval_runtime": 49.5825,
      "eval_samples_per_second": 2016.842,
      "eval_steps_per_second": 15.772,
      "step": 607944
    },
    {
      "epoch": 8.000736909978551,
      "grad_norm": 2.6984264850616455,
      "learning_rate": 6.040877962195171e-05,
      "loss": 1.0448,
      "step": 608000
    },
    {
      "epoch": 8.007316463358467,
      "grad_norm": 2.7494513988494873,
      "learning_rate": 6.03756639688448e-05,
      "loss": 1.0417,
      "step": 608500
    },
    {
      "epoch": 8.013896016738384,
      "grad_norm": 2.42124080657959,
      "learning_rate": 6.034254831573789e-05,
      "loss": 1.0348,
      "step": 609000
    },
    {
      "epoch": 8.0204755701183,
      "grad_norm": 2.4764344692230225,
      "learning_rate": 6.030943266263097e-05,
      "loss": 1.0355,
      "step": 609500
    },
    {
      "epoch": 8.027055123498217,
      "grad_norm": 2.3628056049346924,
      "learning_rate": 6.027638324083028e-05,
      "loss": 1.0291,
      "step": 610000
    },
    {
      "epoch": 8.033634676878133,
      "grad_norm": 2.230449914932251,
      "learning_rate": 6.0243267587723375e-05,
      "loss": 1.0418,
      "step": 610500
    },
    {
      "epoch": 8.04021423025805,
      "grad_norm": 2.7237062454223633,
      "learning_rate": 6.021015193461645e-05,
      "loss": 1.042,
      "step": 611000
    },
    {
      "epoch": 8.046793783637966,
      "grad_norm": 2.3010003566741943,
      "learning_rate": 6.0177036281509545e-05,
      "loss": 1.0332,
      "step": 611500
    },
    {
      "epoch": 8.053373337017883,
      "grad_norm": 2.443387031555176,
      "learning_rate": 6.014392062840264e-05,
      "loss": 1.0453,
      "step": 612000
    },
    {
      "epoch": 8.059952890397799,
      "grad_norm": 2.206455707550049,
      "learning_rate": 6.0110871206601935e-05,
      "loss": 1.0303,
      "step": 612500
    },
    {
      "epoch": 8.066532443777715,
      "grad_norm": 2.783867835998535,
      "learning_rate": 6.007775555349503e-05,
      "loss": 1.0274,
      "step": 613000
    },
    {
      "epoch": 8.073111997157634,
      "grad_norm": 2.6761956214904785,
      "learning_rate": 6.004463990038812e-05,
      "loss": 1.0324,
      "step": 613500
    },
    {
      "epoch": 8.07969155053755,
      "grad_norm": 2.429675817489624,
      "learning_rate": 6.0011524247281205e-05,
      "loss": 1.0377,
      "step": 614000
    },
    {
      "epoch": 8.086271103917467,
      "grad_norm": 2.2728610038757324,
      "learning_rate": 5.99784085941743e-05,
      "loss": 1.0392,
      "step": 614500
    },
    {
      "epoch": 8.092850657297383,
      "grad_norm": 2.2845003604888916,
      "learning_rate": 5.994529294106739e-05,
      "loss": 1.0353,
      "step": 615000
    },
    {
      "epoch": 8.0994302106773,
      "grad_norm": 2.5418317317962646,
      "learning_rate": 5.991217728796048e-05,
      "loss": 1.0407,
      "step": 615500
    },
    {
      "epoch": 8.106009764057216,
      "grad_norm": 2.3908188343048096,
      "learning_rate": 5.987906163485356e-05,
      "loss": 1.0318,
      "step": 616000
    },
    {
      "epoch": 8.112589317437132,
      "grad_norm": 2.481278896331787,
      "learning_rate": 5.984594598174665e-05,
      "loss": 1.0362,
      "step": 616500
    },
    {
      "epoch": 8.119168870817049,
      "grad_norm": 2.3597517013549805,
      "learning_rate": 5.9812830328639744e-05,
      "loss": 1.0344,
      "step": 617000
    },
    {
      "epoch": 8.125748424196965,
      "grad_norm": 2.4025681018829346,
      "learning_rate": 5.977978090683904e-05,
      "loss": 1.0336,
      "step": 617500
    },
    {
      "epoch": 8.132327977576882,
      "grad_norm": 2.6305131912231445,
      "learning_rate": 5.9746665253732134e-05,
      "loss": 1.0366,
      "step": 618000
    },
    {
      "epoch": 8.138907530956798,
      "grad_norm": 2.472848653793335,
      "learning_rate": 5.9713549600625226e-05,
      "loss": 1.0427,
      "step": 618500
    },
    {
      "epoch": 8.145487084336715,
      "grad_norm": 2.394258499145508,
      "learning_rate": 5.968043394751831e-05,
      "loss": 1.0432,
      "step": 619000
    },
    {
      "epoch": 8.152066637716631,
      "grad_norm": 2.9454405307769775,
      "learning_rate": 5.964738452571762e-05,
      "loss": 1.0446,
      "step": 619500
    },
    {
      "epoch": 8.158646191096548,
      "grad_norm": 2.3116981983184814,
      "learning_rate": 5.9614268872610715e-05,
      "loss": 1.0356,
      "step": 620000
    },
    {
      "epoch": 8.165225744476466,
      "grad_norm": 2.5951714515686035,
      "learning_rate": 5.9581153219503794e-05,
      "loss": 1.038,
      "step": 620500
    },
    {
      "epoch": 8.171805297856382,
      "grad_norm": 2.635371208190918,
      "learning_rate": 5.9548037566396886e-05,
      "loss": 1.0395,
      "step": 621000
    },
    {
      "epoch": 8.178384851236299,
      "grad_norm": 2.346372604370117,
      "learning_rate": 5.951492191328998e-05,
      "loss": 1.0247,
      "step": 621500
    },
    {
      "epoch": 8.184964404616215,
      "grad_norm": 2.4553310871124268,
      "learning_rate": 5.948180626018307e-05,
      "loss": 1.0355,
      "step": 622000
    },
    {
      "epoch": 8.191543957996132,
      "grad_norm": 2.523108959197998,
      "learning_rate": 5.9448690607076155e-05,
      "loss": 1.0352,
      "step": 622500
    },
    {
      "epoch": 8.198123511376048,
      "grad_norm": 2.4073572158813477,
      "learning_rate": 5.941557495396925e-05,
      "loss": 1.0338,
      "step": 623000
    },
    {
      "epoch": 8.204703064755964,
      "grad_norm": 2.376962423324585,
      "learning_rate": 5.938245930086234e-05,
      "loss": 1.0389,
      "step": 623500
    },
    {
      "epoch": 8.211282618135881,
      "grad_norm": 2.427370548248291,
      "learning_rate": 5.934934364775543e-05,
      "loss": 1.0399,
      "step": 624000
    },
    {
      "epoch": 8.217862171515797,
      "grad_norm": 3.0242443084716797,
      "learning_rate": 5.931629422595473e-05,
      "loss": 1.0366,
      "step": 624500
    },
    {
      "epoch": 8.224441724895714,
      "grad_norm": 2.3430702686309814,
      "learning_rate": 5.928317857284782e-05,
      "loss": 1.0269,
      "step": 625000
    },
    {
      "epoch": 8.23102127827563,
      "grad_norm": 2.277736186981201,
      "learning_rate": 5.925012915104712e-05,
      "loss": 1.0348,
      "step": 625500
    },
    {
      "epoch": 8.237600831655547,
      "grad_norm": 2.282263994216919,
      "learning_rate": 5.921701349794021e-05,
      "loss": 1.0378,
      "step": 626000
    },
    {
      "epoch": 8.244180385035463,
      "grad_norm": 2.471052408218384,
      "learning_rate": 5.9183897844833304e-05,
      "loss": 1.0358,
      "step": 626500
    },
    {
      "epoch": 8.25075993841538,
      "grad_norm": 2.2341151237487793,
      "learning_rate": 5.915078219172638e-05,
      "loss": 1.0348,
      "step": 627000
    },
    {
      "epoch": 8.257339491795296,
      "grad_norm": 2.5973398685455322,
      "learning_rate": 5.9117666538619474e-05,
      "loss": 1.0313,
      "step": 627500
    },
    {
      "epoch": 8.263919045175214,
      "grad_norm": 2.5571815967559814,
      "learning_rate": 5.9084550885512566e-05,
      "loss": 1.0349,
      "step": 628000
    },
    {
      "epoch": 8.27049859855513,
      "grad_norm": 2.2983481884002686,
      "learning_rate": 5.9051501463711865e-05,
      "loss": 1.029,
      "step": 628500
    },
    {
      "epoch": 8.277078151935047,
      "grad_norm": 2.512882709503174,
      "learning_rate": 5.901838581060496e-05,
      "loss": 1.0343,
      "step": 629000
    },
    {
      "epoch": 8.283657705314964,
      "grad_norm": 2.287933111190796,
      "learning_rate": 5.898527015749805e-05,
      "loss": 1.0318,
      "step": 629500
    },
    {
      "epoch": 8.29023725869488,
      "grad_norm": 2.387803316116333,
      "learning_rate": 5.895215450439114e-05,
      "loss": 1.0369,
      "step": 630000
    },
    {
      "epoch": 8.296816812074796,
      "grad_norm": 2.5611722469329834,
      "learning_rate": 5.891910508259044e-05,
      "loss": 1.0395,
      "step": 630500
    },
    {
      "epoch": 8.303396365454713,
      "grad_norm": 2.3844847679138184,
      "learning_rate": 5.888598942948354e-05,
      "loss": 1.039,
      "step": 631000
    },
    {
      "epoch": 8.30997591883463,
      "grad_norm": 2.2944271564483643,
      "learning_rate": 5.8852873776376616e-05,
      "loss": 1.03,
      "step": 631500
    },
    {
      "epoch": 8.316555472214546,
      "grad_norm": 2.7414891719818115,
      "learning_rate": 5.881975812326971e-05,
      "loss": 1.0389,
      "step": 632000
    },
    {
      "epoch": 8.323135025594462,
      "grad_norm": 2.636613368988037,
      "learning_rate": 5.87866424701628e-05,
      "loss": 1.0379,
      "step": 632500
    },
    {
      "epoch": 8.329714578974379,
      "grad_norm": 2.5744760036468506,
      "learning_rate": 5.875352681705589e-05,
      "loss": 1.0348,
      "step": 633000
    },
    {
      "epoch": 8.336294132354295,
      "grad_norm": 2.3066320419311523,
      "learning_rate": 5.872041116394897e-05,
      "loss": 1.0368,
      "step": 633500
    },
    {
      "epoch": 8.342873685734212,
      "grad_norm": 2.3991386890411377,
      "learning_rate": 5.868736174214828e-05,
      "loss": 1.0363,
      "step": 634000
    },
    {
      "epoch": 8.34945323911413,
      "grad_norm": 2.189988613128662,
      "learning_rate": 5.8654246089041375e-05,
      "loss": 1.0285,
      "step": 634500
    },
    {
      "epoch": 8.356032792494046,
      "grad_norm": 2.323079824447632,
      "learning_rate": 5.862113043593446e-05,
      "loss": 1.0283,
      "step": 635000
    },
    {
      "epoch": 8.362612345873963,
      "grad_norm": 2.37141489982605,
      "learning_rate": 5.8588081014133765e-05,
      "loss": 1.042,
      "step": 635500
    },
    {
      "epoch": 8.36919189925388,
      "grad_norm": 2.3009605407714844,
      "learning_rate": 5.855496536102685e-05,
      "loss": 1.0289,
      "step": 636000
    },
    {
      "epoch": 8.375771452633796,
      "grad_norm": 2.3031013011932373,
      "learning_rate": 5.852184970791994e-05,
      "loss": 1.0389,
      "step": 636500
    },
    {
      "epoch": 8.382351006013712,
      "grad_norm": 2.4699807167053223,
      "learning_rate": 5.8488734054813034e-05,
      "loss": 1.0303,
      "step": 637000
    },
    {
      "epoch": 8.388930559393629,
      "grad_norm": 2.2680208683013916,
      "learning_rate": 5.8455618401706126e-05,
      "loss": 1.0268,
      "step": 637500
    },
    {
      "epoch": 8.395510112773545,
      "grad_norm": 2.3964381217956543,
      "learning_rate": 5.8422502748599205e-05,
      "loss": 1.0403,
      "step": 638000
    },
    {
      "epoch": 8.402089666153461,
      "grad_norm": 2.5674514770507812,
      "learning_rate": 5.83893870954923e-05,
      "loss": 1.0267,
      "step": 638500
    },
    {
      "epoch": 8.408669219533378,
      "grad_norm": 2.5489284992218018,
      "learning_rate": 5.835627144238539e-05,
      "loss": 1.0378,
      "step": 639000
    },
    {
      "epoch": 8.415248772913294,
      "grad_norm": 2.5360658168792725,
      "learning_rate": 5.832315578927848e-05,
      "loss": 1.0297,
      "step": 639500
    },
    {
      "epoch": 8.42182832629321,
      "grad_norm": 2.442746162414551,
      "learning_rate": 5.8290040136171567e-05,
      "loss": 1.033,
      "step": 640000
    },
    {
      "epoch": 8.428407879673127,
      "grad_norm": 2.32891845703125,
      "learning_rate": 5.825692448306466e-05,
      "loss": 1.032,
      "step": 640500
    },
    {
      "epoch": 8.434987433053044,
      "grad_norm": 2.3841395378112793,
      "learning_rate": 5.822380882995775e-05,
      "loss": 1.0345,
      "step": 641000
    },
    {
      "epoch": 8.44156698643296,
      "grad_norm": 2.5299503803253174,
      "learning_rate": 5.819069317685084e-05,
      "loss": 1.022,
      "step": 641500
    },
    {
      "epoch": 8.448146539812878,
      "grad_norm": 2.628889560699463,
      "learning_rate": 5.815764375505014e-05,
      "loss": 1.0332,
      "step": 642000
    },
    {
      "epoch": 8.454726093192795,
      "grad_norm": 2.1325323581695557,
      "learning_rate": 5.812452810194323e-05,
      "loss": 1.026,
      "step": 642500
    },
    {
      "epoch": 8.461305646572711,
      "grad_norm": 3.179929733276367,
      "learning_rate": 5.8091412448836325e-05,
      "loss": 1.0279,
      "step": 643000
    },
    {
      "epoch": 8.467885199952628,
      "grad_norm": 3.0765671730041504,
      "learning_rate": 5.8058296795729404e-05,
      "loss": 1.0366,
      "step": 643500
    },
    {
      "epoch": 8.474464753332544,
      "grad_norm": 2.332472562789917,
      "learning_rate": 5.8025181142622496e-05,
      "loss": 1.0352,
      "step": 644000
    },
    {
      "epoch": 8.48104430671246,
      "grad_norm": 2.3397326469421387,
      "learning_rate": 5.7992131720821794e-05,
      "loss": 1.0258,
      "step": 644500
    },
    {
      "epoch": 8.487623860092377,
      "grad_norm": 2.4364960193634033,
      "learning_rate": 5.7959016067714886e-05,
      "loss": 1.0308,
      "step": 645000
    },
    {
      "epoch": 8.494203413472293,
      "grad_norm": 2.609815835952759,
      "learning_rate": 5.792590041460798e-05,
      "loss": 1.0235,
      "step": 645500
    },
    {
      "epoch": 8.50078296685221,
      "grad_norm": 2.556823492050171,
      "learning_rate": 5.789278476150107e-05,
      "loss": 1.0334,
      "step": 646000
    },
    {
      "epoch": 8.507362520232126,
      "grad_norm": 2.747586250305176,
      "learning_rate": 5.7859669108394155e-05,
      "loss": 1.0289,
      "step": 646500
    },
    {
      "epoch": 8.513942073612043,
      "grad_norm": 2.6411736011505127,
      "learning_rate": 5.782655345528725e-05,
      "loss": 1.024,
      "step": 647000
    },
    {
      "epoch": 8.52052162699196,
      "grad_norm": 2.8549866676330566,
      "learning_rate": 5.779350403348656e-05,
      "loss": 1.0286,
      "step": 647500
    },
    {
      "epoch": 8.527101180371876,
      "grad_norm": 2.368204116821289,
      "learning_rate": 5.776038838037964e-05,
      "loss": 1.0215,
      "step": 648000
    },
    {
      "epoch": 8.533680733751792,
      "grad_norm": 2.4579379558563232,
      "learning_rate": 5.772733895857895e-05,
      "loss": 1.0254,
      "step": 648500
    },
    {
      "epoch": 8.540260287131709,
      "grad_norm": 2.444427490234375,
      "learning_rate": 5.769422330547203e-05,
      "loss": 1.0288,
      "step": 649000
    },
    {
      "epoch": 8.546839840511627,
      "grad_norm": 2.2931902408599854,
      "learning_rate": 5.766110765236512e-05,
      "loss": 1.0263,
      "step": 649500
    },
    {
      "epoch": 8.553419393891543,
      "grad_norm": 2.1708967685699463,
      "learning_rate": 5.762799199925821e-05,
      "loss": 1.0341,
      "step": 650000
    },
    {
      "epoch": 8.55999894727146,
      "grad_norm": 2.8284096717834473,
      "learning_rate": 5.7594876346151304e-05,
      "loss": 1.0315,
      "step": 650500
    },
    {
      "epoch": 8.566578500651376,
      "grad_norm": 2.4913406372070312,
      "learning_rate": 5.756176069304439e-05,
      "loss": 1.0304,
      "step": 651000
    },
    {
      "epoch": 8.573158054031293,
      "grad_norm": 2.275942087173462,
      "learning_rate": 5.752864503993748e-05,
      "loss": 1.0162,
      "step": 651500
    },
    {
      "epoch": 8.579737607411209,
      "grad_norm": 2.786104917526245,
      "learning_rate": 5.7495529386830573e-05,
      "loss": 1.0267,
      "step": 652000
    },
    {
      "epoch": 8.586317160791126,
      "grad_norm": 2.577934741973877,
      "learning_rate": 5.746247996502987e-05,
      "loss": 1.0228,
      "step": 652500
    },
    {
      "epoch": 8.592896714171042,
      "grad_norm": 2.724302291870117,
      "learning_rate": 5.7429364311922964e-05,
      "loss": 1.0242,
      "step": 653000
    },
    {
      "epoch": 8.599476267550958,
      "grad_norm": 2.5967893600463867,
      "learning_rate": 5.7396248658816056e-05,
      "loss": 1.0305,
      "step": 653500
    },
    {
      "epoch": 8.606055820930875,
      "grad_norm": 2.457547187805176,
      "learning_rate": 5.7363199237015354e-05,
      "loss": 1.0327,
      "step": 654000
    },
    {
      "epoch": 8.612635374310791,
      "grad_norm": 2.423680067062378,
      "learning_rate": 5.7330083583908446e-05,
      "loss": 1.0242,
      "step": 654500
    },
    {
      "epoch": 8.619214927690708,
      "grad_norm": 2.51533842086792,
      "learning_rate": 5.729696793080154e-05,
      "loss": 1.0296,
      "step": 655000
    },
    {
      "epoch": 8.625794481070624,
      "grad_norm": 2.891371726989746,
      "learning_rate": 5.7263852277694616e-05,
      "loss": 1.0311,
      "step": 655500
    },
    {
      "epoch": 8.632374034450542,
      "grad_norm": 2.265681505203247,
      "learning_rate": 5.723073662458771e-05,
      "loss": 1.0273,
      "step": 656000
    },
    {
      "epoch": 8.638953587830459,
      "grad_norm": 2.667985439300537,
      "learning_rate": 5.71976209714808e-05,
      "loss": 1.0283,
      "step": 656500
    },
    {
      "epoch": 8.645533141210375,
      "grad_norm": 2.718975782394409,
      "learning_rate": 5.716450531837389e-05,
      "loss": 1.0269,
      "step": 657000
    },
    {
      "epoch": 8.652112694590292,
      "grad_norm": 2.797088623046875,
      "learning_rate": 5.713138966526698e-05,
      "loss": 1.0294,
      "step": 657500
    },
    {
      "epoch": 8.658692247970208,
      "grad_norm": 2.4555227756500244,
      "learning_rate": 5.709834024346628e-05,
      "loss": 1.0232,
      "step": 658000
    },
    {
      "epoch": 8.665271801350125,
      "grad_norm": 2.587780237197876,
      "learning_rate": 5.706522459035938e-05,
      "loss": 1.0284,
      "step": 658500
    },
    {
      "epoch": 8.671851354730041,
      "grad_norm": 2.3176467418670654,
      "learning_rate": 5.703210893725246e-05,
      "loss": 1.0276,
      "step": 659000
    },
    {
      "epoch": 8.678430908109958,
      "grad_norm": 2.277057647705078,
      "learning_rate": 5.699899328414555e-05,
      "loss": 1.0248,
      "step": 659500
    },
    {
      "epoch": 8.685010461489874,
      "grad_norm": 2.4831950664520264,
      "learning_rate": 5.6965877631038644e-05,
      "loss": 1.0248,
      "step": 660000
    },
    {
      "epoch": 8.69159001486979,
      "grad_norm": 2.546532154083252,
      "learning_rate": 5.6932761977931736e-05,
      "loss": 1.0324,
      "step": 660500
    },
    {
      "epoch": 8.698169568249707,
      "grad_norm": 2.417001247406006,
      "learning_rate": 5.6899646324824815e-05,
      "loss": 1.0293,
      "step": 661000
    },
    {
      "epoch": 8.704749121629623,
      "grad_norm": 2.5277788639068604,
      "learning_rate": 5.686653067171791e-05,
      "loss": 1.0299,
      "step": 661500
    },
    {
      "epoch": 8.71132867500954,
      "grad_norm": 2.3117923736572266,
      "learning_rate": 5.6833415018611006e-05,
      "loss": 1.0197,
      "step": 662000
    },
    {
      "epoch": 8.717908228389456,
      "grad_norm": 2.507659912109375,
      "learning_rate": 5.680043182811652e-05,
      "loss": 1.0309,
      "step": 662500
    },
    {
      "epoch": 8.724487781769373,
      "grad_norm": 2.1502559185028076,
      "learning_rate": 5.676731617500961e-05,
      "loss": 1.0251,
      "step": 663000
    },
    {
      "epoch": 8.731067335149291,
      "grad_norm": 2.593579053878784,
      "learning_rate": 5.6734200521902694e-05,
      "loss": 1.0182,
      "step": 663500
    },
    {
      "epoch": 8.737646888529207,
      "grad_norm": 2.584958076477051,
      "learning_rate": 5.6701084868795786e-05,
      "loss": 1.0231,
      "step": 664000
    },
    {
      "epoch": 8.744226441909124,
      "grad_norm": 2.654439687728882,
      "learning_rate": 5.666796921568888e-05,
      "loss": 1.0259,
      "step": 664500
    },
    {
      "epoch": 8.75080599528904,
      "grad_norm": 2.5505917072296143,
      "learning_rate": 5.6634919793888176e-05,
      "loss": 1.0206,
      "step": 665000
    },
    {
      "epoch": 8.757385548668957,
      "grad_norm": 2.659614086151123,
      "learning_rate": 5.660180414078127e-05,
      "loss": 1.0369,
      "step": 665500
    },
    {
      "epoch": 8.763965102048873,
      "grad_norm": 2.420771598815918,
      "learning_rate": 5.656868848767436e-05,
      "loss": 1.0306,
      "step": 666000
    },
    {
      "epoch": 8.77054465542879,
      "grad_norm": 2.436971426010132,
      "learning_rate": 5.653557283456744e-05,
      "loss": 1.0172,
      "step": 666500
    },
    {
      "epoch": 8.777124208808706,
      "grad_norm": 2.366729259490967,
      "learning_rate": 5.650245718146053e-05,
      "loss": 1.0221,
      "step": 667000
    },
    {
      "epoch": 8.783703762188622,
      "grad_norm": 2.3280067443847656,
      "learning_rate": 5.646934152835362e-05,
      "loss": 1.0173,
      "step": 667500
    },
    {
      "epoch": 8.790283315568539,
      "grad_norm": 2.5947253704071045,
      "learning_rate": 5.6436225875246715e-05,
      "loss": 1.024,
      "step": 668000
    },
    {
      "epoch": 8.796862868948455,
      "grad_norm": 2.4971601963043213,
      "learning_rate": 5.64031102221398e-05,
      "loss": 1.028,
      "step": 668500
    },
    {
      "epoch": 8.803442422328372,
      "grad_norm": 2.2647433280944824,
      "learning_rate": 5.6370060800339106e-05,
      "loss": 1.0249,
      "step": 669000
    },
    {
      "epoch": 8.810021975708288,
      "grad_norm": 2.2092397212982178,
      "learning_rate": 5.63369451472322e-05,
      "loss": 1.0271,
      "step": 669500
    },
    {
      "epoch": 8.816601529088205,
      "grad_norm": 2.449645519256592,
      "learning_rate": 5.630382949412528e-05,
      "loss": 1.0169,
      "step": 670000
    },
    {
      "epoch": 8.823181082468121,
      "grad_norm": 2.4934544563293457,
      "learning_rate": 5.6270780072324595e-05,
      "loss": 1.0305,
      "step": 670500
    },
    {
      "epoch": 8.82976063584804,
      "grad_norm": 2.328583002090454,
      "learning_rate": 5.623766441921769e-05,
      "loss": 1.024,
      "step": 671000
    },
    {
      "epoch": 8.836340189227956,
      "grad_norm": 2.545358180999756,
      "learning_rate": 5.6204548766110765e-05,
      "loss": 1.0349,
      "step": 671500
    },
    {
      "epoch": 8.842919742607872,
      "grad_norm": 2.231290817260742,
      "learning_rate": 5.617143311300386e-05,
      "loss": 1.0254,
      "step": 672000
    },
    {
      "epoch": 8.849499295987789,
      "grad_norm": 2.396618366241455,
      "learning_rate": 5.613831745989695e-05,
      "loss": 1.0191,
      "step": 672500
    },
    {
      "epoch": 8.856078849367705,
      "grad_norm": 2.2147295475006104,
      "learning_rate": 5.610520180679003e-05,
      "loss": 1.0281,
      "step": 673000
    },
    {
      "epoch": 8.862658402747622,
      "grad_norm": 2.380382537841797,
      "learning_rate": 5.607208615368312e-05,
      "loss": 1.0186,
      "step": 673500
    },
    {
      "epoch": 8.869237956127538,
      "grad_norm": 2.105659008026123,
      "learning_rate": 5.603897050057622e-05,
      "loss": 1.027,
      "step": 674000
    },
    {
      "epoch": 8.875817509507455,
      "grad_norm": 2.6671173572540283,
      "learning_rate": 5.600585484746931e-05,
      "loss": 1.0224,
      "step": 674500
    },
    {
      "epoch": 8.882397062887371,
      "grad_norm": 2.5597283840179443,
      "learning_rate": 5.597280542566861e-05,
      "loss": 1.0187,
      "step": 675000
    },
    {
      "epoch": 8.888976616267287,
      "grad_norm": 2.0597169399261475,
      "learning_rate": 5.593988846648034e-05,
      "loss": 1.0245,
      "step": 675500
    },
    {
      "epoch": 8.895556169647204,
      "grad_norm": 2.2719500064849854,
      "learning_rate": 5.590677281337343e-05,
      "loss": 1.025,
      "step": 676000
    },
    {
      "epoch": 8.90213572302712,
      "grad_norm": 2.2690963745117188,
      "learning_rate": 5.587365716026651e-05,
      "loss": 1.0213,
      "step": 676500
    },
    {
      "epoch": 8.908715276407037,
      "grad_norm": 2.6087021827697754,
      "learning_rate": 5.58405415071596e-05,
      "loss": 1.0178,
      "step": 677000
    },
    {
      "epoch": 8.915294829786955,
      "grad_norm": 2.3730080127716064,
      "learning_rate": 5.5807425854052694e-05,
      "loss": 1.0254,
      "step": 677500
    },
    {
      "epoch": 8.921874383166871,
      "grad_norm": 2.285616874694824,
      "learning_rate": 5.5774310200945786e-05,
      "loss": 1.0209,
      "step": 678000
    },
    {
      "epoch": 8.928453936546788,
      "grad_norm": 2.4945435523986816,
      "learning_rate": 5.574119454783887e-05,
      "loss": 1.022,
      "step": 678500
    },
    {
      "epoch": 8.935033489926704,
      "grad_norm": 2.675630569458008,
      "learning_rate": 5.5708078894731964e-05,
      "loss": 1.0231,
      "step": 679000
    },
    {
      "epoch": 8.94161304330662,
      "grad_norm": 2.3427608013153076,
      "learning_rate": 5.5674963241625056e-05,
      "loss": 1.0263,
      "step": 679500
    },
    {
      "epoch": 8.948192596686537,
      "grad_norm": 2.4128799438476562,
      "learning_rate": 5.564184758851815e-05,
      "loss": 1.0209,
      "step": 680000
    },
    {
      "epoch": 8.954772150066454,
      "grad_norm": 2.258212089538574,
      "learning_rate": 5.5608731935411226e-05,
      "loss": 1.017,
      "step": 680500
    },
    {
      "epoch": 8.96135170344637,
      "grad_norm": 2.5299015045166016,
      "learning_rate": 5.557561628230432e-05,
      "loss": 1.0275,
      "step": 681000
    },
    {
      "epoch": 8.967931256826287,
      "grad_norm": 2.5074775218963623,
      "learning_rate": 5.554250062919741e-05,
      "loss": 1.0137,
      "step": 681500
    },
    {
      "epoch": 8.974510810206203,
      "grad_norm": 2.7985944747924805,
      "learning_rate": 5.550938497609051e-05,
      "loss": 1.0171,
      "step": 682000
    },
    {
      "epoch": 8.98109036358612,
      "grad_norm": 2.8720815181732178,
      "learning_rate": 5.547626932298359e-05,
      "loss": 1.0144,
      "step": 682500
    },
    {
      "epoch": 8.987669916966036,
      "grad_norm": 2.474212169647217,
      "learning_rate": 5.544315366987668e-05,
      "loss": 1.012,
      "step": 683000
    },
    {
      "epoch": 8.994249470345952,
      "grad_norm": 2.41593337059021,
      "learning_rate": 5.541010424807598e-05,
      "loss": 1.0256,
      "step": 683500
    },
    {
      "epoch": 9.0,
      "eval_loss": 0.9784762263298035,
      "eval_runtime": 49.6557,
      "eval_samples_per_second": 2013.866,
      "eval_steps_per_second": 15.748,
      "step": 683937
    },
    {
      "epoch": 9.000829023725869,
      "grad_norm": 2.1412956714630127,
      "learning_rate": 5.537698859496907e-05,
      "loss": 1.0131,
      "step": 684000
    },
    {
      "epoch": 9.007408577105785,
      "grad_norm": 2.5615575313568115,
      "learning_rate": 5.534387294186216e-05,
      "loss": 1.0058,
      "step": 684500
    },
    {
      "epoch": 9.013988130485703,
      "grad_norm": 2.0701401233673096,
      "learning_rate": 5.531082352006146e-05,
      "loss": 1.0082,
      "step": 685000
    },
    {
      "epoch": 9.02056768386562,
      "grad_norm": 2.604063034057617,
      "learning_rate": 5.527777409826077e-05,
      "loss": 1.0064,
      "step": 685500
    },
    {
      "epoch": 9.027147237245536,
      "grad_norm": 2.615218162536621,
      "learning_rate": 5.5244658445153864e-05,
      "loss": 1.018,
      "step": 686000
    },
    {
      "epoch": 9.033726790625453,
      "grad_norm": 2.6799168586730957,
      "learning_rate": 5.521154279204694e-05,
      "loss": 1.0012,
      "step": 686500
    },
    {
      "epoch": 9.04030634400537,
      "grad_norm": 2.3685429096221924,
      "learning_rate": 5.5178427138940035e-05,
      "loss": 1.0188,
      "step": 687000
    },
    {
      "epoch": 9.046885897385286,
      "grad_norm": 2.5950613021850586,
      "learning_rate": 5.514531148583313e-05,
      "loss": 1.0121,
      "step": 687500
    },
    {
      "epoch": 9.053465450765202,
      "grad_norm": 2.3145687580108643,
      "learning_rate": 5.511219583272622e-05,
      "loss": 1.0114,
      "step": 688000
    },
    {
      "epoch": 9.060045004145119,
      "grad_norm": 2.5194828510284424,
      "learning_rate": 5.5079080179619304e-05,
      "loss": 1.0065,
      "step": 688500
    },
    {
      "epoch": 9.066624557525035,
      "grad_norm": 2.6217167377471924,
      "learning_rate": 5.5045964526512396e-05,
      "loss": 1.0102,
      "step": 689000
    },
    {
      "epoch": 9.073204110904951,
      "grad_norm": 2.2669341564178467,
      "learning_rate": 5.501284887340549e-05,
      "loss": 1.0038,
      "step": 689500
    },
    {
      "epoch": 9.079783664284868,
      "grad_norm": 3.3380045890808105,
      "learning_rate": 5.4979799451604786e-05,
      "loss": 1.0098,
      "step": 690000
    },
    {
      "epoch": 9.086363217664784,
      "grad_norm": 2.487865686416626,
      "learning_rate": 5.494668379849788e-05,
      "loss": 1.0088,
      "step": 690500
    },
    {
      "epoch": 9.0929427710447,
      "grad_norm": 2.5170772075653076,
      "learning_rate": 5.491356814539097e-05,
      "loss": 1.0091,
      "step": 691000
    },
    {
      "epoch": 9.099522324424617,
      "grad_norm": 2.3515872955322266,
      "learning_rate": 5.488045249228405e-05,
      "loss": 1.0134,
      "step": 691500
    },
    {
      "epoch": 9.106101877804536,
      "grad_norm": 3.7126684188842773,
      "learning_rate": 5.484733683917714e-05,
      "loss": 1.0096,
      "step": 692000
    },
    {
      "epoch": 9.112681431184452,
      "grad_norm": 2.749504327774048,
      "learning_rate": 5.481422118607023e-05,
      "loss": 1.0109,
      "step": 692500
    },
    {
      "epoch": 9.119260984564368,
      "grad_norm": 2.478034496307373,
      "learning_rate": 5.4781105532963325e-05,
      "loss": 0.9994,
      "step": 693000
    },
    {
      "epoch": 9.125840537944285,
      "grad_norm": 2.433748483657837,
      "learning_rate": 5.474798987985641e-05,
      "loss": 1.0123,
      "step": 693500
    },
    {
      "epoch": 9.132420091324201,
      "grad_norm": 2.718661308288574,
      "learning_rate": 5.47148742267495e-05,
      "loss": 1.0091,
      "step": 694000
    },
    {
      "epoch": 9.138999644704118,
      "grad_norm": 2.594400405883789,
      "learning_rate": 5.4681824804948814e-05,
      "loss": 1.0099,
      "step": 694500
    },
    {
      "epoch": 9.145579198084034,
      "grad_norm": 2.5376298427581787,
      "learning_rate": 5.464870915184189e-05,
      "loss": 1.0087,
      "step": 695000
    },
    {
      "epoch": 9.15215875146395,
      "grad_norm": 2.4413647651672363,
      "learning_rate": 5.4615593498734985e-05,
      "loss": 1.0117,
      "step": 695500
    },
    {
      "epoch": 9.158738304843867,
      "grad_norm": 2.2596423625946045,
      "learning_rate": 5.458247784562808e-05,
      "loss": 1.0067,
      "step": 696000
    },
    {
      "epoch": 9.165317858223784,
      "grad_norm": 2.2017221450805664,
      "learning_rate": 5.4549362192521156e-05,
      "loss": 1.0115,
      "step": 696500
    },
    {
      "epoch": 9.1718974116037,
      "grad_norm": 2.4018497467041016,
      "learning_rate": 5.451624653941425e-05,
      "loss": 1.0047,
      "step": 697000
    },
    {
      "epoch": 9.178476964983616,
      "grad_norm": 2.495126485824585,
      "learning_rate": 5.448319711761356e-05,
      "loss": 1.0134,
      "step": 697500
    },
    {
      "epoch": 9.185056518363533,
      "grad_norm": 2.4791440963745117,
      "learning_rate": 5.4450081464506645e-05,
      "loss": 1.0108,
      "step": 698000
    },
    {
      "epoch": 9.19163607174345,
      "grad_norm": 2.4899401664733887,
      "learning_rate": 5.441696581139974e-05,
      "loss": 1.0148,
      "step": 698500
    },
    {
      "epoch": 9.198215625123366,
      "grad_norm": 2.295897960662842,
      "learning_rate": 5.438385015829283e-05,
      "loss": 1.0062,
      "step": 699000
    },
    {
      "epoch": 9.204795178503284,
      "grad_norm": 2.5979936122894287,
      "learning_rate": 5.435073450518592e-05,
      "loss": 1.0097,
      "step": 699500
    },
    {
      "epoch": 9.2113747318832,
      "grad_norm": 2.3412837982177734,
      "learning_rate": 5.4317618852079e-05,
      "loss": 1.0115,
      "step": 700000
    },
    {
      "epoch": 9.217954285263117,
      "grad_norm": 2.423480272293091,
      "learning_rate": 5.428450319897209e-05,
      "loss": 1.0056,
      "step": 700500
    },
    {
      "epoch": 9.224533838643033,
      "grad_norm": 2.163783073425293,
      "learning_rate": 5.4251387545865184e-05,
      "loss": 1.0071,
      "step": 701000
    },
    {
      "epoch": 9.23111339202295,
      "grad_norm": 2.5224781036376953,
      "learning_rate": 5.4218271892758276e-05,
      "loss": 1.0018,
      "step": 701500
    },
    {
      "epoch": 9.237692945402866,
      "grad_norm": 2.391568183898926,
      "learning_rate": 5.4185222470957574e-05,
      "loss": 1.0054,
      "step": 702000
    },
    {
      "epoch": 9.244272498782783,
      "grad_norm": 2.4445836544036865,
      "learning_rate": 5.4152106817850666e-05,
      "loss": 1.0106,
      "step": 702500
    },
    {
      "epoch": 9.2508520521627,
      "grad_norm": 2.53814697265625,
      "learning_rate": 5.411899116474375e-05,
      "loss": 1.0078,
      "step": 703000
    },
    {
      "epoch": 9.257431605542616,
      "grad_norm": 2.524665355682373,
      "learning_rate": 5.408587551163684e-05,
      "loss": 1.0103,
      "step": 703500
    },
    {
      "epoch": 9.264011158922532,
      "grad_norm": 2.3974673748016357,
      "learning_rate": 5.4052759858529935e-05,
      "loss": 1.0113,
      "step": 704000
    },
    {
      "epoch": 9.270590712302448,
      "grad_norm": 2.2323646545410156,
      "learning_rate": 5.401964420542303e-05,
      "loss": 1.0156,
      "step": 704500
    },
    {
      "epoch": 9.277170265682365,
      "grad_norm": 2.36283016204834,
      "learning_rate": 5.3986528552316106e-05,
      "loss": 1.0097,
      "step": 705000
    },
    {
      "epoch": 9.283749819062281,
      "grad_norm": 2.1502583026885986,
      "learning_rate": 5.395354536182163e-05,
      "loss": 1.004,
      "step": 705500
    },
    {
      "epoch": 9.290329372442198,
      "grad_norm": 2.341468095779419,
      "learning_rate": 5.3920429708714716e-05,
      "loss": 1.0026,
      "step": 706000
    },
    {
      "epoch": 9.296908925822116,
      "grad_norm": 2.241232395172119,
      "learning_rate": 5.388731405560781e-05,
      "loss": 1.0097,
      "step": 706500
    },
    {
      "epoch": 9.303488479202032,
      "grad_norm": 2.514885187149048,
      "learning_rate": 5.38541984025009e-05,
      "loss": 1.0194,
      "step": 707000
    },
    {
      "epoch": 9.310068032581949,
      "grad_norm": 2.4522829055786133,
      "learning_rate": 5.382108274939398e-05,
      "loss": 1.0095,
      "step": 707500
    },
    {
      "epoch": 9.316647585961865,
      "grad_norm": 2.259676694869995,
      "learning_rate": 5.378796709628707e-05,
      "loss": 1.0126,
      "step": 708000
    },
    {
      "epoch": 9.323227139341782,
      "grad_norm": 2.500340223312378,
      "learning_rate": 5.375485144318016e-05,
      "loss": 1.0082,
      "step": 708500
    },
    {
      "epoch": 9.329806692721698,
      "grad_norm": 2.1173946857452393,
      "learning_rate": 5.372180202137946e-05,
      "loss": 1.0117,
      "step": 709000
    },
    {
      "epoch": 9.336386246101615,
      "grad_norm": 2.471531867980957,
      "learning_rate": 5.368868636827256e-05,
      "loss": 1.0095,
      "step": 709500
    },
    {
      "epoch": 9.342965799481531,
      "grad_norm": 3.011401653289795,
      "learning_rate": 5.365557071516565e-05,
      "loss": 1.008,
      "step": 710000
    },
    {
      "epoch": 9.349545352861448,
      "grad_norm": 2.564829111099243,
      "learning_rate": 5.362252129336495e-05,
      "loss": 1.0091,
      "step": 710500
    },
    {
      "epoch": 9.356124906241364,
      "grad_norm": 2.206346035003662,
      "learning_rate": 5.358940564025804e-05,
      "loss": 1.0018,
      "step": 711000
    },
    {
      "epoch": 9.36270445962128,
      "grad_norm": 2.2454521656036377,
      "learning_rate": 5.3556289987151134e-05,
      "loss": 1.0141,
      "step": 711500
    },
    {
      "epoch": 9.369284013001197,
      "grad_norm": 2.2386419773101807,
      "learning_rate": 5.3523174334044226e-05,
      "loss": 1.0107,
      "step": 712000
    },
    {
      "epoch": 9.375863566381113,
      "grad_norm": 2.335500478744507,
      "learning_rate": 5.3490058680937304e-05,
      "loss": 1.0022,
      "step": 712500
    },
    {
      "epoch": 9.38244311976103,
      "grad_norm": 2.4644594192504883,
      "learning_rate": 5.3456943027830396e-05,
      "loss": 1.0039,
      "step": 713000
    },
    {
      "epoch": 9.389022673140946,
      "grad_norm": 2.308063268661499,
      "learning_rate": 5.342382737472349e-05,
      "loss": 1.0046,
      "step": 713500
    },
    {
      "epoch": 9.395602226520865,
      "grad_norm": 2.391129970550537,
      "learning_rate": 5.3390711721616574e-05,
      "loss": 1.0081,
      "step": 714000
    },
    {
      "epoch": 9.402181779900781,
      "grad_norm": 2.4653866291046143,
      "learning_rate": 5.3357596068509666e-05,
      "loss": 1.0043,
      "step": 714500
    },
    {
      "epoch": 9.408761333280697,
      "grad_norm": 2.5103392601013184,
      "learning_rate": 5.332454664670897e-05,
      "loss": 1.0139,
      "step": 715000
    },
    {
      "epoch": 9.415340886660614,
      "grad_norm": 2.31780743598938,
      "learning_rate": 5.3291430993602056e-05,
      "loss": 1.0118,
      "step": 715500
    },
    {
      "epoch": 9.42192044004053,
      "grad_norm": 2.522263288497925,
      "learning_rate": 5.325831534049515e-05,
      "loss": 1.0009,
      "step": 716000
    },
    {
      "epoch": 9.428499993420447,
      "grad_norm": 2.442810297012329,
      "learning_rate": 5.322519968738824e-05,
      "loss": 1.0024,
      "step": 716500
    },
    {
      "epoch": 9.435079546800363,
      "grad_norm": 2.3423073291778564,
      "learning_rate": 5.319208403428133e-05,
      "loss": 1.0117,
      "step": 717000
    },
    {
      "epoch": 9.44165910018028,
      "grad_norm": 2.486964464187622,
      "learning_rate": 5.315896838117441e-05,
      "loss": 1.0022,
      "step": 717500
    },
    {
      "epoch": 9.448238653560196,
      "grad_norm": 2.5217270851135254,
      "learning_rate": 5.312591895937372e-05,
      "loss": 1.0034,
      "step": 718000
    },
    {
      "epoch": 9.454818206940113,
      "grad_norm": 2.4628283977508545,
      "learning_rate": 5.3092803306266815e-05,
      "loss": 1.0082,
      "step": 718500
    },
    {
      "epoch": 9.461397760320029,
      "grad_norm": 2.547471046447754,
      "learning_rate": 5.305968765315989e-05,
      "loss": 1.0162,
      "step": 719000
    },
    {
      "epoch": 9.467977313699945,
      "grad_norm": 2.6064882278442383,
      "learning_rate": 5.3026572000052985e-05,
      "loss": 1.0103,
      "step": 719500
    },
    {
      "epoch": 9.474556867079862,
      "grad_norm": 2.4691529273986816,
      "learning_rate": 5.299345634694608e-05,
      "loss": 1.0099,
      "step": 720000
    },
    {
      "epoch": 9.481136420459778,
      "grad_norm": 2.3809762001037598,
      "learning_rate": 5.296034069383916e-05,
      "loss": 1.0011,
      "step": 720500
    },
    {
      "epoch": 9.487715973839697,
      "grad_norm": 2.378480911254883,
      "learning_rate": 5.2927225040732255e-05,
      "loss": 1.0039,
      "step": 721000
    },
    {
      "epoch": 9.494295527219613,
      "grad_norm": 2.3661258220672607,
      "learning_rate": 5.289410938762535e-05,
      "loss": 1.0073,
      "step": 721500
    },
    {
      "epoch": 9.50087508059953,
      "grad_norm": 2.2613861560821533,
      "learning_rate": 5.2861059965824645e-05,
      "loss": 1.0007,
      "step": 722000
    },
    {
      "epoch": 9.507454633979446,
      "grad_norm": 2.301043748855591,
      "learning_rate": 5.282794431271774e-05,
      "loss": 1.0088,
      "step": 722500
    },
    {
      "epoch": 9.514034187359362,
      "grad_norm": 2.5813300609588623,
      "learning_rate": 5.279482865961083e-05,
      "loss": 1.0103,
      "step": 723000
    },
    {
      "epoch": 9.520613740739279,
      "grad_norm": 2.320704221725464,
      "learning_rate": 5.276177923781013e-05,
      "loss": 1.0125,
      "step": 723500
    },
    {
      "epoch": 9.527193294119195,
      "grad_norm": 2.679919481277466,
      "learning_rate": 5.272866358470322e-05,
      "loss": 0.9999,
      "step": 724000
    },
    {
      "epoch": 9.533772847499112,
      "grad_norm": 2.474592685699463,
      "learning_rate": 5.269554793159631e-05,
      "loss": 0.9939,
      "step": 724500
    },
    {
      "epoch": 9.540352400879028,
      "grad_norm": 2.564836025238037,
      "learning_rate": 5.26624322784894e-05,
      "loss": 1.0046,
      "step": 725000
    },
    {
      "epoch": 9.546931954258945,
      "grad_norm": 2.5351290702819824,
      "learning_rate": 5.26293828566887e-05,
      "loss": 1.0001,
      "step": 725500
    },
    {
      "epoch": 9.553511507638861,
      "grad_norm": 7.19432258605957,
      "learning_rate": 5.2596333434888e-05,
      "loss": 1.0064,
      "step": 726000
    },
    {
      "epoch": 9.560091061018777,
      "grad_norm": 2.701205015182495,
      "learning_rate": 5.256321778178109e-05,
      "loss": 1.0001,
      "step": 726500
    },
    {
      "epoch": 9.566670614398694,
      "grad_norm": 2.484943151473999,
      "learning_rate": 5.2530102128674184e-05,
      "loss": 1.0023,
      "step": 727000
    },
    {
      "epoch": 9.57325016777861,
      "grad_norm": 2.141024112701416,
      "learning_rate": 5.2496986475567276e-05,
      "loss": 1.0113,
      "step": 727500
    },
    {
      "epoch": 9.579829721158529,
      "grad_norm": 2.6327450275421143,
      "learning_rate": 5.246387082246036e-05,
      "loss": 1.0137,
      "step": 728000
    },
    {
      "epoch": 9.586409274538445,
      "grad_norm": 2.179381847381592,
      "learning_rate": 5.243075516935345e-05,
      "loss": 1.0028,
      "step": 728500
    },
    {
      "epoch": 9.592988827918361,
      "grad_norm": 2.27762770652771,
      "learning_rate": 5.2397639516246545e-05,
      "loss": 0.9947,
      "step": 729000
    },
    {
      "epoch": 9.599568381298278,
      "grad_norm": 2.6732370853424072,
      "learning_rate": 5.236452386313964e-05,
      "loss": 1.0079,
      "step": 729500
    },
    {
      "epoch": 9.606147934678194,
      "grad_norm": 2.318561553955078,
      "learning_rate": 5.2331408210032716e-05,
      "loss": 1.0083,
      "step": 730000
    },
    {
      "epoch": 9.61272748805811,
      "grad_norm": 2.5626509189605713,
      "learning_rate": 5.229835878823203e-05,
      "loss": 1.005,
      "step": 730500
    },
    {
      "epoch": 9.619307041438027,
      "grad_norm": 3.0316247940063477,
      "learning_rate": 5.2265243135125106e-05,
      "loss": 0.9952,
      "step": 731000
    },
    {
      "epoch": 9.625886594817944,
      "grad_norm": 2.7185633182525635,
      "learning_rate": 5.22321274820182e-05,
      "loss": 1.0035,
      "step": 731500
    },
    {
      "epoch": 9.63246614819786,
      "grad_norm": 2.463538885116577,
      "learning_rate": 5.219901182891129e-05,
      "loss": 1.0075,
      "step": 732000
    },
    {
      "epoch": 9.639045701577777,
      "grad_norm": 2.4850950241088867,
      "learning_rate": 5.216589617580438e-05,
      "loss": 1.0,
      "step": 732500
    },
    {
      "epoch": 9.645625254957693,
      "grad_norm": 2.9232118129730225,
      "learning_rate": 5.213278052269747e-05,
      "loss": 1.0025,
      "step": 733000
    },
    {
      "epoch": 9.65220480833761,
      "grad_norm": 2.4739880561828613,
      "learning_rate": 5.209966486959056e-05,
      "loss": 1.0086,
      "step": 733500
    },
    {
      "epoch": 9.658784361717526,
      "grad_norm": 2.6840248107910156,
      "learning_rate": 5.206654921648365e-05,
      "loss": 1.0024,
      "step": 734000
    },
    {
      "epoch": 9.665363915097442,
      "grad_norm": 2.4039132595062256,
      "learning_rate": 5.203349979468295e-05,
      "loss": 1.0021,
      "step": 734500
    },
    {
      "epoch": 9.671943468477359,
      "grad_norm": 2.647582769393921,
      "learning_rate": 5.200045037288226e-05,
      "loss": 0.9986,
      "step": 735000
    },
    {
      "epoch": 9.678523021857277,
      "grad_norm": 2.4439055919647217,
      "learning_rate": 5.1967334719775354e-05,
      "loss": 1.0076,
      "step": 735500
    },
    {
      "epoch": 9.685102575237194,
      "grad_norm": 2.4711875915527344,
      "learning_rate": 5.193421906666843e-05,
      "loss": 1.0001,
      "step": 736000
    },
    {
      "epoch": 9.69168212861711,
      "grad_norm": 2.962740898132324,
      "learning_rate": 5.1901103413561524e-05,
      "loss": 1.0059,
      "step": 736500
    },
    {
      "epoch": 9.698261681997026,
      "grad_norm": 2.380990743637085,
      "learning_rate": 5.1867987760454616e-05,
      "loss": 0.9959,
      "step": 737000
    },
    {
      "epoch": 9.704841235376943,
      "grad_norm": 3.113856077194214,
      "learning_rate": 5.18348721073477e-05,
      "loss": 1.0053,
      "step": 737500
    },
    {
      "epoch": 9.71142078875686,
      "grad_norm": 2.31178879737854,
      "learning_rate": 5.1801756454240794e-05,
      "loss": 1.0017,
      "step": 738000
    },
    {
      "epoch": 9.718000342136776,
      "grad_norm": 2.4143640995025635,
      "learning_rate": 5.1768640801133886e-05,
      "loss": 0.9975,
      "step": 738500
    },
    {
      "epoch": 9.724579895516692,
      "grad_norm": 2.3216147422790527,
      "learning_rate": 5.1735591379333184e-05,
      "loss": 0.992,
      "step": 739000
    },
    {
      "epoch": 9.731159448896609,
      "grad_norm": 2.3239922523498535,
      "learning_rate": 5.1702475726226276e-05,
      "loss": 1.0061,
      "step": 739500
    },
    {
      "epoch": 9.737739002276525,
      "grad_norm": 2.5815324783325195,
      "learning_rate": 5.166936007311937e-05,
      "loss": 1.0025,
      "step": 740000
    },
    {
      "epoch": 9.744318555656442,
      "grad_norm": 2.275278091430664,
      "learning_rate": 5.163624442001246e-05,
      "loss": 0.997,
      "step": 740500
    },
    {
      "epoch": 9.750898109036358,
      "grad_norm": 2.5368878841400146,
      "learning_rate": 5.160312876690554e-05,
      "loss": 1.0016,
      "step": 741000
    },
    {
      "epoch": 9.757477662416274,
      "grad_norm": 2.3725292682647705,
      "learning_rate": 5.157001311379863e-05,
      "loss": 1.0015,
      "step": 741500
    },
    {
      "epoch": 9.764057215796193,
      "grad_norm": 2.4756901264190674,
      "learning_rate": 5.153689746069172e-05,
      "loss": 1.0037,
      "step": 742000
    },
    {
      "epoch": 9.77063676917611,
      "grad_norm": 2.6736655235290527,
      "learning_rate": 5.1503781807584815e-05,
      "loss": 0.9938,
      "step": 742500
    },
    {
      "epoch": 9.777216322556026,
      "grad_norm": 2.704430341720581,
      "learning_rate": 5.147073238578411e-05,
      "loss": 0.9872,
      "step": 743000
    },
    {
      "epoch": 9.783795875935942,
      "grad_norm": 2.668353319168091,
      "learning_rate": 5.1437616732677205e-05,
      "loss": 0.9945,
      "step": 743500
    },
    {
      "epoch": 9.790375429315858,
      "grad_norm": 2.489607095718384,
      "learning_rate": 5.14045673108765e-05,
      "loss": 0.9994,
      "step": 744000
    },
    {
      "epoch": 9.796954982695775,
      "grad_norm": 2.4930145740509033,
      "learning_rate": 5.1371451657769595e-05,
      "loss": 1.003,
      "step": 744500
    },
    {
      "epoch": 9.803534536075691,
      "grad_norm": 2.442681312561035,
      "learning_rate": 5.133833600466269e-05,
      "loss": 1.0028,
      "step": 745000
    },
    {
      "epoch": 9.810114089455608,
      "grad_norm": 2.329023838043213,
      "learning_rate": 5.130522035155577e-05,
      "loss": 0.9961,
      "step": 745500
    },
    {
      "epoch": 9.816693642835524,
      "grad_norm": 2.427381753921509,
      "learning_rate": 5.1272104698448865e-05,
      "loss": 0.9929,
      "step": 746000
    },
    {
      "epoch": 9.82327319621544,
      "grad_norm": 2.5633628368377686,
      "learning_rate": 5.123898904534196e-05,
      "loss": 0.9979,
      "step": 746500
    },
    {
      "epoch": 9.829852749595357,
      "grad_norm": 2.443080425262451,
      "learning_rate": 5.120587339223505e-05,
      "loss": 0.996,
      "step": 747000
    },
    {
      "epoch": 9.836432302975274,
      "grad_norm": 2.3495988845825195,
      "learning_rate": 5.117282397043435e-05,
      "loss": 0.9986,
      "step": 747500
    },
    {
      "epoch": 9.84301185635519,
      "grad_norm": 2.3023977279663086,
      "learning_rate": 5.113970831732744e-05,
      "loss": 1.0021,
      "step": 748000
    },
    {
      "epoch": 9.849591409735106,
      "grad_norm": 2.403987407684326,
      "learning_rate": 5.110659266422053e-05,
      "loss": 0.991,
      "step": 748500
    },
    {
      "epoch": 9.856170963115023,
      "grad_norm": 2.798985004425049,
      "learning_rate": 5.1073477011113616e-05,
      "loss": 1.0052,
      "step": 749000
    },
    {
      "epoch": 9.862750516494941,
      "grad_norm": 2.537194013595581,
      "learning_rate": 5.104036135800671e-05,
      "loss": 1.0008,
      "step": 749500
    },
    {
      "epoch": 9.869330069874858,
      "grad_norm": 2.234875440597534,
      "learning_rate": 5.10072457048998e-05,
      "loss": 0.9967,
      "step": 750000
    },
    {
      "epoch": 9.875909623254774,
      "grad_norm": 2.340471029281616,
      "learning_rate": 5.097413005179288e-05,
      "loss": 1.013,
      "step": 750500
    },
    {
      "epoch": 9.88248917663469,
      "grad_norm": 2.2297921180725098,
      "learning_rate": 5.094101439868597e-05,
      "loss": 0.9895,
      "step": 751000
    },
    {
      "epoch": 9.889068730014607,
      "grad_norm": 2.6086418628692627,
      "learning_rate": 5.090789874557906e-05,
      "loss": 0.9931,
      "step": 751500
    },
    {
      "epoch": 9.895648283394523,
      "grad_norm": 2.6045987606048584,
      "learning_rate": 5.0874783092472155e-05,
      "loss": 1.0006,
      "step": 752000
    },
    {
      "epoch": 9.90222783677444,
      "grad_norm": 2.4599313735961914,
      "learning_rate": 5.084173367067145e-05,
      "loss": 0.995,
      "step": 752500
    },
    {
      "epoch": 9.908807390154356,
      "grad_norm": 2.3697571754455566,
      "learning_rate": 5.0808618017564545e-05,
      "loss": 0.9955,
      "step": 753000
    },
    {
      "epoch": 9.915386943534273,
      "grad_norm": 2.295306444168091,
      "learning_rate": 5.077550236445764e-05,
      "loss": 0.9984,
      "step": 753500
    },
    {
      "epoch": 9.92196649691419,
      "grad_norm": 2.056053876876831,
      "learning_rate": 5.074238671135072e-05,
      "loss": 0.9847,
      "step": 754000
    },
    {
      "epoch": 9.928546050294106,
      "grad_norm": 2.3312461376190186,
      "learning_rate": 5.0709271058243815e-05,
      "loss": 0.9952,
      "step": 754500
    },
    {
      "epoch": 9.935125603674022,
      "grad_norm": 2.458573341369629,
      "learning_rate": 5.067615540513691e-05,
      "loss": 0.993,
      "step": 755000
    },
    {
      "epoch": 9.941705157053939,
      "grad_norm": 2.0865023136138916,
      "learning_rate": 5.064303975203e-05,
      "loss": 0.9943,
      "step": 755500
    },
    {
      "epoch": 9.948284710433855,
      "grad_norm": 2.5671510696411133,
      "learning_rate": 5.060992409892308e-05,
      "loss": 0.9902,
      "step": 756000
    },
    {
      "epoch": 9.954864263813771,
      "grad_norm": 2.0934972763061523,
      "learning_rate": 5.057680844581617e-05,
      "loss": 0.9989,
      "step": 756500
    },
    {
      "epoch": 9.96144381719369,
      "grad_norm": 2.6107261180877686,
      "learning_rate": 5.054369279270926e-05,
      "loss": 0.999,
      "step": 757000
    },
    {
      "epoch": 9.968023370573606,
      "grad_norm": 2.321563482284546,
      "learning_rate": 5.051064337090856e-05,
      "loss": 0.9966,
      "step": 757500
    },
    {
      "epoch": 9.974602923953523,
      "grad_norm": 2.6480202674865723,
      "learning_rate": 5.047752771780165e-05,
      "loss": 1.0012,
      "step": 758000
    },
    {
      "epoch": 9.981182477333439,
      "grad_norm": 2.2465851306915283,
      "learning_rate": 5.0444412064694744e-05,
      "loss": 1.0001,
      "step": 758500
    },
    {
      "epoch": 9.987762030713355,
      "grad_norm": 2.859222173690796,
      "learning_rate": 5.041129641158783e-05,
      "loss": 0.9994,
      "step": 759000
    },
    {
      "epoch": 9.994341584093272,
      "grad_norm": 2.513317108154297,
      "learning_rate": 5.037818075848092e-05,
      "loss": 0.9889,
      "step": 759500
    },
    {
      "epoch": 10.0,
      "eval_loss": 0.9525216221809387,
      "eval_runtime": 49.4818,
      "eval_samples_per_second": 2020.944,
      "eval_steps_per_second": 15.804,
      "step": 759930
    },
    {
      "epoch": 10.000921137473188,
      "grad_norm": 2.898550510406494,
      "learning_rate": 5.0345131336680226e-05,
      "loss": 0.9981,
      "step": 760000
    },
    {
      "epoch": 10.007500690853105,
      "grad_norm": 2.407926559448242,
      "learning_rate": 5.031201568357331e-05,
      "loss": 0.9909,
      "step": 760500
    },
    {
      "epoch": 10.014080244233021,
      "grad_norm": 2.5236923694610596,
      "learning_rate": 5.0278900030466404e-05,
      "loss": 0.981,
      "step": 761000
    },
    {
      "epoch": 10.020659797612938,
      "grad_norm": 2.290113925933838,
      "learning_rate": 5.0245784377359496e-05,
      "loss": 0.9868,
      "step": 761500
    },
    {
      "epoch": 10.027239350992854,
      "grad_norm": 2.3969709873199463,
      "learning_rate": 5.021266872425259e-05,
      "loss": 0.9924,
      "step": 762000
    },
    {
      "epoch": 10.03381890437277,
      "grad_norm": 2.4402008056640625,
      "learning_rate": 5.0179553071145666e-05,
      "loss": 0.9851,
      "step": 762500
    },
    {
      "epoch": 10.040398457752687,
      "grad_norm": 2.4106216430664062,
      "learning_rate": 5.014643741803876e-05,
      "loss": 0.9824,
      "step": 763000
    },
    {
      "epoch": 10.046978011132603,
      "grad_norm": 2.5941996574401855,
      "learning_rate": 5.0113387996238056e-05,
      "loss": 0.983,
      "step": 763500
    },
    {
      "epoch": 10.053557564512522,
      "grad_norm": 2.286069869995117,
      "learning_rate": 5.008033857443737e-05,
      "loss": 0.9822,
      "step": 764000
    },
    {
      "epoch": 10.060137117892438,
      "grad_norm": 2.890443801879883,
      "learning_rate": 5.004722292133046e-05,
      "loss": 0.9879,
      "step": 764500
    },
    {
      "epoch": 10.066716671272355,
      "grad_norm": 2.5676498413085938,
      "learning_rate": 5.0014107268223546e-05,
      "loss": 0.9828,
      "step": 765000
    },
    {
      "epoch": 10.073296224652271,
      "grad_norm": 2.39786434173584,
      "learning_rate": 4.998099161511664e-05,
      "loss": 0.9918,
      "step": 765500
    },
    {
      "epoch": 10.079875778032187,
      "grad_norm": 2.5086417198181152,
      "learning_rate": 4.994787596200973e-05,
      "loss": 0.9945,
      "step": 766000
    },
    {
      "epoch": 10.086455331412104,
      "grad_norm": 2.677349805831909,
      "learning_rate": 4.9914760308902815e-05,
      "loss": 0.9832,
      "step": 766500
    },
    {
      "epoch": 10.09303488479202,
      "grad_norm": 2.806389570236206,
      "learning_rate": 4.988164465579591e-05,
      "loss": 0.9889,
      "step": 767000
    },
    {
      "epoch": 10.099614438171937,
      "grad_norm": 2.8172130584716797,
      "learning_rate": 4.984852900268899e-05,
      "loss": 0.9866,
      "step": 767500
    },
    {
      "epoch": 10.106193991551853,
      "grad_norm": 2.3985612392425537,
      "learning_rate": 4.9815413349582084e-05,
      "loss": 0.9811,
      "step": 768000
    },
    {
      "epoch": 10.11277354493177,
      "grad_norm": 2.2312583923339844,
      "learning_rate": 4.978229769647517e-05,
      "loss": 0.9864,
      "step": 768500
    },
    {
      "epoch": 10.119353098311686,
      "grad_norm": 3.5855956077575684,
      "learning_rate": 4.974918204336826e-05,
      "loss": 0.9853,
      "step": 769000
    },
    {
      "epoch": 10.125932651691603,
      "grad_norm": 2.777825355529785,
      "learning_rate": 4.971613262156756e-05,
      "loss": 0.9929,
      "step": 769500
    },
    {
      "epoch": 10.132512205071519,
      "grad_norm": 2.312972068786621,
      "learning_rate": 4.968301696846065e-05,
      "loss": 0.9817,
      "step": 770000
    },
    {
      "epoch": 10.139091758451436,
      "grad_norm": 2.5331454277038574,
      "learning_rate": 4.9649901315353744e-05,
      "loss": 0.9891,
      "step": 770500
    },
    {
      "epoch": 10.145671311831354,
      "grad_norm": 2.686095714569092,
      "learning_rate": 4.961685189355305e-05,
      "loss": 0.9849,
      "step": 771000
    },
    {
      "epoch": 10.15225086521127,
      "grad_norm": 2.0885305404663086,
      "learning_rate": 4.958373624044614e-05,
      "loss": 0.9924,
      "step": 771500
    },
    {
      "epoch": 10.158830418591187,
      "grad_norm": 2.6747570037841797,
      "learning_rate": 4.9550620587339226e-05,
      "loss": 0.9934,
      "step": 772000
    },
    {
      "epoch": 10.165409971971103,
      "grad_norm": 2.684269666671753,
      "learning_rate": 4.951750493423232e-05,
      "loss": 0.9889,
      "step": 772500
    },
    {
      "epoch": 10.17198952535102,
      "grad_norm": 2.5675225257873535,
      "learning_rate": 4.9484389281125404e-05,
      "loss": 0.9794,
      "step": 773000
    },
    {
      "epoch": 10.178569078730936,
      "grad_norm": 2.281873941421509,
      "learning_rate": 4.9451273628018496e-05,
      "loss": 0.9932,
      "step": 773500
    },
    {
      "epoch": 10.185148632110852,
      "grad_norm": 2.940592050552368,
      "learning_rate": 4.941815797491158e-05,
      "loss": 0.9883,
      "step": 774000
    },
    {
      "epoch": 10.191728185490769,
      "grad_norm": 2.584343433380127,
      "learning_rate": 4.938504232180467e-05,
      "loss": 0.985,
      "step": 774500
    },
    {
      "epoch": 10.198307738870685,
      "grad_norm": 2.4274230003356934,
      "learning_rate": 4.935199290000397e-05,
      "loss": 0.976,
      "step": 775000
    },
    {
      "epoch": 10.204887292250602,
      "grad_norm": 2.4000918865203857,
      "learning_rate": 4.931887724689706e-05,
      "loss": 0.9887,
      "step": 775500
    },
    {
      "epoch": 10.211466845630518,
      "grad_norm": 2.6482064723968506,
      "learning_rate": 4.928582782509637e-05,
      "loss": 0.9839,
      "step": 776000
    },
    {
      "epoch": 10.218046399010435,
      "grad_norm": 2.3926737308502197,
      "learning_rate": 4.9252712171989454e-05,
      "loss": 0.9934,
      "step": 776500
    },
    {
      "epoch": 10.224625952390351,
      "grad_norm": 2.2718942165374756,
      "learning_rate": 4.921959651888255e-05,
      "loss": 0.9868,
      "step": 777000
    },
    {
      "epoch": 10.231205505770268,
      "grad_norm": 2.5193214416503906,
      "learning_rate": 4.918648086577564e-05,
      "loss": 0.9824,
      "step": 777500
    },
    {
      "epoch": 10.237785059150184,
      "grad_norm": 2.680084705352783,
      "learning_rate": 4.915336521266873e-05,
      "loss": 0.9792,
      "step": 778000
    },
    {
      "epoch": 10.244364612530102,
      "grad_norm": 2.451486349105835,
      "learning_rate": 4.9120249559561815e-05,
      "loss": 0.9813,
      "step": 778500
    },
    {
      "epoch": 10.250944165910019,
      "grad_norm": 2.7559351921081543,
      "learning_rate": 4.908713390645491e-05,
      "loss": 0.9834,
      "step": 779000
    },
    {
      "epoch": 10.257523719289935,
      "grad_norm": 2.401939868927002,
      "learning_rate": 4.905401825334799e-05,
      "loss": 0.9861,
      "step": 779500
    },
    {
      "epoch": 10.264103272669852,
      "grad_norm": 2.484374761581421,
      "learning_rate": 4.9020902600241085e-05,
      "loss": 0.986,
      "step": 780000
    },
    {
      "epoch": 10.270682826049768,
      "grad_norm": 2.039027690887451,
      "learning_rate": 4.898785317844038e-05,
      "loss": 0.9824,
      "step": 780500
    },
    {
      "epoch": 10.277262379429684,
      "grad_norm": 2.1608269214630127,
      "learning_rate": 4.8954737525333475e-05,
      "loss": 0.978,
      "step": 781000
    },
    {
      "epoch": 10.283841932809601,
      "grad_norm": 2.3909130096435547,
      "learning_rate": 4.892162187222657e-05,
      "loss": 0.9904,
      "step": 781500
    },
    {
      "epoch": 10.290421486189517,
      "grad_norm": 2.60012149810791,
      "learning_rate": 4.888850621911966e-05,
      "loss": 0.9868,
      "step": 782000
    },
    {
      "epoch": 10.297001039569434,
      "grad_norm": 2.3618838787078857,
      "learning_rate": 4.885545679731896e-05,
      "loss": 0.9889,
      "step": 782500
    },
    {
      "epoch": 10.30358059294935,
      "grad_norm": 2.7092435359954834,
      "learning_rate": 4.882234114421205e-05,
      "loss": 0.9902,
      "step": 783000
    },
    {
      "epoch": 10.310160146329267,
      "grad_norm": 2.500535488128662,
      "learning_rate": 4.8789291722411354e-05,
      "loss": 0.9833,
      "step": 783500
    },
    {
      "epoch": 10.316739699709183,
      "grad_norm": 2.54061222076416,
      "learning_rate": 4.8756176069304446e-05,
      "loss": 0.9842,
      "step": 784000
    },
    {
      "epoch": 10.3233192530891,
      "grad_norm": 2.5620055198669434,
      "learning_rate": 4.872306041619753e-05,
      "loss": 0.9842,
      "step": 784500
    },
    {
      "epoch": 10.329898806469016,
      "grad_norm": 2.620837688446045,
      "learning_rate": 4.868994476309062e-05,
      "loss": 0.9882,
      "step": 785000
    },
    {
      "epoch": 10.336478359848934,
      "grad_norm": 2.3303675651550293,
      "learning_rate": 4.865682910998371e-05,
      "loss": 0.9837,
      "step": 785500
    },
    {
      "epoch": 10.34305791322885,
      "grad_norm": 2.4005377292633057,
      "learning_rate": 4.86237134568768e-05,
      "loss": 0.9816,
      "step": 786000
    },
    {
      "epoch": 10.349637466608767,
      "grad_norm": 2.726207971572876,
      "learning_rate": 4.8590597803769886e-05,
      "loss": 0.983,
      "step": 786500
    },
    {
      "epoch": 10.356217019988684,
      "grad_norm": 2.3396553993225098,
      "learning_rate": 4.855748215066298e-05,
      "loss": 0.984,
      "step": 787000
    },
    {
      "epoch": 10.3627965733686,
      "grad_norm": 2.3163866996765137,
      "learning_rate": 4.852436649755607e-05,
      "loss": 0.9857,
      "step": 787500
    },
    {
      "epoch": 10.369376126748516,
      "grad_norm": 2.6324427127838135,
      "learning_rate": 4.8491250844449156e-05,
      "loss": 0.9842,
      "step": 788000
    },
    {
      "epoch": 10.375955680128433,
      "grad_norm": 2.458669900894165,
      "learning_rate": 4.845813519134225e-05,
      "loss": 0.9843,
      "step": 788500
    },
    {
      "epoch": 10.38253523350835,
      "grad_norm": 2.2589080333709717,
      "learning_rate": 4.842501953823533e-05,
      "loss": 0.9855,
      "step": 789000
    },
    {
      "epoch": 10.389114786888266,
      "grad_norm": 2.660067081451416,
      "learning_rate": 4.839197011643464e-05,
      "loss": 0.9826,
      "step": 789500
    },
    {
      "epoch": 10.395694340268182,
      "grad_norm": 2.6722676753997803,
      "learning_rate": 4.835885446332773e-05,
      "loss": 0.9792,
      "step": 790000
    },
    {
      "epoch": 10.402273893648099,
      "grad_norm": 2.6681644916534424,
      "learning_rate": 4.8325738810220815e-05,
      "loss": 0.9802,
      "step": 790500
    },
    {
      "epoch": 10.408853447028015,
      "grad_norm": 2.4631078243255615,
      "learning_rate": 4.829262315711391e-05,
      "loss": 0.9773,
      "step": 791000
    },
    {
      "epoch": 10.415433000407932,
      "grad_norm": 2.4630541801452637,
      "learning_rate": 4.825950750400699e-05,
      "loss": 0.9847,
      "step": 791500
    },
    {
      "epoch": 10.422012553787848,
      "grad_norm": 2.2567756175994873,
      "learning_rate": 4.82264580822063e-05,
      "loss": 0.9874,
      "step": 792000
    },
    {
      "epoch": 10.428592107167766,
      "grad_norm": 2.329645872116089,
      "learning_rate": 4.819334242909939e-05,
      "loss": 0.9886,
      "step": 792500
    },
    {
      "epoch": 10.435171660547683,
      "grad_norm": 2.4637391567230225,
      "learning_rate": 4.816029300729869e-05,
      "loss": 0.9833,
      "step": 793000
    },
    {
      "epoch": 10.4417512139276,
      "grad_norm": 2.2361395359039307,
      "learning_rate": 4.812717735419178e-05,
      "loss": 0.9782,
      "step": 793500
    },
    {
      "epoch": 10.448330767307516,
      "grad_norm": 2.7022833824157715,
      "learning_rate": 4.809406170108487e-05,
      "loss": 0.9823,
      "step": 794000
    },
    {
      "epoch": 10.454910320687432,
      "grad_norm": 2.480915069580078,
      "learning_rate": 4.8060946047977964e-05,
      "loss": 0.9759,
      "step": 794500
    },
    {
      "epoch": 10.461489874067349,
      "grad_norm": 2.7854037284851074,
      "learning_rate": 4.802783039487105e-05,
      "loss": 0.9823,
      "step": 795000
    },
    {
      "epoch": 10.468069427447265,
      "grad_norm": 2.4441335201263428,
      "learning_rate": 4.7994780973070354e-05,
      "loss": 0.9877,
      "step": 795500
    },
    {
      "epoch": 10.474648980827181,
      "grad_norm": 3.0623013973236084,
      "learning_rate": 4.7961665319963446e-05,
      "loss": 0.9774,
      "step": 796000
    },
    {
      "epoch": 10.481228534207098,
      "grad_norm": 2.2532808780670166,
      "learning_rate": 4.7928615898162744e-05,
      "loss": 0.9879,
      "step": 796500
    },
    {
      "epoch": 10.487808087587014,
      "grad_norm": 2.2261199951171875,
      "learning_rate": 4.7895500245055836e-05,
      "loss": 0.9746,
      "step": 797000
    },
    {
      "epoch": 10.49438764096693,
      "grad_norm": 2.517273426055908,
      "learning_rate": 4.786238459194892e-05,
      "loss": 0.9878,
      "step": 797500
    },
    {
      "epoch": 10.500967194346847,
      "grad_norm": 2.468841075897217,
      "learning_rate": 4.7829268938842014e-05,
      "loss": 0.9786,
      "step": 798000
    },
    {
      "epoch": 10.507546747726764,
      "grad_norm": 2.24019193649292,
      "learning_rate": 4.77961532857351e-05,
      "loss": 0.9896,
      "step": 798500
    },
    {
      "epoch": 10.51412630110668,
      "grad_norm": 2.265193223953247,
      "learning_rate": 4.7763103863934404e-05,
      "loss": 0.9753,
      "step": 799000
    },
    {
      "epoch": 10.520705854486597,
      "grad_norm": 2.5627481937408447,
      "learning_rate": 4.7729988210827496e-05,
      "loss": 0.9796,
      "step": 799500
    },
    {
      "epoch": 10.527285407866515,
      "grad_norm": 2.360138416290283,
      "learning_rate": 4.769687255772058e-05,
      "loss": 0.9894,
      "step": 800000
    },
    {
      "epoch": 10.533864961246431,
      "grad_norm": 2.2837674617767334,
      "learning_rate": 4.766375690461368e-05,
      "loss": 0.9802,
      "step": 800500
    },
    {
      "epoch": 10.540444514626348,
      "grad_norm": 2.3754656314849854,
      "learning_rate": 4.7630641251506765e-05,
      "loss": 0.98,
      "step": 801000
    },
    {
      "epoch": 10.547024068006264,
      "grad_norm": 2.5204129219055176,
      "learning_rate": 4.759752559839986e-05,
      "loss": 0.9759,
      "step": 801500
    },
    {
      "epoch": 10.55360362138618,
      "grad_norm": 2.7886977195739746,
      "learning_rate": 4.756440994529294e-05,
      "loss": 0.9779,
      "step": 802000
    },
    {
      "epoch": 10.560183174766097,
      "grad_norm": 2.513833999633789,
      "learning_rate": 4.7531294292186035e-05,
      "loss": 0.9862,
      "step": 802500
    },
    {
      "epoch": 10.566762728146013,
      "grad_norm": 2.3916983604431152,
      "learning_rate": 4.749817863907912e-05,
      "loss": 0.9798,
      "step": 803000
    },
    {
      "epoch": 10.57334228152593,
      "grad_norm": 2.3602359294891357,
      "learning_rate": 4.7465129217278425e-05,
      "loss": 0.9685,
      "step": 803500
    },
    {
      "epoch": 10.579921834905846,
      "grad_norm": 2.4090263843536377,
      "learning_rate": 4.743201356417151e-05,
      "loss": 0.9857,
      "step": 804000
    },
    {
      "epoch": 10.586501388285763,
      "grad_norm": 2.146928071975708,
      "learning_rate": 4.73988979110646e-05,
      "loss": 0.9763,
      "step": 804500
    },
    {
      "epoch": 10.59308094166568,
      "grad_norm": 2.8193535804748535,
      "learning_rate": 4.7365782257957694e-05,
      "loss": 0.9787,
      "step": 805000
    },
    {
      "epoch": 10.599660495045596,
      "grad_norm": 2.830648183822632,
      "learning_rate": 4.7332666604850787e-05,
      "loss": 0.9724,
      "step": 805500
    },
    {
      "epoch": 10.606240048425512,
      "grad_norm": 2.533747673034668,
      "learning_rate": 4.729955095174387e-05,
      "loss": 0.9766,
      "step": 806000
    },
    {
      "epoch": 10.61281960180543,
      "grad_norm": 2.3696253299713135,
      "learning_rate": 4.7266435298636964e-05,
      "loss": 0.9793,
      "step": 806500
    },
    {
      "epoch": 10.619399155185347,
      "grad_norm": 2.598986864089966,
      "learning_rate": 4.723338587683627e-05,
      "loss": 0.9839,
      "step": 807000
    },
    {
      "epoch": 10.625978708565263,
      "grad_norm": 2.364098072052002,
      "learning_rate": 4.7200270223729354e-05,
      "loss": 0.9815,
      "step": 807500
    },
    {
      "epoch": 10.63255826194518,
      "grad_norm": 3.095686435699463,
      "learning_rate": 4.7167154570622446e-05,
      "loss": 0.992,
      "step": 808000
    },
    {
      "epoch": 10.639137815325096,
      "grad_norm": 2.2326924800872803,
      "learning_rate": 4.713403891751553e-05,
      "loss": 0.9794,
      "step": 808500
    },
    {
      "epoch": 10.645717368705013,
      "grad_norm": 2.7340102195739746,
      "learning_rate": 4.7100989495714836e-05,
      "loss": 0.9772,
      "step": 809000
    },
    {
      "epoch": 10.652296922084929,
      "grad_norm": 2.134033441543579,
      "learning_rate": 4.706787384260792e-05,
      "loss": 0.9865,
      "step": 809500
    },
    {
      "epoch": 10.658876475464846,
      "grad_norm": 2.295295238494873,
      "learning_rate": 4.7034758189501014e-05,
      "loss": 0.9833,
      "step": 810000
    },
    {
      "epoch": 10.665456028844762,
      "grad_norm": 2.3134875297546387,
      "learning_rate": 4.7001642536394106e-05,
      "loss": 0.9751,
      "step": 810500
    },
    {
      "epoch": 10.672035582224678,
      "grad_norm": 2.5994668006896973,
      "learning_rate": 4.69685268832872e-05,
      "loss": 0.9864,
      "step": 811000
    },
    {
      "epoch": 10.678615135604595,
      "grad_norm": 2.515199899673462,
      "learning_rate": 4.6935477461486496e-05,
      "loss": 0.9771,
      "step": 811500
    },
    {
      "epoch": 10.685194688984511,
      "grad_norm": 2.4897685050964355,
      "learning_rate": 4.690236180837959e-05,
      "loss": 0.9754,
      "step": 812000
    },
    {
      "epoch": 10.691774242364428,
      "grad_norm": 2.586759090423584,
      "learning_rate": 4.686924615527268e-05,
      "loss": 0.9765,
      "step": 812500
    },
    {
      "epoch": 10.698353795744344,
      "grad_norm": 2.5678155422210693,
      "learning_rate": 4.6836130502165765e-05,
      "loss": 0.9794,
      "step": 813000
    },
    {
      "epoch": 10.70493334912426,
      "grad_norm": 2.682298421859741,
      "learning_rate": 4.680301484905886e-05,
      "loss": 0.982,
      "step": 813500
    },
    {
      "epoch": 10.711512902504179,
      "grad_norm": 2.288167715072632,
      "learning_rate": 4.676996542725816e-05,
      "loss": 0.9805,
      "step": 814000
    },
    {
      "epoch": 10.718092455884095,
      "grad_norm": 2.6375906467437744,
      "learning_rate": 4.673691600545746e-05,
      "loss": 0.9791,
      "step": 814500
    },
    {
      "epoch": 10.724672009264012,
      "grad_norm": 2.932880401611328,
      "learning_rate": 4.670380035235055e-05,
      "loss": 0.9843,
      "step": 815000
    },
    {
      "epoch": 10.731251562643928,
      "grad_norm": 2.523190498352051,
      "learning_rate": 4.667068469924364e-05,
      "loss": 0.9808,
      "step": 815500
    },
    {
      "epoch": 10.737831116023845,
      "grad_norm": 2.59161114692688,
      "learning_rate": 4.663756904613673e-05,
      "loss": 0.9762,
      "step": 816000
    },
    {
      "epoch": 10.744410669403761,
      "grad_norm": 2.639683723449707,
      "learning_rate": 4.6604519624336035e-05,
      "loss": 0.9795,
      "step": 816500
    },
    {
      "epoch": 10.750990222783678,
      "grad_norm": 2.411996841430664,
      "learning_rate": 4.657147020253534e-05,
      "loss": 0.9751,
      "step": 817000
    },
    {
      "epoch": 10.757569776163594,
      "grad_norm": 2.4638476371765137,
      "learning_rate": 4.6538354549428425e-05,
      "loss": 0.9771,
      "step": 817500
    },
    {
      "epoch": 10.76414932954351,
      "grad_norm": 2.491480827331543,
      "learning_rate": 4.650523889632152e-05,
      "loss": 0.9763,
      "step": 818000
    },
    {
      "epoch": 10.770728882923427,
      "grad_norm": 2.5934019088745117,
      "learning_rate": 4.64721232432146e-05,
      "loss": 0.9798,
      "step": 818500
    },
    {
      "epoch": 10.777308436303343,
      "grad_norm": 2.3425941467285156,
      "learning_rate": 4.6439007590107695e-05,
      "loss": 0.9794,
      "step": 819000
    },
    {
      "epoch": 10.78388798968326,
      "grad_norm": 2.3646135330200195,
      "learning_rate": 4.6405891937000787e-05,
      "loss": 0.9744,
      "step": 819500
    },
    {
      "epoch": 10.790467543063176,
      "grad_norm": 2.5843026638031006,
      "learning_rate": 4.637277628389387e-05,
      "loss": 0.9684,
      "step": 820000
    },
    {
      "epoch": 10.797047096443093,
      "grad_norm": 2.2044870853424072,
      "learning_rate": 4.6339660630786964e-05,
      "loss": 0.9817,
      "step": 820500
    },
    {
      "epoch": 10.80362664982301,
      "grad_norm": 2.2737948894500732,
      "learning_rate": 4.630654497768005e-05,
      "loss": 0.9724,
      "step": 821000
    },
    {
      "epoch": 10.810206203202927,
      "grad_norm": 2.6094350814819336,
      "learning_rate": 4.6273495555879354e-05,
      "loss": 0.9792,
      "step": 821500
    },
    {
      "epoch": 10.816785756582844,
      "grad_norm": 2.476140022277832,
      "learning_rate": 4.6240379902772446e-05,
      "loss": 0.9774,
      "step": 822000
    },
    {
      "epoch": 10.82336530996276,
      "grad_norm": 2.4611616134643555,
      "learning_rate": 4.620726424966553e-05,
      "loss": 0.976,
      "step": 822500
    },
    {
      "epoch": 10.829944863342677,
      "grad_norm": 2.2498273849487305,
      "learning_rate": 4.6174148596558624e-05,
      "loss": 0.972,
      "step": 823000
    },
    {
      "epoch": 10.836524416722593,
      "grad_norm": 2.1751651763916016,
      "learning_rate": 4.614103294345171e-05,
      "loss": 0.9799,
      "step": 823500
    },
    {
      "epoch": 10.84310397010251,
      "grad_norm": 2.7735979557037354,
      "learning_rate": 4.61079172903448e-05,
      "loss": 0.9766,
      "step": 824000
    },
    {
      "epoch": 10.849683523482426,
      "grad_norm": 2.70236873626709,
      "learning_rate": 4.607480163723789e-05,
      "loss": 0.9715,
      "step": 824500
    },
    {
      "epoch": 10.856263076862342,
      "grad_norm": 2.3606200218200684,
      "learning_rate": 4.60417522154372e-05,
      "loss": 0.9691,
      "step": 825000
    },
    {
      "epoch": 10.862842630242259,
      "grad_norm": 2.298070192337036,
      "learning_rate": 4.600863656233028e-05,
      "loss": 0.976,
      "step": 825500
    },
    {
      "epoch": 10.869422183622175,
      "grad_norm": 2.526909351348877,
      "learning_rate": 4.5975520909223375e-05,
      "loss": 0.9731,
      "step": 826000
    },
    {
      "epoch": 10.876001737002092,
      "grad_norm": 2.6407549381256104,
      "learning_rate": 4.594240525611646e-05,
      "loss": 0.9762,
      "step": 826500
    },
    {
      "epoch": 10.882581290382008,
      "grad_norm": 2.216975688934326,
      "learning_rate": 4.590928960300955e-05,
      "loss": 0.9726,
      "step": 827000
    },
    {
      "epoch": 10.889160843761925,
      "grad_norm": 2.390777826309204,
      "learning_rate": 4.587630641251507e-05,
      "loss": 0.9807,
      "step": 827500
    },
    {
      "epoch": 10.895740397141843,
      "grad_norm": 2.525742530822754,
      "learning_rate": 4.584319075940816e-05,
      "loss": 0.9712,
      "step": 828000
    },
    {
      "epoch": 10.90231995052176,
      "grad_norm": 2.8025565147399902,
      "learning_rate": 4.581007510630125e-05,
      "loss": 0.976,
      "step": 828500
    },
    {
      "epoch": 10.908899503901676,
      "grad_norm": 2.401888847351074,
      "learning_rate": 4.577695945319434e-05,
      "loss": 0.9665,
      "step": 829000
    },
    {
      "epoch": 10.915479057281592,
      "grad_norm": 2.175368309020996,
      "learning_rate": 4.5743843800087425e-05,
      "loss": 0.9654,
      "step": 829500
    },
    {
      "epoch": 10.922058610661509,
      "grad_norm": 2.2589190006256104,
      "learning_rate": 4.571072814698052e-05,
      "loss": 0.9781,
      "step": 830000
    },
    {
      "epoch": 10.928638164041425,
      "grad_norm": 2.615504264831543,
      "learning_rate": 4.567761249387361e-05,
      "loss": 0.9774,
      "step": 830500
    },
    {
      "epoch": 10.935217717421342,
      "grad_norm": 2.4324285984039307,
      "learning_rate": 4.564456307207291e-05,
      "loss": 0.9713,
      "step": 831000
    },
    {
      "epoch": 10.941797270801258,
      "grad_norm": 2.488187074661255,
      "learning_rate": 4.5611447418966e-05,
      "loss": 0.973,
      "step": 831500
    },
    {
      "epoch": 10.948376824181175,
      "grad_norm": 2.351598024368286,
      "learning_rate": 4.557833176585909e-05,
      "loss": 0.9701,
      "step": 832000
    },
    {
      "epoch": 10.954956377561091,
      "grad_norm": 2.218923330307007,
      "learning_rate": 4.554521611275218e-05,
      "loss": 0.966,
      "step": 832500
    },
    {
      "epoch": 10.961535930941007,
      "grad_norm": 2.549154043197632,
      "learning_rate": 4.551210045964527e-05,
      "loss": 0.9789,
      "step": 833000
    },
    {
      "epoch": 10.968115484320924,
      "grad_norm": 2.2942450046539307,
      "learning_rate": 4.5478984806538354e-05,
      "loss": 0.9791,
      "step": 833500
    },
    {
      "epoch": 10.97469503770084,
      "grad_norm": 2.5488781929016113,
      "learning_rate": 4.5445869153431446e-05,
      "loss": 0.9777,
      "step": 834000
    },
    {
      "epoch": 10.981274591080757,
      "grad_norm": 2.6308984756469727,
      "learning_rate": 4.541275350032453e-05,
      "loss": 0.9747,
      "step": 834500
    },
    {
      "epoch": 10.987854144460673,
      "grad_norm": 2.583834409713745,
      "learning_rate": 4.5379637847217624e-05,
      "loss": 0.9713,
      "step": 835000
    },
    {
      "epoch": 10.994433697840591,
      "grad_norm": 2.623495578765869,
      "learning_rate": 4.5346522194110716e-05,
      "loss": 0.9722,
      "step": 835500
    },
    {
      "epoch": 11.0,
      "eval_loss": 0.9303607940673828,
      "eval_runtime": 49.6236,
      "eval_samples_per_second": 2015.17,
      "eval_steps_per_second": 15.759,
      "step": 835923
    },
    {
      "epoch": 11.001013251220508,
      "grad_norm": 2.4778926372528076,
      "learning_rate": 4.531340654100381e-05,
      "loss": 0.9676,
      "step": 836000
    },
    {
      "epoch": 11.007592804600424,
      "grad_norm": 2.2692413330078125,
      "learning_rate": 4.528029088789689e-05,
      "loss": 0.9627,
      "step": 836500
    },
    {
      "epoch": 11.01417235798034,
      "grad_norm": 2.934178590774536,
      "learning_rate": 4.5247175234789985e-05,
      "loss": 0.9704,
      "step": 837000
    },
    {
      "epoch": 11.020751911360257,
      "grad_norm": 2.497109889984131,
      "learning_rate": 4.521412581298929e-05,
      "loss": 0.9625,
      "step": 837500
    },
    {
      "epoch": 11.027331464740174,
      "grad_norm": 2.7897510528564453,
      "learning_rate": 4.518107639118859e-05,
      "loss": 0.9607,
      "step": 838000
    },
    {
      "epoch": 11.03391101812009,
      "grad_norm": 2.6356070041656494,
      "learning_rate": 4.514796073808168e-05,
      "loss": 0.9632,
      "step": 838500
    },
    {
      "epoch": 11.040490571500007,
      "grad_norm": 2.649120807647705,
      "learning_rate": 4.5114845084974766e-05,
      "loss": 0.9683,
      "step": 839000
    },
    {
      "epoch": 11.047070124879923,
      "grad_norm": 2.3797571659088135,
      "learning_rate": 4.508172943186786e-05,
      "loss": 0.9627,
      "step": 839500
    },
    {
      "epoch": 11.05364967825984,
      "grad_norm": 2.567385673522949,
      "learning_rate": 4.504861377876094e-05,
      "loss": 0.9687,
      "step": 840000
    },
    {
      "epoch": 11.060229231639756,
      "grad_norm": 2.488161087036133,
      "learning_rate": 4.5015498125654035e-05,
      "loss": 0.9666,
      "step": 840500
    },
    {
      "epoch": 11.066808785019672,
      "grad_norm": 2.401324510574341,
      "learning_rate": 4.498238247254713e-05,
      "loss": 0.9619,
      "step": 841000
    },
    {
      "epoch": 11.073388338399589,
      "grad_norm": 2.3896374702453613,
      "learning_rate": 4.494926681944022e-05,
      "loss": 0.9637,
      "step": 841500
    },
    {
      "epoch": 11.079967891779505,
      "grad_norm": 2.6308984756469727,
      "learning_rate": 4.4916151166333305e-05,
      "loss": 0.9613,
      "step": 842000
    },
    {
      "epoch": 11.086547445159422,
      "grad_norm": 2.6675517559051514,
      "learning_rate": 4.4883035513226397e-05,
      "loss": 0.9634,
      "step": 842500
    },
    {
      "epoch": 11.09312699853934,
      "grad_norm": 2.5653774738311768,
      "learning_rate": 4.484991986011948e-05,
      "loss": 0.9753,
      "step": 843000
    },
    {
      "epoch": 11.099706551919256,
      "grad_norm": 2.5163817405700684,
      "learning_rate": 4.481687043831879e-05,
      "loss": 0.971,
      "step": 843500
    },
    {
      "epoch": 11.106286105299173,
      "grad_norm": 2.263578414916992,
      "learning_rate": 4.478375478521188e-05,
      "loss": 0.963,
      "step": 844000
    },
    {
      "epoch": 11.11286565867909,
      "grad_norm": 2.7144386768341064,
      "learning_rate": 4.4750639132104964e-05,
      "loss": 0.9663,
      "step": 844500
    },
    {
      "epoch": 11.119445212059006,
      "grad_norm": 2.5585572719573975,
      "learning_rate": 4.471752347899805e-05,
      "loss": 0.9642,
      "step": 845000
    },
    {
      "epoch": 11.126024765438922,
      "grad_norm": 2.4438962936401367,
      "learning_rate": 4.468440782589114e-05,
      "loss": 0.9651,
      "step": 845500
    },
    {
      "epoch": 11.132604318818839,
      "grad_norm": 2.2291665077209473,
      "learning_rate": 4.4651292172784234e-05,
      "loss": 0.9616,
      "step": 846000
    },
    {
      "epoch": 11.139183872198755,
      "grad_norm": 2.5703623294830322,
      "learning_rate": 4.4618176519677326e-05,
      "loss": 0.9595,
      "step": 846500
    },
    {
      "epoch": 11.145763425578672,
      "grad_norm": 2.5077242851257324,
      "learning_rate": 4.458506086657041e-05,
      "loss": 0.9573,
      "step": 847000
    },
    {
      "epoch": 11.152342978958588,
      "grad_norm": 2.3390369415283203,
      "learning_rate": 4.45519452134635e-05,
      "loss": 0.9681,
      "step": 847500
    },
    {
      "epoch": 11.158922532338504,
      "grad_norm": 2.5171425342559814,
      "learning_rate": 4.451889579166281e-05,
      "loss": 0.9636,
      "step": 848000
    },
    {
      "epoch": 11.16550208571842,
      "grad_norm": 1.8663150072097778,
      "learning_rate": 4.448584636986211e-05,
      "loss": 0.9609,
      "step": 848500
    },
    {
      "epoch": 11.172081639098337,
      "grad_norm": 2.2955167293548584,
      "learning_rate": 4.44527307167552e-05,
      "loss": 0.9727,
      "step": 849000
    },
    {
      "epoch": 11.178661192478254,
      "grad_norm": 2.3873538970947266,
      "learning_rate": 4.441961506364829e-05,
      "loss": 0.9626,
      "step": 849500
    },
    {
      "epoch": 11.185240745858172,
      "grad_norm": 2.745875597000122,
      "learning_rate": 4.4386499410541376e-05,
      "loss": 0.9678,
      "step": 850000
    },
    {
      "epoch": 11.191820299238088,
      "grad_norm": 2.5594100952148438,
      "learning_rate": 4.435338375743446e-05,
      "loss": 0.9666,
      "step": 850500
    },
    {
      "epoch": 11.198399852618005,
      "grad_norm": 2.3770923614501953,
      "learning_rate": 4.4320334335633766e-05,
      "loss": 0.9547,
      "step": 851000
    },
    {
      "epoch": 11.204979405997921,
      "grad_norm": 2.60593843460083,
      "learning_rate": 4.428721868252686e-05,
      "loss": 0.9591,
      "step": 851500
    },
    {
      "epoch": 11.211558959377838,
      "grad_norm": 2.5480306148529053,
      "learning_rate": 4.425410302941995e-05,
      "loss": 0.961,
      "step": 852000
    },
    {
      "epoch": 11.218138512757754,
      "grad_norm": 2.5856833457946777,
      "learning_rate": 4.422098737631304e-05,
      "loss": 0.9659,
      "step": 852500
    },
    {
      "epoch": 11.22471806613767,
      "grad_norm": 2.521989583969116,
      "learning_rate": 4.418787172320613e-05,
      "loss": 0.9658,
      "step": 853000
    },
    {
      "epoch": 11.231297619517587,
      "grad_norm": 2.6422102451324463,
      "learning_rate": 4.415475607009922e-05,
      "loss": 0.9609,
      "step": 853500
    },
    {
      "epoch": 11.237877172897504,
      "grad_norm": 2.4558308124542236,
      "learning_rate": 4.4121640416992305e-05,
      "loss": 0.9622,
      "step": 854000
    },
    {
      "epoch": 11.24445672627742,
      "grad_norm": 2.712721347808838,
      "learning_rate": 4.40885247638854e-05,
      "loss": 0.9613,
      "step": 854500
    },
    {
      "epoch": 11.251036279657336,
      "grad_norm": 2.674182176589966,
      "learning_rate": 4.4055541573390914e-05,
      "loss": 0.9599,
      "step": 855000
    },
    {
      "epoch": 11.257615833037253,
      "grad_norm": 2.5726115703582764,
      "learning_rate": 4.4022425920284e-05,
      "loss": 0.9648,
      "step": 855500
    },
    {
      "epoch": 11.26419538641717,
      "grad_norm": 2.364079713821411,
      "learning_rate": 4.398931026717709e-05,
      "loss": 0.9672,
      "step": 856000
    },
    {
      "epoch": 11.270774939797086,
      "grad_norm": 2.2532331943511963,
      "learning_rate": 4.395619461407018e-05,
      "loss": 0.9669,
      "step": 856500
    },
    {
      "epoch": 11.277354493177004,
      "grad_norm": 2.215424060821533,
      "learning_rate": 4.392307896096327e-05,
      "loss": 0.9624,
      "step": 857000
    },
    {
      "epoch": 11.28393404655692,
      "grad_norm": 2.396182060241699,
      "learning_rate": 4.3890029539162574e-05,
      "loss": 0.9607,
      "step": 857500
    },
    {
      "epoch": 11.290513599936837,
      "grad_norm": 2.412142038345337,
      "learning_rate": 4.385691388605566e-05,
      "loss": 0.9581,
      "step": 858000
    },
    {
      "epoch": 11.297093153316753,
      "grad_norm": 2.6201868057250977,
      "learning_rate": 4.382379823294875e-05,
      "loss": 0.957,
      "step": 858500
    },
    {
      "epoch": 11.30367270669667,
      "grad_norm": 2.176497220993042,
      "learning_rate": 4.3790682579841843e-05,
      "loss": 0.9672,
      "step": 859000
    },
    {
      "epoch": 11.310252260076586,
      "grad_norm": 2.5378577709198,
      "learning_rate": 4.3757566926734936e-05,
      "loss": 0.9618,
      "step": 859500
    },
    {
      "epoch": 11.316831813456503,
      "grad_norm": 2.4436557292938232,
      "learning_rate": 4.372445127362802e-05,
      "loss": 0.9744,
      "step": 860000
    },
    {
      "epoch": 11.32341136683642,
      "grad_norm": 2.4822378158569336,
      "learning_rate": 4.369133562052111e-05,
      "loss": 0.9626,
      "step": 860500
    },
    {
      "epoch": 11.329990920216336,
      "grad_norm": 2.3843917846679688,
      "learning_rate": 4.365828619872041e-05,
      "loss": 0.9575,
      "step": 861000
    },
    {
      "epoch": 11.336570473596252,
      "grad_norm": 2.295708656311035,
      "learning_rate": 4.36251705456135e-05,
      "loss": 0.957,
      "step": 861500
    },
    {
      "epoch": 11.343150026976168,
      "grad_norm": 2.3934996128082275,
      "learning_rate": 4.359212112381281e-05,
      "loss": 0.9612,
      "step": 862000
    },
    {
      "epoch": 11.349729580356085,
      "grad_norm": 2.7619266510009766,
      "learning_rate": 4.355900547070589e-05,
      "loss": 0.9655,
      "step": 862500
    },
    {
      "epoch": 11.356309133736001,
      "grad_norm": 2.2051408290863037,
      "learning_rate": 4.3525889817598985e-05,
      "loss": 0.9599,
      "step": 863000
    },
    {
      "epoch": 11.362888687115918,
      "grad_norm": 2.620851755142212,
      "learning_rate": 4.349277416449207e-05,
      "loss": 0.9716,
      "step": 863500
    },
    {
      "epoch": 11.369468240495834,
      "grad_norm": 2.6089227199554443,
      "learning_rate": 4.345965851138516e-05,
      "loss": 0.9562,
      "step": 864000
    },
    {
      "epoch": 11.376047793875752,
      "grad_norm": 2.5986483097076416,
      "learning_rate": 4.3426542858278255e-05,
      "loss": 0.9591,
      "step": 864500
    },
    {
      "epoch": 11.382627347255669,
      "grad_norm": 2.4784274101257324,
      "learning_rate": 4.339342720517135e-05,
      "loss": 0.9535,
      "step": 865000
    },
    {
      "epoch": 11.389206900635585,
      "grad_norm": 2.560560941696167,
      "learning_rate": 4.336031155206443e-05,
      "loss": 0.9571,
      "step": 865500
    },
    {
      "epoch": 11.395786454015502,
      "grad_norm": 2.777209758758545,
      "learning_rate": 4.332726213026374e-05,
      "loss": 0.9621,
      "step": 866000
    },
    {
      "epoch": 11.402366007395418,
      "grad_norm": 2.587310314178467,
      "learning_rate": 4.329414647715683e-05,
      "loss": 0.9498,
      "step": 866500
    },
    {
      "epoch": 11.408945560775335,
      "grad_norm": 2.692927122116089,
      "learning_rate": 4.326109705535613e-05,
      "loss": 0.9669,
      "step": 867000
    },
    {
      "epoch": 11.415525114155251,
      "grad_norm": 2.413130283355713,
      "learning_rate": 4.322798140224922e-05,
      "loss": 0.9504,
      "step": 867500
    },
    {
      "epoch": 11.422104667535168,
      "grad_norm": 2.4550976753234863,
      "learning_rate": 4.3194865749142305e-05,
      "loss": 0.9655,
      "step": 868000
    },
    {
      "epoch": 11.428684220915084,
      "grad_norm": 2.699207305908203,
      "learning_rate": 4.31617500960354e-05,
      "loss": 0.9634,
      "step": 868500
    },
    {
      "epoch": 11.435263774295,
      "grad_norm": 2.512977123260498,
      "learning_rate": 4.312863444292848e-05,
      "loss": 0.9583,
      "step": 869000
    },
    {
      "epoch": 11.441843327674917,
      "grad_norm": 2.6517066955566406,
      "learning_rate": 4.3095518789821574e-05,
      "loss": 0.9701,
      "step": 869500
    },
    {
      "epoch": 11.448422881054833,
      "grad_norm": 2.436677932739258,
      "learning_rate": 4.3062403136714666e-05,
      "loss": 0.9589,
      "step": 870000
    },
    {
      "epoch": 11.45500243443475,
      "grad_norm": 2.6689887046813965,
      "learning_rate": 4.302928748360776e-05,
      "loss": 0.9578,
      "step": 870500
    },
    {
      "epoch": 11.461581987814666,
      "grad_norm": 2.774374485015869,
      "learning_rate": 4.2996171830500844e-05,
      "loss": 0.9601,
      "step": 871000
    },
    {
      "epoch": 11.468161541194585,
      "grad_norm": 2.305863857269287,
      "learning_rate": 4.2963056177393936e-05,
      "loss": 0.9642,
      "step": 871500
    },
    {
      "epoch": 11.474741094574501,
      "grad_norm": 2.303440570831299,
      "learning_rate": 4.292994052428702e-05,
      "loss": 0.9541,
      "step": 872000
    },
    {
      "epoch": 11.481320647954417,
      "grad_norm": 2.4044559001922607,
      "learning_rate": 4.289682487118011e-05,
      "loss": 0.9553,
      "step": 872500
    },
    {
      "epoch": 11.487900201334334,
      "grad_norm": 2.632408857345581,
      "learning_rate": 4.28637092180732e-05,
      "loss": 0.964,
      "step": 873000
    },
    {
      "epoch": 11.49447975471425,
      "grad_norm": 2.3124687671661377,
      "learning_rate": 4.28306597962725e-05,
      "loss": 0.9676,
      "step": 873500
    },
    {
      "epoch": 11.501059308094167,
      "grad_norm": 2.2101402282714844,
      "learning_rate": 4.279754414316559e-05,
      "loss": 0.951,
      "step": 874000
    },
    {
      "epoch": 11.507638861474083,
      "grad_norm": 2.6401724815368652,
      "learning_rate": 4.2764494721364893e-05,
      "loss": 0.9623,
      "step": 874500
    },
    {
      "epoch": 11.514218414854,
      "grad_norm": 2.3889169692993164,
      "learning_rate": 4.2731379068257985e-05,
      "loss": 0.9704,
      "step": 875000
    },
    {
      "epoch": 11.520797968233916,
      "grad_norm": 2.358790874481201,
      "learning_rate": 4.269826341515107e-05,
      "loss": 0.9634,
      "step": 875500
    },
    {
      "epoch": 11.527377521613833,
      "grad_norm": 2.1274709701538086,
      "learning_rate": 4.266514776204417e-05,
      "loss": 0.9524,
      "step": 876000
    },
    {
      "epoch": 11.533957074993749,
      "grad_norm": 2.6271438598632812,
      "learning_rate": 4.2632032108937255e-05,
      "loss": 0.9594,
      "step": 876500
    },
    {
      "epoch": 11.540536628373665,
      "grad_norm": 2.3911101818084717,
      "learning_rate": 4.259891645583035e-05,
      "loss": 0.9613,
      "step": 877000
    },
    {
      "epoch": 11.547116181753582,
      "grad_norm": 2.246640920639038,
      "learning_rate": 4.256580080272343e-05,
      "loss": 0.9562,
      "step": 877500
    },
    {
      "epoch": 11.553695735133498,
      "grad_norm": 2.7922708988189697,
      "learning_rate": 4.2532685149616524e-05,
      "loss": 0.9581,
      "step": 878000
    },
    {
      "epoch": 11.560275288513417,
      "grad_norm": 2.4782254695892334,
      "learning_rate": 4.249956949650961e-05,
      "loss": 0.9466,
      "step": 878500
    },
    {
      "epoch": 11.566854841893333,
      "grad_norm": 2.3616840839385986,
      "learning_rate": 4.2466520074708915e-05,
      "loss": 0.9544,
      "step": 879000
    },
    {
      "epoch": 11.57343439527325,
      "grad_norm": 2.469120979309082,
      "learning_rate": 4.2433404421602e-05,
      "loss": 0.9608,
      "step": 879500
    },
    {
      "epoch": 11.580013948653166,
      "grad_norm": 2.6079933643341064,
      "learning_rate": 4.2400354999801305e-05,
      "loss": 0.9582,
      "step": 880000
    },
    {
      "epoch": 11.586593502033082,
      "grad_norm": 2.3994600772857666,
      "learning_rate": 4.23672393466944e-05,
      "loss": 0.9585,
      "step": 880500
    },
    {
      "epoch": 11.593173055412999,
      "grad_norm": 2.7381210327148438,
      "learning_rate": 4.233412369358748e-05,
      "loss": 0.9534,
      "step": 881000
    },
    {
      "epoch": 11.599752608792915,
      "grad_norm": 2.591468095779419,
      "learning_rate": 4.2301008040480574e-05,
      "loss": 0.9599,
      "step": 881500
    },
    {
      "epoch": 11.606332162172832,
      "grad_norm": 2.382180690765381,
      "learning_rate": 4.2267892387373666e-05,
      "loss": 0.9609,
      "step": 882000
    },
    {
      "epoch": 11.612911715552748,
      "grad_norm": 2.8840065002441406,
      "learning_rate": 4.223477673426676e-05,
      "loss": 0.9571,
      "step": 882500
    },
    {
      "epoch": 11.619491268932665,
      "grad_norm": 2.284471035003662,
      "learning_rate": 4.2201661081159844e-05,
      "loss": 0.9608,
      "step": 883000
    },
    {
      "epoch": 11.626070822312581,
      "grad_norm": 2.692561149597168,
      "learning_rate": 4.2168545428052936e-05,
      "loss": 0.9517,
      "step": 883500
    },
    {
      "epoch": 11.632650375692497,
      "grad_norm": 2.417341470718384,
      "learning_rate": 4.213542977494602e-05,
      "loss": 0.9609,
      "step": 884000
    },
    {
      "epoch": 11.639229929072414,
      "grad_norm": 2.3873119354248047,
      "learning_rate": 4.2102380353145326e-05,
      "loss": 0.9561,
      "step": 884500
    },
    {
      "epoch": 11.64580948245233,
      "grad_norm": 2.1936519145965576,
      "learning_rate": 4.206926470003842e-05,
      "loss": 0.9528,
      "step": 885000
    },
    {
      "epoch": 11.652389035832247,
      "grad_norm": 2.7516424655914307,
      "learning_rate": 4.20361490469315e-05,
      "loss": 0.9518,
      "step": 885500
    },
    {
      "epoch": 11.658968589212165,
      "grad_norm": 2.643690347671509,
      "learning_rate": 4.2003033393824595e-05,
      "loss": 0.958,
      "step": 886000
    },
    {
      "epoch": 11.665548142592082,
      "grad_norm": 2.241525650024414,
      "learning_rate": 4.197005020333011e-05,
      "loss": 0.9548,
      "step": 886500
    },
    {
      "epoch": 11.672127695971998,
      "grad_norm": 2.4962618350982666,
      "learning_rate": 4.19369345502232e-05,
      "loss": 0.9578,
      "step": 887000
    },
    {
      "epoch": 11.678707249351914,
      "grad_norm": 2.3136088848114014,
      "learning_rate": 4.190381889711629e-05,
      "loss": 0.9504,
      "step": 887500
    },
    {
      "epoch": 11.68528680273183,
      "grad_norm": 2.638343095779419,
      "learning_rate": 4.187070324400938e-05,
      "loss": 0.9553,
      "step": 888000
    },
    {
      "epoch": 11.691866356111747,
      "grad_norm": 2.4359843730926514,
      "learning_rate": 4.1837587590902475e-05,
      "loss": 0.9645,
      "step": 888500
    },
    {
      "epoch": 11.698445909491664,
      "grad_norm": 2.5936169624328613,
      "learning_rate": 4.180447193779556e-05,
      "loss": 0.9537,
      "step": 889000
    },
    {
      "epoch": 11.70502546287158,
      "grad_norm": 2.506950855255127,
      "learning_rate": 4.177135628468865e-05,
      "loss": 0.9585,
      "step": 889500
    },
    {
      "epoch": 11.711605016251497,
      "grad_norm": 2.627814531326294,
      "learning_rate": 4.173824063158174e-05,
      "loss": 0.9557,
      "step": 890000
    },
    {
      "epoch": 11.718184569631413,
      "grad_norm": 2.7561142444610596,
      "learning_rate": 4.170519120978104e-05,
      "loss": 0.9596,
      "step": 890500
    },
    {
      "epoch": 11.72476412301133,
      "grad_norm": 2.256720781326294,
      "learning_rate": 4.167207555667413e-05,
      "loss": 0.9504,
      "step": 891000
    },
    {
      "epoch": 11.731343676391246,
      "grad_norm": 2.937973737716675,
      "learning_rate": 4.163895990356722e-05,
      "loss": 0.961,
      "step": 891500
    },
    {
      "epoch": 11.737923229771162,
      "grad_norm": 2.56589412689209,
      "learning_rate": 4.1605844250460305e-05,
      "loss": 0.9598,
      "step": 892000
    },
    {
      "epoch": 11.744502783151079,
      "grad_norm": 2.7545721530914307,
      "learning_rate": 4.157279482865961e-05,
      "loss": 0.9522,
      "step": 892500
    },
    {
      "epoch": 11.751082336530997,
      "grad_norm": 2.490661144256592,
      "learning_rate": 4.15396791755527e-05,
      "loss": 0.9565,
      "step": 893000
    },
    {
      "epoch": 11.757661889910914,
      "grad_norm": 2.4657530784606934,
      "learning_rate": 4.150656352244579e-05,
      "loss": 0.9535,
      "step": 893500
    },
    {
      "epoch": 11.76424144329083,
      "grad_norm": 2.6909751892089844,
      "learning_rate": 4.1473447869338886e-05,
      "loss": 0.9604,
      "step": 894000
    },
    {
      "epoch": 11.770820996670746,
      "grad_norm": 2.3139758110046387,
      "learning_rate": 4.1440398447538184e-05,
      "loss": 0.9556,
      "step": 894500
    },
    {
      "epoch": 11.777400550050663,
      "grad_norm": 2.2235519886016846,
      "learning_rate": 4.1407282794431276e-05,
      "loss": 0.9506,
      "step": 895000
    },
    {
      "epoch": 11.78398010343058,
      "grad_norm": 2.3748505115509033,
      "learning_rate": 4.137416714132437e-05,
      "loss": 0.9506,
      "step": 895500
    },
    {
      "epoch": 11.790559656810496,
      "grad_norm": 2.4440979957580566,
      "learning_rate": 4.1341051488217454e-05,
      "loss": 0.9507,
      "step": 896000
    },
    {
      "epoch": 11.797139210190412,
      "grad_norm": 2.8733632564544678,
      "learning_rate": 4.130793583511054e-05,
      "loss": 0.9553,
      "step": 896500
    },
    {
      "epoch": 11.803718763570329,
      "grad_norm": 2.4168190956115723,
      "learning_rate": 4.127482018200363e-05,
      "loss": 0.9526,
      "step": 897000
    },
    {
      "epoch": 11.810298316950245,
      "grad_norm": 2.490551233291626,
      "learning_rate": 4.1241704528896716e-05,
      "loss": 0.9449,
      "step": 897500
    },
    {
      "epoch": 11.816877870330162,
      "grad_norm": 2.8310248851776123,
      "learning_rate": 4.120858887578981e-05,
      "loss": 0.9589,
      "step": 898000
    },
    {
      "epoch": 11.823457423710078,
      "grad_norm": 2.4333369731903076,
      "learning_rate": 4.11754732226829e-05,
      "loss": 0.9552,
      "step": 898500
    },
    {
      "epoch": 11.830036977089994,
      "grad_norm": 2.1984481811523438,
      "learning_rate": 4.114235756957599e-05,
      "loss": 0.9529,
      "step": 899000
    },
    {
      "epoch": 11.836616530469911,
      "grad_norm": 2.367145299911499,
      "learning_rate": 4.110924191646908e-05,
      "loss": 0.9512,
      "step": 899500
    },
    {
      "epoch": 11.84319608384983,
      "grad_norm": 2.436112403869629,
      "learning_rate": 4.107612626336217e-05,
      "loss": 0.9462,
      "step": 900000
    },
    {
      "epoch": 11.849775637229746,
      "grad_norm": 2.460280656814575,
      "learning_rate": 4.1043010610255255e-05,
      "loss": 0.9537,
      "step": 900500
    },
    {
      "epoch": 11.856355190609662,
      "grad_norm": 2.224945306777954,
      "learning_rate": 4.101002741976078e-05,
      "loss": 0.956,
      "step": 901000
    },
    {
      "epoch": 11.862934743989578,
      "grad_norm": 2.5830395221710205,
      "learning_rate": 4.0976911766653865e-05,
      "loss": 0.9615,
      "step": 901500
    },
    {
      "epoch": 11.869514297369495,
      "grad_norm": 2.3404428958892822,
      "learning_rate": 4.094379611354696e-05,
      "loss": 0.9571,
      "step": 902000
    },
    {
      "epoch": 11.876093850749411,
      "grad_norm": 2.671221971511841,
      "learning_rate": 4.091068046044004e-05,
      "loss": 0.9454,
      "step": 902500
    },
    {
      "epoch": 11.882673404129328,
      "grad_norm": 2.4098451137542725,
      "learning_rate": 4.087756480733313e-05,
      "loss": 0.9555,
      "step": 903000
    },
    {
      "epoch": 11.889252957509244,
      "grad_norm": 2.661771059036255,
      "learning_rate": 4.084444915422622e-05,
      "loss": 0.9608,
      "step": 903500
    },
    {
      "epoch": 11.89583251088916,
      "grad_norm": 2.4740805625915527,
      "learning_rate": 4.081133350111931e-05,
      "loss": 0.9492,
      "step": 904000
    },
    {
      "epoch": 11.902412064269077,
      "grad_norm": 2.302919387817383,
      "learning_rate": 4.077828407931861e-05,
      "loss": 0.9629,
      "step": 904500
    },
    {
      "epoch": 11.908991617648994,
      "grad_norm": 2.3882834911346436,
      "learning_rate": 4.07451684262117e-05,
      "loss": 0.9502,
      "step": 905000
    },
    {
      "epoch": 11.91557117102891,
      "grad_norm": 2.5503478050231934,
      "learning_rate": 4.0712052773104794e-05,
      "loss": 0.9507,
      "step": 905500
    },
    {
      "epoch": 11.922150724408827,
      "grad_norm": 2.1634163856506348,
      "learning_rate": 4.06790033513041e-05,
      "loss": 0.9524,
      "step": 906000
    },
    {
      "epoch": 11.928730277788743,
      "grad_norm": 2.5142011642456055,
      "learning_rate": 4.064588769819719e-05,
      "loss": 0.9556,
      "step": 906500
    },
    {
      "epoch": 11.93530983116866,
      "grad_norm": 2.5396482944488525,
      "learning_rate": 4.0612772045090276e-05,
      "loss": 0.9436,
      "step": 907000
    },
    {
      "epoch": 11.941889384548578,
      "grad_norm": 2.8139073848724365,
      "learning_rate": 4.057965639198337e-05,
      "loss": 0.9503,
      "step": 907500
    },
    {
      "epoch": 11.948468937928494,
      "grad_norm": 2.4089066982269287,
      "learning_rate": 4.0546540738876454e-05,
      "loss": 0.9578,
      "step": 908000
    },
    {
      "epoch": 11.95504849130841,
      "grad_norm": 2.420736074447632,
      "learning_rate": 4.051342508576954e-05,
      "loss": 0.9536,
      "step": 908500
    },
    {
      "epoch": 11.961628044688327,
      "grad_norm": 2.2821009159088135,
      "learning_rate": 4.048030943266263e-05,
      "loss": 0.9537,
      "step": 909000
    },
    {
      "epoch": 11.968207598068243,
      "grad_norm": 2.549213409423828,
      "learning_rate": 4.0447260010861936e-05,
      "loss": 0.9632,
      "step": 909500
    },
    {
      "epoch": 11.97478715144816,
      "grad_norm": 2.6384074687957764,
      "learning_rate": 4.041414435775502e-05,
      "loss": 0.9548,
      "step": 910000
    },
    {
      "epoch": 11.981366704828076,
      "grad_norm": 2.822671413421631,
      "learning_rate": 4.038102870464811e-05,
      "loss": 0.9417,
      "step": 910500
    },
    {
      "epoch": 11.987946258207993,
      "grad_norm": 2.7668731212615967,
      "learning_rate": 4.0347913051541205e-05,
      "loss": 0.9521,
      "step": 911000
    },
    {
      "epoch": 11.99452581158791,
      "grad_norm": 2.4806032180786133,
      "learning_rate": 4.03147973984343e-05,
      "loss": 0.9501,
      "step": 911500
    },
    {
      "epoch": 12.0,
      "eval_loss": 0.9055215716362,
      "eval_runtime": 49.6559,
      "eval_samples_per_second": 2013.86,
      "eval_steps_per_second": 15.748,
      "step": 911916
    },
    {
      "epoch": 12.001105364967826,
      "grad_norm": 2.7411370277404785,
      "learning_rate": 4.028168174532738e-05,
      "loss": 0.951,
      "step": 912000
    },
    {
      "epoch": 12.007684918347742,
      "grad_norm": 2.306725025177002,
      "learning_rate": 4.0248566092220475e-05,
      "loss": 0.9379,
      "step": 912500
    },
    {
      "epoch": 12.014264471727659,
      "grad_norm": 2.4029695987701416,
      "learning_rate": 4.021545043911356e-05,
      "loss": 0.9404,
      "step": 913000
    },
    {
      "epoch": 12.020844025107575,
      "grad_norm": 2.5791382789611816,
      "learning_rate": 4.018233478600665e-05,
      "loss": 0.9437,
      "step": 913500
    },
    {
      "epoch": 12.027423578487491,
      "grad_norm": 2.4579949378967285,
      "learning_rate": 4.014928536420596e-05,
      "loss": 0.9394,
      "step": 914000
    },
    {
      "epoch": 12.03400313186741,
      "grad_norm": 2.701305389404297,
      "learning_rate": 4.011616971109904e-05,
      "loss": 0.9441,
      "step": 914500
    },
    {
      "epoch": 12.040582685247326,
      "grad_norm": 2.717609405517578,
      "learning_rate": 4.008305405799213e-05,
      "loss": 0.9418,
      "step": 915000
    },
    {
      "epoch": 12.047162238627243,
      "grad_norm": 2.6141068935394287,
      "learning_rate": 4.005000463619143e-05,
      "loss": 0.9484,
      "step": 915500
    },
    {
      "epoch": 12.053741792007159,
      "grad_norm": 3.045130491256714,
      "learning_rate": 4.0016888983084525e-05,
      "loss": 0.9381,
      "step": 916000
    },
    {
      "epoch": 12.060321345387075,
      "grad_norm": 2.455056667327881,
      "learning_rate": 3.998377332997762e-05,
      "loss": 0.9449,
      "step": 916500
    },
    {
      "epoch": 12.066900898766992,
      "grad_norm": 2.4916796684265137,
      "learning_rate": 3.995065767687071e-05,
      "loss": 0.951,
      "step": 917000
    },
    {
      "epoch": 12.073480452146908,
      "grad_norm": 2.455735206604004,
      "learning_rate": 3.9917542023763794e-05,
      "loss": 0.9469,
      "step": 917500
    },
    {
      "epoch": 12.080060005526825,
      "grad_norm": 2.381875514984131,
      "learning_rate": 3.9884426370656886e-05,
      "loss": 0.9444,
      "step": 918000
    },
    {
      "epoch": 12.086639558906741,
      "grad_norm": 2.512298107147217,
      "learning_rate": 3.985131071754997e-05,
      "loss": 0.9434,
      "step": 918500
    },
    {
      "epoch": 12.093219112286658,
      "grad_norm": 3.128302812576294,
      "learning_rate": 3.9818195064443064e-05,
      "loss": 0.9433,
      "step": 919000
    },
    {
      "epoch": 12.099798665666574,
      "grad_norm": 2.447597026824951,
      "learning_rate": 3.978507941133615e-05,
      "loss": 0.9439,
      "step": 919500
    },
    {
      "epoch": 12.10637821904649,
      "grad_norm": 2.6863319873809814,
      "learning_rate": 3.975196375822924e-05,
      "loss": 0.947,
      "step": 920000
    },
    {
      "epoch": 12.112957772426407,
      "grad_norm": 2.2843849658966064,
      "learning_rate": 3.971884810512233e-05,
      "loss": 0.9436,
      "step": 920500
    },
    {
      "epoch": 12.119537325806323,
      "grad_norm": 2.7917544841766357,
      "learning_rate": 3.9685732452015425e-05,
      "loss": 0.9433,
      "step": 921000
    },
    {
      "epoch": 12.126116879186242,
      "grad_norm": 2.485072612762451,
      "learning_rate": 3.965261679890851e-05,
      "loss": 0.9478,
      "step": 921500
    },
    {
      "epoch": 12.132696432566158,
      "grad_norm": 2.4283149242401123,
      "learning_rate": 3.96195011458016e-05,
      "loss": 0.9374,
      "step": 922000
    },
    {
      "epoch": 12.139275985946075,
      "grad_norm": 2.7853994369506836,
      "learning_rate": 3.958638549269469e-05,
      "loss": 0.9451,
      "step": 922500
    },
    {
      "epoch": 12.145855539325991,
      "grad_norm": 2.5187220573425293,
      "learning_rate": 3.955326983958778e-05,
      "loss": 0.9417,
      "step": 923000
    },
    {
      "epoch": 12.152435092705907,
      "grad_norm": 2.161076307296753,
      "learning_rate": 3.952022041778708e-05,
      "loss": 0.9431,
      "step": 923500
    },
    {
      "epoch": 12.159014646085824,
      "grad_norm": 2.609158515930176,
      "learning_rate": 3.948710476468017e-05,
      "loss": 0.9415,
      "step": 924000
    },
    {
      "epoch": 12.16559419946574,
      "grad_norm": 2.4458460807800293,
      "learning_rate": 3.9454055342879475e-05,
      "loss": 0.9405,
      "step": 924500
    },
    {
      "epoch": 12.172173752845657,
      "grad_norm": 2.5047459602355957,
      "learning_rate": 3.942093968977256e-05,
      "loss": 0.9492,
      "step": 925000
    },
    {
      "epoch": 12.178753306225573,
      "grad_norm": 2.3535518646240234,
      "learning_rate": 3.938782403666565e-05,
      "loss": 0.938,
      "step": 925500
    },
    {
      "epoch": 12.18533285960549,
      "grad_norm": 2.5538721084594727,
      "learning_rate": 3.9354708383558744e-05,
      "loss": 0.9355,
      "step": 926000
    },
    {
      "epoch": 12.191912412985406,
      "grad_norm": 2.451535940170288,
      "learning_rate": 3.932165896175804e-05,
      "loss": 0.9496,
      "step": 926500
    },
    {
      "epoch": 12.198491966365323,
      "grad_norm": 2.5214099884033203,
      "learning_rate": 3.9288543308651135e-05,
      "loss": 0.9372,
      "step": 927000
    },
    {
      "epoch": 12.205071519745239,
      "grad_norm": 2.6848721504211426,
      "learning_rate": 3.925542765554423e-05,
      "loss": 0.9398,
      "step": 927500
    },
    {
      "epoch": 12.211651073125156,
      "grad_norm": 2.452131986618042,
      "learning_rate": 3.922244446504974e-05,
      "loss": 0.9398,
      "step": 928000
    },
    {
      "epoch": 12.218230626505072,
      "grad_norm": 2.35976505279541,
      "learning_rate": 3.918932881194283e-05,
      "loss": 0.9444,
      "step": 928500
    },
    {
      "epoch": 12.22481017988499,
      "grad_norm": 2.6175663471221924,
      "learning_rate": 3.915621315883592e-05,
      "loss": 0.9365,
      "step": 929000
    },
    {
      "epoch": 12.231389733264907,
      "grad_norm": 2.4605283737182617,
      "learning_rate": 3.9123097505729014e-05,
      "loss": 0.9368,
      "step": 929500
    },
    {
      "epoch": 12.237969286644823,
      "grad_norm": 2.6456422805786133,
      "learning_rate": 3.90899818526221e-05,
      "loss": 0.9404,
      "step": 930000
    },
    {
      "epoch": 12.24454884002474,
      "grad_norm": 2.476017951965332,
      "learning_rate": 3.905686619951519e-05,
      "loss": 0.9425,
      "step": 930500
    },
    {
      "epoch": 12.251128393404656,
      "grad_norm": 2.3883750438690186,
      "learning_rate": 3.9023750546408276e-05,
      "loss": 0.9402,
      "step": 931000
    },
    {
      "epoch": 12.257707946784572,
      "grad_norm": 2.8651206493377686,
      "learning_rate": 3.899070112460758e-05,
      "loss": 0.9446,
      "step": 931500
    },
    {
      "epoch": 12.264287500164489,
      "grad_norm": 2.512606143951416,
      "learning_rate": 3.895758547150067e-05,
      "loss": 0.9416,
      "step": 932000
    },
    {
      "epoch": 12.270867053544405,
      "grad_norm": 2.7454047203063965,
      "learning_rate": 3.892446981839376e-05,
      "loss": 0.9434,
      "step": 932500
    },
    {
      "epoch": 12.277446606924322,
      "grad_norm": 2.6098828315734863,
      "learning_rate": 3.889135416528685e-05,
      "loss": 0.939,
      "step": 933000
    },
    {
      "epoch": 12.284026160304238,
      "grad_norm": 2.6959567070007324,
      "learning_rate": 3.885823851217994e-05,
      "loss": 0.9423,
      "step": 933500
    },
    {
      "epoch": 12.290605713684155,
      "grad_norm": 2.284959554672241,
      "learning_rate": 3.882512285907303e-05,
      "loss": 0.9369,
      "step": 934000
    },
    {
      "epoch": 12.297185267064071,
      "grad_norm": 2.967664957046509,
      "learning_rate": 3.879200720596612e-05,
      "loss": 0.9374,
      "step": 934500
    },
    {
      "epoch": 12.303764820443988,
      "grad_norm": 2.4216930866241455,
      "learning_rate": 3.8758891552859206e-05,
      "loss": 0.9414,
      "step": 935000
    },
    {
      "epoch": 12.310344373823904,
      "grad_norm": 2.4164633750915527,
      "learning_rate": 3.87257758997523e-05,
      "loss": 0.9432,
      "step": 935500
    },
    {
      "epoch": 12.316923927203822,
      "grad_norm": 2.6306254863739014,
      "learning_rate": 3.869266024664538e-05,
      "loss": 0.9473,
      "step": 936000
    },
    {
      "epoch": 12.323503480583739,
      "grad_norm": 2.727522611618042,
      "learning_rate": 3.865961082484469e-05,
      "loss": 0.9401,
      "step": 936500
    },
    {
      "epoch": 12.330083033963655,
      "grad_norm": 2.394541025161743,
      "learning_rate": 3.862649517173778e-05,
      "loss": 0.9428,
      "step": 937000
    },
    {
      "epoch": 12.336662587343572,
      "grad_norm": 2.510908842086792,
      "learning_rate": 3.8593379518630865e-05,
      "loss": 0.9394,
      "step": 937500
    },
    {
      "epoch": 12.343242140723488,
      "grad_norm": 2.7781662940979004,
      "learning_rate": 3.856026386552396e-05,
      "loss": 0.9464,
      "step": 938000
    },
    {
      "epoch": 12.349821694103404,
      "grad_norm": 2.906533718109131,
      "learning_rate": 3.852714821241705e-05,
      "loss": 0.9374,
      "step": 938500
    },
    {
      "epoch": 12.356401247483321,
      "grad_norm": 2.458218574523926,
      "learning_rate": 3.849403255931014e-05,
      "loss": 0.9385,
      "step": 939000
    },
    {
      "epoch": 12.362980800863237,
      "grad_norm": 2.4165093898773193,
      "learning_rate": 3.846091690620323e-05,
      "loss": 0.9363,
      "step": 939500
    },
    {
      "epoch": 12.369560354243154,
      "grad_norm": 2.843764305114746,
      "learning_rate": 3.842780125309631e-05,
      "loss": 0.9363,
      "step": 940000
    },
    {
      "epoch": 12.37613990762307,
      "grad_norm": 2.7647275924682617,
      "learning_rate": 3.839475183129562e-05,
      "loss": 0.95,
      "step": 940500
    },
    {
      "epoch": 12.382719461002987,
      "grad_norm": 2.9224534034729004,
      "learning_rate": 3.836163617818871e-05,
      "loss": 0.9445,
      "step": 941000
    },
    {
      "epoch": 12.389299014382903,
      "grad_norm": 2.697849750518799,
      "learning_rate": 3.8328520525081794e-05,
      "loss": 0.9381,
      "step": 941500
    },
    {
      "epoch": 12.39587856776282,
      "grad_norm": 2.648733139038086,
      "learning_rate": 3.8295404871974886e-05,
      "loss": 0.9391,
      "step": 942000
    },
    {
      "epoch": 12.402458121142736,
      "grad_norm": 2.6267433166503906,
      "learning_rate": 3.826235545017419e-05,
      "loss": 0.944,
      "step": 942500
    },
    {
      "epoch": 12.409037674522654,
      "grad_norm": 2.443747043609619,
      "learning_rate": 3.8229239797067277e-05,
      "loss": 0.9436,
      "step": 943000
    },
    {
      "epoch": 12.41561722790257,
      "grad_norm": 2.9066731929779053,
      "learning_rate": 3.819619037526658e-05,
      "loss": 0.945,
      "step": 943500
    },
    {
      "epoch": 12.422196781282487,
      "grad_norm": 2.5504724979400635,
      "learning_rate": 3.816307472215967e-05,
      "loss": 0.941,
      "step": 944000
    },
    {
      "epoch": 12.428776334662404,
      "grad_norm": 2.210841417312622,
      "learning_rate": 3.812995906905276e-05,
      "loss": 0.9449,
      "step": 944500
    },
    {
      "epoch": 12.43535588804232,
      "grad_norm": 2.692566156387329,
      "learning_rate": 3.809684341594585e-05,
      "loss": 0.9377,
      "step": 945000
    },
    {
      "epoch": 12.441935441422237,
      "grad_norm": 2.5973336696624756,
      "learning_rate": 3.806372776283894e-05,
      "loss": 0.9476,
      "step": 945500
    },
    {
      "epoch": 12.448514994802153,
      "grad_norm": 2.547689199447632,
      "learning_rate": 3.803061210973203e-05,
      "loss": 0.9443,
      "step": 946000
    },
    {
      "epoch": 12.45509454818207,
      "grad_norm": 2.4989867210388184,
      "learning_rate": 3.799749645662512e-05,
      "loss": 0.9374,
      "step": 946500
    },
    {
      "epoch": 12.461674101561986,
      "grad_norm": 2.4335012435913086,
      "learning_rate": 3.7964380803518206e-05,
      "loss": 0.939,
      "step": 947000
    },
    {
      "epoch": 12.468253654941902,
      "grad_norm": 2.9823453426361084,
      "learning_rate": 3.79312651504113e-05,
      "loss": 0.9378,
      "step": 947500
    },
    {
      "epoch": 12.474833208321819,
      "grad_norm": 2.4281928539276123,
      "learning_rate": 3.789814949730438e-05,
      "loss": 0.937,
      "step": 948000
    },
    {
      "epoch": 12.481412761701735,
      "grad_norm": 2.663776397705078,
      "learning_rate": 3.7865033844197475e-05,
      "loss": 0.9395,
      "step": 948500
    },
    {
      "epoch": 12.487992315081652,
      "grad_norm": 2.373051166534424,
      "learning_rate": 3.783191819109057e-05,
      "loss": 0.9426,
      "step": 949000
    },
    {
      "epoch": 12.494571868461568,
      "grad_norm": 2.36515736579895,
      "learning_rate": 3.779880253798366e-05,
      "loss": 0.9356,
      "step": 949500
    },
    {
      "epoch": 12.501151421841485,
      "grad_norm": 2.5899064540863037,
      "learning_rate": 3.7765753116182964e-05,
      "loss": 0.938,
      "step": 950000
    },
    {
      "epoch": 12.507730975221403,
      "grad_norm": 2.7917821407318115,
      "learning_rate": 3.773263746307605e-05,
      "loss": 0.9393,
      "step": 950500
    },
    {
      "epoch": 12.51431052860132,
      "grad_norm": 2.3709757328033447,
      "learning_rate": 3.769952180996914e-05,
      "loss": 0.9427,
      "step": 951000
    },
    {
      "epoch": 12.520890081981236,
      "grad_norm": 3.1219916343688965,
      "learning_rate": 3.766640615686223e-05,
      "loss": 0.9364,
      "step": 951500
    },
    {
      "epoch": 12.527469635361152,
      "grad_norm": 2.640300989151001,
      "learning_rate": 3.763329050375532e-05,
      "loss": 0.9363,
      "step": 952000
    },
    {
      "epoch": 12.534049188741069,
      "grad_norm": 2.4558019638061523,
      "learning_rate": 3.7600174850648404e-05,
      "loss": 0.9478,
      "step": 952500
    },
    {
      "epoch": 12.540628742120985,
      "grad_norm": 2.3953473567962646,
      "learning_rate": 3.7567059197541496e-05,
      "loss": 0.9398,
      "step": 953000
    },
    {
      "epoch": 12.547208295500901,
      "grad_norm": 2.580373525619507,
      "learning_rate": 3.753394354443459e-05,
      "loss": 0.9376,
      "step": 953500
    },
    {
      "epoch": 12.553787848880818,
      "grad_norm": 2.7454419136047363,
      "learning_rate": 3.7500827891327674e-05,
      "loss": 0.9334,
      "step": 954000
    },
    {
      "epoch": 12.560367402260734,
      "grad_norm": 2.534914493560791,
      "learning_rate": 3.7467712238220766e-05,
      "loss": 0.9312,
      "step": 954500
    },
    {
      "epoch": 12.56694695564065,
      "grad_norm": 2.588970184326172,
      "learning_rate": 3.7434729047726283e-05,
      "loss": 0.9352,
      "step": 955000
    },
    {
      "epoch": 12.573526509020567,
      "grad_norm": 2.9321136474609375,
      "learning_rate": 3.7401613394619375e-05,
      "loss": 0.9306,
      "step": 955500
    },
    {
      "epoch": 12.580106062400484,
      "grad_norm": 2.867405652999878,
      "learning_rate": 3.736849774151246e-05,
      "loss": 0.946,
      "step": 956000
    },
    {
      "epoch": 12.5866856157804,
      "grad_norm": 2.5838077068328857,
      "learning_rate": 3.733538208840555e-05,
      "loss": 0.9316,
      "step": 956500
    },
    {
      "epoch": 12.593265169160317,
      "grad_norm": 2.607590436935425,
      "learning_rate": 3.730233266660485e-05,
      "loss": 0.9364,
      "step": 957000
    },
    {
      "epoch": 12.599844722540235,
      "grad_norm": 2.2641761302948,
      "learning_rate": 3.726921701349794e-05,
      "loss": 0.9472,
      "step": 957500
    },
    {
      "epoch": 12.606424275920151,
      "grad_norm": 2.468677520751953,
      "learning_rate": 3.723610136039103e-05,
      "loss": 0.9301,
      "step": 958000
    },
    {
      "epoch": 12.613003829300068,
      "grad_norm": 2.7013211250305176,
      "learning_rate": 3.720298570728412e-05,
      "loss": 0.936,
      "step": 958500
    },
    {
      "epoch": 12.619583382679984,
      "grad_norm": 2.4551048278808594,
      "learning_rate": 3.7169870054177206e-05,
      "loss": 0.9377,
      "step": 959000
    },
    {
      "epoch": 12.6261629360599,
      "grad_norm": 2.788947105407715,
      "learning_rate": 3.71367544010703e-05,
      "loss": 0.9411,
      "step": 959500
    },
    {
      "epoch": 12.632742489439817,
      "grad_norm": 2.373370409011841,
      "learning_rate": 3.710363874796339e-05,
      "loss": 0.933,
      "step": 960000
    },
    {
      "epoch": 12.639322042819733,
      "grad_norm": 2.563344955444336,
      "learning_rate": 3.707052309485648e-05,
      "loss": 0.9324,
      "step": 960500
    },
    {
      "epoch": 12.64590159619965,
      "grad_norm": 2.6708855628967285,
      "learning_rate": 3.703740744174957e-05,
      "loss": 0.9285,
      "step": 961000
    },
    {
      "epoch": 12.652481149579566,
      "grad_norm": 2.4895777702331543,
      "learning_rate": 3.700435801994887e-05,
      "loss": 0.9397,
      "step": 961500
    },
    {
      "epoch": 12.659060702959483,
      "grad_norm": 2.502978563308716,
      "learning_rate": 3.6971242366841964e-05,
      "loss": 0.9317,
      "step": 962000
    },
    {
      "epoch": 12.6656402563394,
      "grad_norm": 2.6859512329101562,
      "learning_rate": 3.693812671373505e-05,
      "loss": 0.9418,
      "step": 962500
    },
    {
      "epoch": 12.672219809719316,
      "grad_norm": 2.5647342205047607,
      "learning_rate": 3.690501106062814e-05,
      "loss": 0.9313,
      "step": 963000
    },
    {
      "epoch": 12.678799363099232,
      "grad_norm": 2.494080066680908,
      "learning_rate": 3.687196163882744e-05,
      "loss": 0.9357,
      "step": 963500
    },
    {
      "epoch": 12.685378916479149,
      "grad_norm": 2.5116829872131348,
      "learning_rate": 3.683884598572053e-05,
      "loss": 0.9383,
      "step": 964000
    },
    {
      "epoch": 12.691958469859067,
      "grad_norm": 2.594998359680176,
      "learning_rate": 3.680573033261362e-05,
      "loss": 0.9302,
      "step": 964500
    },
    {
      "epoch": 12.698538023238983,
      "grad_norm": 2.4021875858306885,
      "learning_rate": 3.677261467950671e-05,
      "loss": 0.9327,
      "step": 965000
    },
    {
      "epoch": 12.7051175766189,
      "grad_norm": 2.4421796798706055,
      "learning_rate": 3.67394990263998e-05,
      "loss": 0.9335,
      "step": 965500
    },
    {
      "epoch": 12.711697129998816,
      "grad_norm": 2.2853453159332275,
      "learning_rate": 3.670638337329289e-05,
      "loss": 0.9333,
      "step": 966000
    },
    {
      "epoch": 12.718276683378733,
      "grad_norm": 2.590184211730957,
      "learning_rate": 3.667333395149219e-05,
      "loss": 0.9378,
      "step": 966500
    },
    {
      "epoch": 12.724856236758649,
      "grad_norm": 2.3992152214050293,
      "learning_rate": 3.6640218298385284e-05,
      "loss": 0.9376,
      "step": 967000
    },
    {
      "epoch": 12.731435790138566,
      "grad_norm": 2.5157086849212646,
      "learning_rate": 3.660716887658459e-05,
      "loss": 0.9321,
      "step": 967500
    },
    {
      "epoch": 12.738015343518482,
      "grad_norm": 2.4546942710876465,
      "learning_rate": 3.657405322347768e-05,
      "loss": 0.9364,
      "step": 968000
    },
    {
      "epoch": 12.744594896898398,
      "grad_norm": 2.448077440261841,
      "learning_rate": 3.6540937570370766e-05,
      "loss": 0.9328,
      "step": 968500
    },
    {
      "epoch": 12.751174450278315,
      "grad_norm": 2.59836745262146,
      "learning_rate": 3.650782191726385e-05,
      "loss": 0.9305,
      "step": 969000
    },
    {
      "epoch": 12.757754003658231,
      "grad_norm": 2.510817289352417,
      "learning_rate": 3.647470626415694e-05,
      "loss": 0.9374,
      "step": 969500
    },
    {
      "epoch": 12.764333557038148,
      "grad_norm": 2.87328839302063,
      "learning_rate": 3.644159061105003e-05,
      "loss": 0.9385,
      "step": 970000
    },
    {
      "epoch": 12.770913110418064,
      "grad_norm": 2.4613845348358154,
      "learning_rate": 3.640847495794312e-05,
      "loss": 0.943,
      "step": 970500
    },
    {
      "epoch": 12.77749266379798,
      "grad_norm": 2.417013645172119,
      "learning_rate": 3.637535930483621e-05,
      "loss": 0.9305,
      "step": 971000
    },
    {
      "epoch": 12.784072217177897,
      "grad_norm": 2.9270899295806885,
      "learning_rate": 3.634230988303551e-05,
      "loss": 0.9294,
      "step": 971500
    },
    {
      "epoch": 12.790651770557815,
      "grad_norm": 2.614017963409424,
      "learning_rate": 3.63091942299286e-05,
      "loss": 0.9318,
      "step": 972000
    },
    {
      "epoch": 12.797231323937732,
      "grad_norm": 2.331340789794922,
      "learning_rate": 3.6276078576821695e-05,
      "loss": 0.9397,
      "step": 972500
    },
    {
      "epoch": 12.803810877317648,
      "grad_norm": 2.4744210243225098,
      "learning_rate": 3.624296292371479e-05,
      "loss": 0.9308,
      "step": 973000
    },
    {
      "epoch": 12.810390430697565,
      "grad_norm": 2.5661206245422363,
      "learning_rate": 3.620984727060787e-05,
      "loss": 0.9356,
      "step": 973500
    },
    {
      "epoch": 12.816969984077481,
      "grad_norm": 2.110337495803833,
      "learning_rate": 3.6176731617500964e-05,
      "loss": 0.9296,
      "step": 974000
    },
    {
      "epoch": 12.823549537457398,
      "grad_norm": 2.6747395992279053,
      "learning_rate": 3.614361596439405e-05,
      "loss": 0.9332,
      "step": 974500
    },
    {
      "epoch": 12.830129090837314,
      "grad_norm": 2.3788418769836426,
      "learning_rate": 3.611050031128714e-05,
      "loss": 0.9372,
      "step": 975000
    },
    {
      "epoch": 12.83670864421723,
      "grad_norm": 2.4151852130889893,
      "learning_rate": 3.607738465818023e-05,
      "loss": 0.9366,
      "step": 975500
    },
    {
      "epoch": 12.843288197597147,
      "grad_norm": 2.559966802597046,
      "learning_rate": 3.604433523637953e-05,
      "loss": 0.939,
      "step": 976000
    },
    {
      "epoch": 12.849867750977063,
      "grad_norm": 2.8981566429138184,
      "learning_rate": 3.6011219583272624e-05,
      "loss": 0.9395,
      "step": 976500
    },
    {
      "epoch": 12.85644730435698,
      "grad_norm": 2.576594591140747,
      "learning_rate": 3.5978103930165716e-05,
      "loss": 0.93,
      "step": 977000
    },
    {
      "epoch": 12.863026857736896,
      "grad_norm": 2.433666944503784,
      "learning_rate": 3.5945054508365014e-05,
      "loss": 0.9359,
      "step": 977500
    },
    {
      "epoch": 12.869606411116813,
      "grad_norm": 2.739530324935913,
      "learning_rate": 3.5911938855258106e-05,
      "loss": 0.9414,
      "step": 978000
    },
    {
      "epoch": 12.87618596449673,
      "grad_norm": 2.3325839042663574,
      "learning_rate": 3.58788232021512e-05,
      "loss": 0.9329,
      "step": 978500
    },
    {
      "epoch": 12.882765517876647,
      "grad_norm": 2.16259765625,
      "learning_rate": 3.5845707549044284e-05,
      "loss": 0.9291,
      "step": 979000
    },
    {
      "epoch": 12.889345071256564,
      "grad_norm": 2.5992989540100098,
      "learning_rate": 3.5812591895937376e-05,
      "loss": 0.9284,
      "step": 979500
    },
    {
      "epoch": 12.89592462463648,
      "grad_norm": 2.822815179824829,
      "learning_rate": 3.577947624283046e-05,
      "loss": 0.9301,
      "step": 980000
    },
    {
      "epoch": 12.902504178016397,
      "grad_norm": 2.486142635345459,
      "learning_rate": 3.574636058972355e-05,
      "loss": 0.9232,
      "step": 980500
    },
    {
      "epoch": 12.909083731396313,
      "grad_norm": 2.4566640853881836,
      "learning_rate": 3.571324493661664e-05,
      "loss": 0.927,
      "step": 981000
    },
    {
      "epoch": 12.91566328477623,
      "grad_norm": 2.622974395751953,
      "learning_rate": 3.568019551481594e-05,
      "loss": 0.932,
      "step": 981500
    },
    {
      "epoch": 12.922242838156146,
      "grad_norm": 2.716106653213501,
      "learning_rate": 3.564707986170903e-05,
      "loss": 0.9283,
      "step": 982000
    },
    {
      "epoch": 12.928822391536062,
      "grad_norm": 2.720606565475464,
      "learning_rate": 3.561396420860213e-05,
      "loss": 0.9286,
      "step": 982500
    },
    {
      "epoch": 12.935401944915979,
      "grad_norm": 2.5303657054901123,
      "learning_rate": 3.558084855549521e-05,
      "loss": 0.9365,
      "step": 983000
    },
    {
      "epoch": 12.941981498295895,
      "grad_norm": 2.244197130203247,
      "learning_rate": 3.554779913369452e-05,
      "loss": 0.9241,
      "step": 983500
    },
    {
      "epoch": 12.948561051675812,
      "grad_norm": 2.250162124633789,
      "learning_rate": 3.5514815943200035e-05,
      "loss": 0.9296,
      "step": 984000
    },
    {
      "epoch": 12.955140605055728,
      "grad_norm": 2.6420605182647705,
      "learning_rate": 3.548170029009312e-05,
      "loss": 0.9319,
      "step": 984500
    },
    {
      "epoch": 12.961720158435645,
      "grad_norm": 2.268486976623535,
      "learning_rate": 3.544858463698621e-05,
      "loss": 0.9311,
      "step": 985000
    },
    {
      "epoch": 12.968299711815561,
      "grad_norm": 2.4809749126434326,
      "learning_rate": 3.5415468983879305e-05,
      "loss": 0.9426,
      "step": 985500
    },
    {
      "epoch": 12.97487926519548,
      "grad_norm": 2.589958429336548,
      "learning_rate": 3.538235333077239e-05,
      "loss": 0.9294,
      "step": 986000
    },
    {
      "epoch": 12.981458818575396,
      "grad_norm": 2.5339672565460205,
      "learning_rate": 3.534923767766548e-05,
      "loss": 0.9314,
      "step": 986500
    },
    {
      "epoch": 12.988038371955312,
      "grad_norm": 2.5816779136657715,
      "learning_rate": 3.531612202455857e-05,
      "loss": 0.9255,
      "step": 987000
    },
    {
      "epoch": 12.994617925335229,
      "grad_norm": 2.521942615509033,
      "learning_rate": 3.528300637145166e-05,
      "loss": 0.9342,
      "step": 987500
    },
    {
      "epoch": 13.0,
      "eval_loss": 0.8899210691452026,
      "eval_runtime": 49.6975,
      "eval_samples_per_second": 2012.173,
      "eval_steps_per_second": 15.735,
      "step": 987909
    },
    {
      "epoch": 13.001197478715145,
      "grad_norm": 2.524386405944824,
      "learning_rate": 3.5249956949650964e-05,
      "loss": 0.9331,
      "step": 988000
    },
    {
      "epoch": 13.007777032095062,
      "grad_norm": 2.553809642791748,
      "learning_rate": 3.521684129654405e-05,
      "loss": 0.9269,
      "step": 988500
    },
    {
      "epoch": 13.014356585474978,
      "grad_norm": 2.7443549633026123,
      "learning_rate": 3.518372564343714e-05,
      "loss": 0.9269,
      "step": 989000
    },
    {
      "epoch": 13.020936138854895,
      "grad_norm": 2.6039342880249023,
      "learning_rate": 3.515060999033023e-05,
      "loss": 0.9248,
      "step": 989500
    },
    {
      "epoch": 13.027515692234811,
      "grad_norm": 2.805849552154541,
      "learning_rate": 3.511749433722332e-05,
      "loss": 0.9343,
      "step": 990000
    },
    {
      "epoch": 13.034095245614727,
      "grad_norm": 2.2887091636657715,
      "learning_rate": 3.508437868411641e-05,
      "loss": 0.9267,
      "step": 990500
    },
    {
      "epoch": 13.040674798994644,
      "grad_norm": 2.487522840499878,
      "learning_rate": 3.50512630310095e-05,
      "loss": 0.9184,
      "step": 991000
    },
    {
      "epoch": 13.04725435237456,
      "grad_norm": 2.565457582473755,
      "learning_rate": 3.50182136092088e-05,
      "loss": 0.9211,
      "step": 991500
    },
    {
      "epoch": 13.053833905754477,
      "grad_norm": 2.516716718673706,
      "learning_rate": 3.4985097956101893e-05,
      "loss": 0.9177,
      "step": 992000
    },
    {
      "epoch": 13.060413459134393,
      "grad_norm": 2.6866402626037598,
      "learning_rate": 3.495198230299498e-05,
      "loss": 0.9166,
      "step": 992500
    },
    {
      "epoch": 13.06699301251431,
      "grad_norm": 2.294609546661377,
      "learning_rate": 3.491886664988807e-05,
      "loss": 0.9192,
      "step": 993000
    },
    {
      "epoch": 13.073572565894228,
      "grad_norm": 2.5830655097961426,
      "learning_rate": 3.4885750996781156e-05,
      "loss": 0.9254,
      "step": 993500
    },
    {
      "epoch": 13.080152119274144,
      "grad_norm": 2.599384069442749,
      "learning_rate": 3.485263534367425e-05,
      "loss": 0.9173,
      "step": 994000
    },
    {
      "epoch": 13.08673167265406,
      "grad_norm": 2.49098801612854,
      "learning_rate": 3.481951969056734e-05,
      "loss": 0.926,
      "step": 994500
    },
    {
      "epoch": 13.093311226033977,
      "grad_norm": 2.3456482887268066,
      "learning_rate": 3.478640403746043e-05,
      "loss": 0.922,
      "step": 995000
    },
    {
      "epoch": 13.099890779413894,
      "grad_norm": 2.2556169033050537,
      "learning_rate": 3.475328838435352e-05,
      "loss": 0.9179,
      "step": 995500
    },
    {
      "epoch": 13.10647033279381,
      "grad_norm": 2.696046829223633,
      "learning_rate": 3.472017273124661e-05,
      "loss": 0.9288,
      "step": 996000
    },
    {
      "epoch": 13.113049886173727,
      "grad_norm": 2.5510318279266357,
      "learning_rate": 3.4687057078139695e-05,
      "loss": 0.923,
      "step": 996500
    },
    {
      "epoch": 13.119629439553643,
      "grad_norm": 3.155848979949951,
      "learning_rate": 3.465394142503279e-05,
      "loss": 0.916,
      "step": 997000
    },
    {
      "epoch": 13.12620899293356,
      "grad_norm": 2.3988595008850098,
      "learning_rate": 3.462089200323209e-05,
      "loss": 0.9223,
      "step": 997500
    },
    {
      "epoch": 13.132788546313476,
      "grad_norm": 2.7004144191741943,
      "learning_rate": 3.458777635012518e-05,
      "loss": 0.9279,
      "step": 998000
    },
    {
      "epoch": 13.139368099693392,
      "grad_norm": 2.601444959640503,
      "learning_rate": 3.455466069701827e-05,
      "loss": 0.9206,
      "step": 998500
    },
    {
      "epoch": 13.145947653073309,
      "grad_norm": 2.554161548614502,
      "learning_rate": 3.4521545043911355e-05,
      "loss": 0.9261,
      "step": 999000
    },
    {
      "epoch": 13.152527206453225,
      "grad_norm": 2.4836132526397705,
      "learning_rate": 3.448842939080445e-05,
      "loss": 0.924,
      "step": 999500
    },
    {
      "epoch": 13.159106759833142,
      "grad_norm": 2.847926378250122,
      "learning_rate": 3.445531373769754e-05,
      "loss": 0.9205,
      "step": 1000000
    },
    {
      "epoch": 13.16568631321306,
      "grad_norm": 2.3740484714508057,
      "learning_rate": 3.4422198084590624e-05,
      "loss": 0.9173,
      "step": 1000500
    },
    {
      "epoch": 13.172265866592976,
      "grad_norm": 2.348984718322754,
      "learning_rate": 3.4389082431483716e-05,
      "loss": 0.9238,
      "step": 1001000
    },
    {
      "epoch": 13.178845419972893,
      "grad_norm": 2.420112133026123,
      "learning_rate": 3.43559667783768e-05,
      "loss": 0.9148,
      "step": 1001500
    },
    {
      "epoch": 13.18542497335281,
      "grad_norm": 2.4463233947753906,
      "learning_rate": 3.4322851125269894e-05,
      "loss": 0.9251,
      "step": 1002000
    },
    {
      "epoch": 13.192004526732726,
      "grad_norm": 2.80840802192688,
      "learning_rate": 3.42898017034692e-05,
      "loss": 0.9194,
      "step": 1002500
    },
    {
      "epoch": 13.198584080112642,
      "grad_norm": 2.452639102935791,
      "learning_rate": 3.4256686050362284e-05,
      "loss": 0.9185,
      "step": 1003000
    },
    {
      "epoch": 13.205163633492559,
      "grad_norm": 2.4475350379943848,
      "learning_rate": 3.422363662856159e-05,
      "loss": 0.9151,
      "step": 1003500
    },
    {
      "epoch": 13.211743186872475,
      "grad_norm": 2.6908719539642334,
      "learning_rate": 3.419052097545468e-05,
      "loss": 0.9255,
      "step": 1004000
    },
    {
      "epoch": 13.218322740252392,
      "grad_norm": 2.706385612487793,
      "learning_rate": 3.4157405322347766e-05,
      "loss": 0.9196,
      "step": 1004500
    },
    {
      "epoch": 13.224902293632308,
      "grad_norm": 2.55344295501709,
      "learning_rate": 3.412428966924086e-05,
      "loss": 0.9203,
      "step": 1005000
    },
    {
      "epoch": 13.231481847012224,
      "grad_norm": 2.6085045337677,
      "learning_rate": 3.409117401613395e-05,
      "loss": 0.9224,
      "step": 1005500
    },
    {
      "epoch": 13.23806140039214,
      "grad_norm": 2.825226068496704,
      "learning_rate": 3.405805836302704e-05,
      "loss": 0.9256,
      "step": 1006000
    },
    {
      "epoch": 13.244640953772057,
      "grad_norm": 2.297616958618164,
      "learning_rate": 3.402494270992013e-05,
      "loss": 0.9215,
      "step": 1006500
    },
    {
      "epoch": 13.251220507151974,
      "grad_norm": 2.629418134689331,
      "learning_rate": 3.399182705681321e-05,
      "loss": 0.9197,
      "step": 1007000
    },
    {
      "epoch": 13.257800060531892,
      "grad_norm": 2.2264821529388428,
      "learning_rate": 3.3958711403706305e-05,
      "loss": 0.9266,
      "step": 1007500
    },
    {
      "epoch": 13.264379613911808,
      "grad_norm": 2.5995354652404785,
      "learning_rate": 3.392559575059939e-05,
      "loss": 0.9182,
      "step": 1008000
    },
    {
      "epoch": 13.270959167291725,
      "grad_norm": 2.3572216033935547,
      "learning_rate": 3.389248009749248e-05,
      "loss": 0.9206,
      "step": 1008500
    },
    {
      "epoch": 13.277538720671641,
      "grad_norm": 2.8019039630889893,
      "learning_rate": 3.3859496906998e-05,
      "loss": 0.9188,
      "step": 1009000
    },
    {
      "epoch": 13.284118274051558,
      "grad_norm": 2.5190441608428955,
      "learning_rate": 3.382638125389109e-05,
      "loss": 0.9267,
      "step": 1009500
    },
    {
      "epoch": 13.290697827431474,
      "grad_norm": 2.604076385498047,
      "learning_rate": 3.379326560078418e-05,
      "loss": 0.9287,
      "step": 1010000
    },
    {
      "epoch": 13.29727738081139,
      "grad_norm": 2.3610150814056396,
      "learning_rate": 3.376014994767727e-05,
      "loss": 0.9135,
      "step": 1010500
    },
    {
      "epoch": 13.303856934191307,
      "grad_norm": 2.550601005554199,
      "learning_rate": 3.372703429457036e-05,
      "loss": 0.9202,
      "step": 1011000
    },
    {
      "epoch": 13.310436487571224,
      "grad_norm": 2.6677393913269043,
      "learning_rate": 3.3693918641463454e-05,
      "loss": 0.9157,
      "step": 1011500
    },
    {
      "epoch": 13.31701604095114,
      "grad_norm": 2.430521249771118,
      "learning_rate": 3.366086921966275e-05,
      "loss": 0.9235,
      "step": 1012000
    },
    {
      "epoch": 13.323595594331056,
      "grad_norm": 2.72461199760437,
      "learning_rate": 3.3627753566555844e-05,
      "loss": 0.9201,
      "step": 1012500
    },
    {
      "epoch": 13.330175147710973,
      "grad_norm": 2.8450281620025635,
      "learning_rate": 3.359463791344893e-05,
      "loss": 0.9192,
      "step": 1013000
    },
    {
      "epoch": 13.33675470109089,
      "grad_norm": 2.4250082969665527,
      "learning_rate": 3.356152226034202e-05,
      "loss": 0.9293,
      "step": 1013500
    },
    {
      "epoch": 13.343334254470806,
      "grad_norm": 2.542271852493286,
      "learning_rate": 3.3528406607235107e-05,
      "loss": 0.9159,
      "step": 1014000
    },
    {
      "epoch": 13.349913807850722,
      "grad_norm": 2.523879051208496,
      "learning_rate": 3.34952909541282e-05,
      "loss": 0.9131,
      "step": 1014500
    },
    {
      "epoch": 13.35649336123064,
      "grad_norm": 2.5041205883026123,
      "learning_rate": 3.3462241532327503e-05,
      "loss": 0.9194,
      "step": 1015000
    },
    {
      "epoch": 13.363072914610557,
      "grad_norm": 2.3351640701293945,
      "learning_rate": 3.342912587922059e-05,
      "loss": 0.9211,
      "step": 1015500
    },
    {
      "epoch": 13.369652467990473,
      "grad_norm": 2.650270700454712,
      "learning_rate": 3.339601022611368e-05,
      "loss": 0.9152,
      "step": 1016000
    },
    {
      "epoch": 13.37623202137039,
      "grad_norm": 2.2187814712524414,
      "learning_rate": 3.336289457300677e-05,
      "loss": 0.9194,
      "step": 1016500
    },
    {
      "epoch": 13.382811574750306,
      "grad_norm": 2.4214794635772705,
      "learning_rate": 3.3329778919899865e-05,
      "loss": 0.9269,
      "step": 1017000
    },
    {
      "epoch": 13.389391128130223,
      "grad_norm": 2.4097800254821777,
      "learning_rate": 3.329666326679295e-05,
      "loss": 0.9155,
      "step": 1017500
    },
    {
      "epoch": 13.39597068151014,
      "grad_norm": 2.768728017807007,
      "learning_rate": 3.326354761368604e-05,
      "loss": 0.9214,
      "step": 1018000
    },
    {
      "epoch": 13.402550234890056,
      "grad_norm": 2.213153123855591,
      "learning_rate": 3.323043196057913e-05,
      "loss": 0.923,
      "step": 1018500
    },
    {
      "epoch": 13.409129788269972,
      "grad_norm": 2.4937806129455566,
      "learning_rate": 3.319731630747221e-05,
      "loss": 0.9236,
      "step": 1019000
    },
    {
      "epoch": 13.415709341649888,
      "grad_norm": 2.4545795917510986,
      "learning_rate": 3.3164200654365305e-05,
      "loss": 0.9224,
      "step": 1019500
    },
    {
      "epoch": 13.422288895029805,
      "grad_norm": 2.4109303951263428,
      "learning_rate": 3.313115123256461e-05,
      "loss": 0.9225,
      "step": 1020000
    },
    {
      "epoch": 13.428868448409721,
      "grad_norm": 2.4428043365478516,
      "learning_rate": 3.3098035579457695e-05,
      "loss": 0.9255,
      "step": 1020500
    },
    {
      "epoch": 13.435448001789638,
      "grad_norm": 2.274282932281494,
      "learning_rate": 3.306491992635079e-05,
      "loss": 0.921,
      "step": 1021000
    },
    {
      "epoch": 13.442027555169554,
      "grad_norm": 2.453484296798706,
      "learning_rate": 3.303180427324388e-05,
      "loss": 0.9202,
      "step": 1021500
    },
    {
      "epoch": 13.448607108549472,
      "grad_norm": 2.6018898487091064,
      "learning_rate": 3.2998754851443184e-05,
      "loss": 0.915,
      "step": 1022000
    },
    {
      "epoch": 13.455186661929389,
      "grad_norm": 2.5118408203125,
      "learning_rate": 3.2965639198336276e-05,
      "loss": 0.9202,
      "step": 1022500
    },
    {
      "epoch": 13.461766215309305,
      "grad_norm": 2.3095924854278564,
      "learning_rate": 3.293252354522936e-05,
      "loss": 0.9233,
      "step": 1023000
    },
    {
      "epoch": 13.468345768689222,
      "grad_norm": 2.85735821723938,
      "learning_rate": 3.2899474123428667e-05,
      "loss": 0.9199,
      "step": 1023500
    },
    {
      "epoch": 13.474925322069138,
      "grad_norm": 2.760960578918457,
      "learning_rate": 3.286635847032175e-05,
      "loss": 0.921,
      "step": 1024000
    },
    {
      "epoch": 13.481504875449055,
      "grad_norm": 3.129897356033325,
      "learning_rate": 3.2833242817214844e-05,
      "loss": 0.9206,
      "step": 1024500
    },
    {
      "epoch": 13.488084428828971,
      "grad_norm": 2.6210134029388428,
      "learning_rate": 3.280012716410793e-05,
      "loss": 0.9177,
      "step": 1025000
    },
    {
      "epoch": 13.494663982208888,
      "grad_norm": 3.194575548171997,
      "learning_rate": 3.276701151100102e-05,
      "loss": 0.9168,
      "step": 1025500
    },
    {
      "epoch": 13.501243535588804,
      "grad_norm": 2.615485906600952,
      "learning_rate": 3.2733962089200326e-05,
      "loss": 0.9159,
      "step": 1026000
    },
    {
      "epoch": 13.50782308896872,
      "grad_norm": 2.428891181945801,
      "learning_rate": 3.270084643609341e-05,
      "loss": 0.9185,
      "step": 1026500
    },
    {
      "epoch": 13.514402642348637,
      "grad_norm": 2.4438560009002686,
      "learning_rate": 3.2667730782986504e-05,
      "loss": 0.92,
      "step": 1027000
    },
    {
      "epoch": 13.520982195728553,
      "grad_norm": 2.2208430767059326,
      "learning_rate": 3.263461512987959e-05,
      "loss": 0.916,
      "step": 1027500
    },
    {
      "epoch": 13.52756174910847,
      "grad_norm": 3.3293731212615967,
      "learning_rate": 3.260149947677269e-05,
      "loss": 0.9164,
      "step": 1028000
    },
    {
      "epoch": 13.534141302488386,
      "grad_norm": 2.4220643043518066,
      "learning_rate": 3.256838382366577e-05,
      "loss": 0.9187,
      "step": 1028500
    },
    {
      "epoch": 13.540720855868305,
      "grad_norm": 2.3512747287750244,
      "learning_rate": 3.2535268170558865e-05,
      "loss": 0.9143,
      "step": 1029000
    },
    {
      "epoch": 13.547300409248221,
      "grad_norm": 2.7214303016662598,
      "learning_rate": 3.250215251745195e-05,
      "loss": 0.9175,
      "step": 1029500
    },
    {
      "epoch": 13.553879962628137,
      "grad_norm": 2.5066981315612793,
      "learning_rate": 3.246903686434504e-05,
      "loss": 0.9115,
      "step": 1030000
    },
    {
      "epoch": 13.560459516008054,
      "grad_norm": 2.4362635612487793,
      "learning_rate": 3.243598744254434e-05,
      "loss": 0.9116,
      "step": 1030500
    },
    {
      "epoch": 13.56703906938797,
      "grad_norm": 2.4326653480529785,
      "learning_rate": 3.240287178943743e-05,
      "loss": 0.9195,
      "step": 1031000
    },
    {
      "epoch": 13.573618622767887,
      "grad_norm": 2.8052749633789062,
      "learning_rate": 3.236975613633052e-05,
      "loss": 0.914,
      "step": 1031500
    },
    {
      "epoch": 13.580198176147803,
      "grad_norm": 2.510035514831543,
      "learning_rate": 3.233664048322361e-05,
      "loss": 0.9234,
      "step": 1032000
    },
    {
      "epoch": 13.58677772952772,
      "grad_norm": 2.6428415775299072,
      "learning_rate": 3.2303591061422915e-05,
      "loss": 0.9162,
      "step": 1032500
    },
    {
      "epoch": 13.593357282907636,
      "grad_norm": 2.883220672607422,
      "learning_rate": 3.227054163962222e-05,
      "loss": 0.9203,
      "step": 1033000
    },
    {
      "epoch": 13.599936836287553,
      "grad_norm": 2.8341879844665527,
      "learning_rate": 3.2237425986515305e-05,
      "loss": 0.9253,
      "step": 1033500
    },
    {
      "epoch": 13.606516389667469,
      "grad_norm": 2.5231354236602783,
      "learning_rate": 3.22043103334084e-05,
      "loss": 0.918,
      "step": 1034000
    },
    {
      "epoch": 13.613095943047385,
      "grad_norm": 2.5268185138702393,
      "learning_rate": 3.2171260911607695e-05,
      "loss": 0.9188,
      "step": 1034500
    },
    {
      "epoch": 13.619675496427302,
      "grad_norm": 2.6874208450317383,
      "learning_rate": 3.213814525850079e-05,
      "loss": 0.9155,
      "step": 1035000
    },
    {
      "epoch": 13.626255049807218,
      "grad_norm": 2.366525888442993,
      "learning_rate": 3.210502960539388e-05,
      "loss": 0.9175,
      "step": 1035500
    },
    {
      "epoch": 13.632834603187135,
      "grad_norm": 2.61045241355896,
      "learning_rate": 3.207191395228697e-05,
      "loss": 0.9145,
      "step": 1036000
    },
    {
      "epoch": 13.639414156567053,
      "grad_norm": 2.304548501968384,
      "learning_rate": 3.203879829918006e-05,
      "loss": 0.9172,
      "step": 1036500
    },
    {
      "epoch": 13.64599370994697,
      "grad_norm": 2.89516019821167,
      "learning_rate": 3.200574887737936e-05,
      "loss": 0.9068,
      "step": 1037000
    },
    {
      "epoch": 13.652573263326886,
      "grad_norm": 2.6875569820404053,
      "learning_rate": 3.1972633224272454e-05,
      "loss": 0.9222,
      "step": 1037500
    },
    {
      "epoch": 13.659152816706802,
      "grad_norm": 2.810596227645874,
      "learning_rate": 3.193951757116554e-05,
      "loss": 0.9216,
      "step": 1038000
    },
    {
      "epoch": 13.665732370086719,
      "grad_norm": 2.368643283843994,
      "learning_rate": 3.190640191805863e-05,
      "loss": 0.9168,
      "step": 1038500
    },
    {
      "epoch": 13.672311923466635,
      "grad_norm": 2.3888771533966064,
      "learning_rate": 3.1873286264951716e-05,
      "loss": 0.9165,
      "step": 1039000
    },
    {
      "epoch": 13.678891476846552,
      "grad_norm": 2.6341392993927,
      "learning_rate": 3.184017061184481e-05,
      "loss": 0.9199,
      "step": 1039500
    },
    {
      "epoch": 13.685471030226468,
      "grad_norm": 2.4582102298736572,
      "learning_rate": 3.18070549587379e-05,
      "loss": 0.9168,
      "step": 1040000
    },
    {
      "epoch": 13.692050583606385,
      "grad_norm": 2.202740430831909,
      "learning_rate": 3.177393930563099e-05,
      "loss": 0.9137,
      "step": 1040500
    },
    {
      "epoch": 13.698630136986301,
      "grad_norm": 2.555426597595215,
      "learning_rate": 3.174082365252408e-05,
      "loss": 0.9165,
      "step": 1041000
    },
    {
      "epoch": 13.705209690366218,
      "grad_norm": 2.5694611072540283,
      "learning_rate": 3.170770799941716e-05,
      "loss": 0.9179,
      "step": 1041500
    },
    {
      "epoch": 13.711789243746134,
      "grad_norm": 2.4105889797210693,
      "learning_rate": 3.1674592346310255e-05,
      "loss": 0.916,
      "step": 1042000
    },
    {
      "epoch": 13.71836879712605,
      "grad_norm": 2.4277987480163574,
      "learning_rate": 3.164147669320334e-05,
      "loss": 0.9136,
      "step": 1042500
    },
    {
      "epoch": 13.724948350505967,
      "grad_norm": 2.3175082206726074,
      "learning_rate": 3.160836104009643e-05,
      "loss": 0.9148,
      "step": 1043000
    },
    {
      "epoch": 13.731527903885883,
      "grad_norm": 2.445439100265503,
      "learning_rate": 3.157537784960195e-05,
      "loss": 0.9141,
      "step": 1043500
    },
    {
      "epoch": 13.738107457265802,
      "grad_norm": 2.7147328853607178,
      "learning_rate": 3.1542328427801255e-05,
      "loss": 0.9212,
      "step": 1044000
    },
    {
      "epoch": 13.744687010645718,
      "grad_norm": 2.638446092605591,
      "learning_rate": 3.150921277469435e-05,
      "loss": 0.9177,
      "step": 1044500
    },
    {
      "epoch": 13.751266564025634,
      "grad_norm": 2.6270694732666016,
      "learning_rate": 3.147609712158743e-05,
      "loss": 0.9171,
      "step": 1045000
    },
    {
      "epoch": 13.75784611740555,
      "grad_norm": 2.52213191986084,
      "learning_rate": 3.144298146848052e-05,
      "loss": 0.9106,
      "step": 1045500
    },
    {
      "epoch": 13.764425670785467,
      "grad_norm": 2.4522087574005127,
      "learning_rate": 3.140986581537361e-05,
      "loss": 0.917,
      "step": 1046000
    },
    {
      "epoch": 13.771005224165384,
      "grad_norm": 2.7453598976135254,
      "learning_rate": 3.13767501622667e-05,
      "loss": 0.9061,
      "step": 1046500
    },
    {
      "epoch": 13.7775847775453,
      "grad_norm": 2.898094892501831,
      "learning_rate": 3.1343634509159794e-05,
      "loss": 0.9154,
      "step": 1047000
    },
    {
      "epoch": 13.784164330925217,
      "grad_norm": 2.692265272140503,
      "learning_rate": 3.131051885605288e-05,
      "loss": 0.9092,
      "step": 1047500
    },
    {
      "epoch": 13.790743884305133,
      "grad_norm": 2.4503936767578125,
      "learning_rate": 3.1277469434252184e-05,
      "loss": 0.9139,
      "step": 1048000
    },
    {
      "epoch": 13.79732343768505,
      "grad_norm": 2.3516604900360107,
      "learning_rate": 3.1244353781145276e-05,
      "loss": 0.9193,
      "step": 1048500
    },
    {
      "epoch": 13.803902991064966,
      "grad_norm": 2.6145174503326416,
      "learning_rate": 3.121123812803836e-05,
      "loss": 0.9164,
      "step": 1049000
    },
    {
      "epoch": 13.810482544444882,
      "grad_norm": 2.5346992015838623,
      "learning_rate": 3.1178122474931454e-05,
      "loss": 0.9098,
      "step": 1049500
    },
    {
      "epoch": 13.817062097824799,
      "grad_norm": 2.837517499923706,
      "learning_rate": 3.114500682182454e-05,
      "loss": 0.9115,
      "step": 1050000
    },
    {
      "epoch": 13.823641651204717,
      "grad_norm": 2.8529129028320312,
      "learning_rate": 3.111189116871763e-05,
      "loss": 0.9252,
      "step": 1050500
    },
    {
      "epoch": 13.830221204584634,
      "grad_norm": 2.686340808868408,
      "learning_rate": 3.1078775515610717e-05,
      "loss": 0.9116,
      "step": 1051000
    },
    {
      "epoch": 13.83680075796455,
      "grad_norm": 2.4534554481506348,
      "learning_rate": 3.104572609381002e-05,
      "loss": 0.9157,
      "step": 1051500
    },
    {
      "epoch": 13.843380311344466,
      "grad_norm": 2.406033515930176,
      "learning_rate": 3.1012610440703114e-05,
      "loss": 0.9107,
      "step": 1052000
    },
    {
      "epoch": 13.849959864724383,
      "grad_norm": 2.487177610397339,
      "learning_rate": 3.0979494787596206e-05,
      "loss": 0.9116,
      "step": 1052500
    },
    {
      "epoch": 13.8565394181043,
      "grad_norm": 2.578256845474243,
      "learning_rate": 3.094637913448929e-05,
      "loss": 0.9071,
      "step": 1053000
    },
    {
      "epoch": 13.863118971484216,
      "grad_norm": 2.6343376636505127,
      "learning_rate": 3.091326348138238e-05,
      "loss": 0.9198,
      "step": 1053500
    },
    {
      "epoch": 13.869698524864132,
      "grad_norm": 2.5724143981933594,
      "learning_rate": 3.088014782827547e-05,
      "loss": 0.9134,
      "step": 1054000
    },
    {
      "epoch": 13.876278078244049,
      "grad_norm": 2.758939266204834,
      "learning_rate": 3.084703217516856e-05,
      "loss": 0.9124,
      "step": 1054500
    },
    {
      "epoch": 13.882857631623965,
      "grad_norm": 2.3007752895355225,
      "learning_rate": 3.0813982753367865e-05,
      "loss": 0.9135,
      "step": 1055000
    },
    {
      "epoch": 13.889437185003882,
      "grad_norm": 2.759356737136841,
      "learning_rate": 3.078086710026095e-05,
      "loss": 0.908,
      "step": 1055500
    },
    {
      "epoch": 13.896016738383798,
      "grad_norm": 2.585275650024414,
      "learning_rate": 3.074775144715404e-05,
      "loss": 0.9097,
      "step": 1056000
    },
    {
      "epoch": 13.902596291763714,
      "grad_norm": 2.1886649131774902,
      "learning_rate": 3.071463579404713e-05,
      "loss": 0.9163,
      "step": 1056500
    },
    {
      "epoch": 13.909175845143631,
      "grad_norm": 2.582624912261963,
      "learning_rate": 3.068158637224643e-05,
      "loss": 0.9176,
      "step": 1057000
    },
    {
      "epoch": 13.915755398523547,
      "grad_norm": 2.4564285278320312,
      "learning_rate": 3.0648470719139525e-05,
      "loss": 0.9107,
      "step": 1057500
    },
    {
      "epoch": 13.922334951903466,
      "grad_norm": 2.957817316055298,
      "learning_rate": 3.061542129733882e-05,
      "loss": 0.9207,
      "step": 1058000
    },
    {
      "epoch": 13.928914505283382,
      "grad_norm": 2.6041412353515625,
      "learning_rate": 3.0582305644231915e-05,
      "loss": 0.9119,
      "step": 1058500
    },
    {
      "epoch": 13.935494058663298,
      "grad_norm": 2.5386927127838135,
      "learning_rate": 3.054918999112501e-05,
      "loss": 0.9132,
      "step": 1059000
    },
    {
      "epoch": 13.942073612043215,
      "grad_norm": 2.684072494506836,
      "learning_rate": 3.0516074338018096e-05,
      "loss": 0.9087,
      "step": 1059500
    },
    {
      "epoch": 13.948653165423131,
      "grad_norm": 2.441704511642456,
      "learning_rate": 3.0482958684911185e-05,
      "loss": 0.9109,
      "step": 1060000
    },
    {
      "epoch": 13.955232718803048,
      "grad_norm": 2.4301233291625977,
      "learning_rate": 3.0449843031804277e-05,
      "loss": 0.9069,
      "step": 1060500
    },
    {
      "epoch": 13.961812272182964,
      "grad_norm": 2.639575242996216,
      "learning_rate": 3.041685984130979e-05,
      "loss": 0.9051,
      "step": 1061000
    },
    {
      "epoch": 13.96839182556288,
      "grad_norm": 2.2765684127807617,
      "learning_rate": 3.0383744188202883e-05,
      "loss": 0.9159,
      "step": 1061500
    },
    {
      "epoch": 13.974971378942797,
      "grad_norm": 2.5186986923217773,
      "learning_rate": 3.035062853509597e-05,
      "loss": 0.9139,
      "step": 1062000
    },
    {
      "epoch": 13.981550932322714,
      "grad_norm": 2.6405608654022217,
      "learning_rate": 3.0317512881989057e-05,
      "loss": 0.9067,
      "step": 1062500
    },
    {
      "epoch": 13.98813048570263,
      "grad_norm": 2.9276986122131348,
      "learning_rate": 3.028439722888215e-05,
      "loss": 0.9065,
      "step": 1063000
    },
    {
      "epoch": 13.994710039082547,
      "grad_norm": 2.3941919803619385,
      "learning_rate": 3.0251281575775238e-05,
      "loss": 0.924,
      "step": 1063500
    },
    {
      "epoch": 14.0,
      "eval_loss": 0.8748059868812561,
      "eval_runtime": 49.6167,
      "eval_samples_per_second": 2015.452,
      "eval_steps_per_second": 15.761,
      "step": 1063902
    }
  ],
  "logging_steps": 500,
  "max_steps": 1519860,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 7.734329432206522e+18,
  "train_batch_size": 128,
  "trial_name": null,
  "trial_params": null
}
