# HelBERT üë®‚Äç‚öñÔ∏è

‚öñüìù **HelBERT** √© um reposit√≥rio completo para pr√©-treinamento, fine-tuning, avalia√ß√£o e an√°lise de modelos de linguagem baseados em BERT, com foco em textos de Editais de Licita√ß√µes e dom√≠nio jur√≠dico-administrativo brasileiro.

---

## Sum√°rio

- [Sobre o Projeto](#sobre-o-projeto)
- [Estrutura do Reposit√≥rio](#estrutura-do-reposit√≥rio)
- [Pr√©-treinamento do HelBERT](#pr√©-treinamento-do-helbert)
- [Fine-tuning e Avalia√ß√£o](#fine-tuning-e-avalia√ß√£o)
- [Como Executar](#como-executar)
- [Resultados e M√©tricas](#resultados-e-m√©tricas)
- [HelBERT no Hugging Face](#helbert-no-hugging-face)

---

## Sobre o Projeto

Este reposit√≥rio cont√©m todo o pipeline para:
- **Pr√©-treinamento** de modelos BERT no dom√≠nio de licita√ß√µes p√∫blicas.
- **Fine-tuning** para tarefas espec√≠ficas (classifica√ß√£o, NER, etc).
- **Avalia√ß√£o** e an√°lise de m√©tricas dos modelos.
- **Scripts utilit√°rios** para limpeza, prepara√ß√£o de dados e visualiza√ß√£o de resultados.

O HelBERT foi treinado do zero utilizando grandes volumes de editais e documentos p√∫blicos, visando melhorar o desempenho em tarefas jur√≠dicas e administrativas.

---

## Estrutura do Reposit√≥rio

A pasta principal de c√≥digo √© [`Codigos/`](Codigos/), organizada da seguinte forma:

- **BaseDeDados/**: Scripts de limpeza, prepara√ß√£o e manipula√ß√£o de datasets.
- **CalculoMetricasHelBERTs/**: C√°lculo e an√°lise de m√©tricas dos modelos.
- **Distilled/**: T√©cnicas de destila√ß√£o de modelos.
- **FineTuning/**: Scripts para fine-tuning e avalia√ß√£o em diferentes tarefas (Classifica√ß√£o, NER, etc).
- **Graficos/**: Gera√ß√£o de gr√°ficos e visualiza√ß√µes.
- **LSG/**: M√©todos para long sequence modeling.
- **PreTreinamento/**: Scripts para pr√©-treinamento do HelBERT.
- **utils/**: Fun√ß√µes utilit√°rias para manipula√ß√£o de arquivos, m√©tricas, etc.

Al√©m disso, o reposit√≥rio cont√©m exemplos de datasets, modelos treinados e resultados de experimentos.

---

## Pr√©-treinamento do HelBERT

O pr√©-treinamento do HelBERT √© realizado a partir de grandes corpora de editais, utilizando scripts em [`Codigos/PreTreinamento/`](Codigos/PreTreinamento/). O processo inclui:
- Limpeza e normaliza√ß√£o dos textos ([`cleaner_pretreinamento.py`](Codigos/BaseDeDados/cleaner_pretreinamento.py))
- Tokeniza√ß√£o e prepara√ß√£o dos dados
- Treinamento do modelo BERT com Masked Language Modeling

---

## Fine-tuning e Avalia√ß√£o

O fine-tuning √© realizado para diferentes tarefas, como:
- **Classifica√ß√£o de textos** ([`FineTuning/Classificacao/`](Codigos/FineTuning/Classificacao/))
- **Reconhecimento de Entidades Nomeadas (NER)** ([`FineTuning/EntidadesNomeadas/`](Codigos/FineTuning/EntidadesNomeadas/))
- **Outras tarefas espec√≠ficas** (Fertilidade, Objetos, etc)

Scripts de avalia√ß√£o e c√°lculo de m√©tricas est√£o dispon√≠veis em [`CalculoMetricasHelBERTs/`](Codigos/CalculoMetricasHelBERTs/) e [`FineTuning/*/`](Codigos/FineTuning/).

---

## Como Executar

1. **Instale as depend√™ncias:**
   ```bash
   pip install -r requirements.txt

2. **Prepare os dados:**

Utilize os scripts em Codigos/BaseDeDados/ para limpeza e prepara√ß√£o dos datasets.

3. **Pr√©-treine o modelo:**

Execute os scripts em Codigos/PreTreinamento/ para treinar o HelBERT do zero.

4. **Fine-tuning e avalia√ß√£o:**

Utilize os scripts em Codigos/FineTuning/ para treinar e avaliar o modelo em tarefas espec√≠ficas.

5. **Visualize os resultados:**

Gere gr√°ficos e relat√≥rios com os notebooks em Codigos/Graficos/ e Codigos/CalculoMetricasHelBERTs/.

---

## Resultados e M√©tricas

Os resultados dos experimentos, m√©tricas de avalia√ß√£o e compara√ß√µes com outros modelos est√£o dispon√≠veis em:

Codigos/CalculoMetricasHelBERTs/metricas_modelos.json

Notebooks de an√°lise em Codigos/CalculoMetricasHelBERTs/

---

## HelBERT no Hugging Face ü§ó

O modelo **HelBERT-base** est√° dispon√≠vel publicamente no [Hugging Face Hub](https://huggingface.co/vic35get/HelBERT-base):

[![Hugging Face](https://img.shields.io/badge/HuggingFace-HelBERT--base-yellow?logo=huggingface)](https://huggingface.co/vic35get/HelBERT-base)

---

### Como utilizar o HelBERT-base

Voc√™ pode importar e utilizar o modelo diretamente em seu c√≥digo Python com a biblioteca `transformers`:

```python
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("vic35get/HelBERT-base")
model = AutoModel.from_pretrained("vic35get/HelBERT-base")

# Exemplo de uso
inputs = tokenizer("Exemplo de texto jur√≠dico para o HelBERT.", return_tensors="pt")
outputs = model(**inputs)

---