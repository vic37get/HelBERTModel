{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install tqdm\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_split(text: str, tokenizer: AutoTokenizer, length: int = 250, overlap: int = 0, max_chunks: int = 200) -> list:\n",
    "        \"\"\"\n",
    "        Função que divide o texto em pedaços de tamanho length com overlap de tamanho overlap.\n",
    "        Parâmetros:\n",
    "            text: texto a ser dividido\n",
    "            length: tamanho de cada pedaço\n",
    "            overlap: tamanho da sobreposição entre os pedaços\n",
    "            max_chunks: número máximo de pedaços\n",
    "        Retorno:\n",
    "            l_total: lista com os pedaços do texto\n",
    "        \"\"\"\n",
    "        l_total = []\n",
    "        l_parcial = []\n",
    "        n_words = len(text.split()) \n",
    "        #n_words = len(tokenizer.tokenize(text))\n",
    "        n = n_words//(length-overlap)+1\n",
    "        if n_words % (length-overlap) == 0:\n",
    "            n = n-1\n",
    "        if n ==0:\n",
    "            n = 1\n",
    "        n = min(n, max_chunks)\n",
    "        for w in range(n):\n",
    "            if w == 0:\n",
    "                l_parcial = text.split()[:length]\n",
    "            else:\n",
    "                l_parcial = text.split()[w*(length-overlap):w*(length-overlap) + length]\n",
    "            l = \" \".join(l_parcial)\n",
    "            if w==n-1:\n",
    "                if len(l_parcial) < 0.75*length and n!=1:\n",
    "                    continue\n",
    "            l_total.append(l)\n",
    "        return l_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "dados = pd.read_csv(\"/var/projetos/Jupyterhubstorage/victor.silva/HelBERTModel/Datasets/Indicios/DatasetsGold/dataset_gold_hab_atualizada.csv\")\n",
    "dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos = []\n",
    "for indice in tqdm(dados.index):\n",
    "    textos.extend(get_text_split(dados['text'][indice], tokenizer, length=200, overlap=0, max_chunks=200))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(textos, columns=['text'])\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "dataframe['tamanho'] = dataframe['text'].progress_apply(lambda x: len(tokenizer.tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['tamanho'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe[dataframe['tamanho'] >= 10]\n",
    "dataframe = dataframe[dataframe['tamanho'] <= 500]\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe.select_dtypes(include=['object'])\n",
    "dataframe.drop_duplicates(inplace=True)\n",
    "dataframe.dropna(inplace=True)\n",
    "dataframe.reset_index(drop=True, inplace=True)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_pickle(\"/var/projetos/Jupyterhubstorage/victor.silva/HelBERTModel/Datasets/MaskedLanguageModeling/Indicios/dataset_gold_hab_atualizada_chunks.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
