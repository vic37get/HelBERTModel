{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from transformers import BertForMaskedLM, BertTokenizer, BertConfig\n",
    "from transformers.models.bert.modeling_bert import BertEncoder, BertModel, BertPreTrainedModel\n",
    "from torch.nn import Module\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('/var/projetos/Jupyterhubstorage/victor.silva/HelBERTModel/Modelos/PreTreinamento/HelBERT-uncased-fs/checkpoint-epoca-6')\n",
    "modelo = BertModel.from_pretrained('/var/projetos/Jupyterhubstorage/victor.silva/HelBERTModel/Modelos/PreTreinamento/HelBERT-uncased-fs/checkpoint-epoca-6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_children(\n",
    "    object : Any,\n",
    "    level : int = 0,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Prints the children of (object) and their children too, if there are any.\n",
    "    Uses the current depth (level) to print things in a ordonnate manner.\n",
    "    \"\"\"\n",
    "    print(f\"{'   ' * level}{level}- {type(object).__name__}\")\n",
    "    try:\n",
    "        for child in object.children():\n",
    "            visualize_children(child, level + 1)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "visualize_children(modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distill_bert_weights(\n",
    "    teacher: Module,\n",
    "    student: Module,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Recursively copies the weights of the (teacher) to the (student).\n",
    "    This function is meant to be first called on a BertFor... model, but is then called on every children of that model recursively.\n",
    "    The only part that's not fully copied is the encoder, of which only half is copied.\n",
    "    \"\"\"\n",
    "    # If the part is an entire BERT model or a BertFor..., unpack and iterate\n",
    "    if isinstance(teacher, BertModel) or type(teacher).__name__.startswith('BertFor'):\n",
    "        for teacher_part, student_part in zip(teacher.children(), student.children()):\n",
    "            distill_bert_weights(teacher_part, student_part)\n",
    "    # Else if the part is an encoder, copy one out of every layer\n",
    "    elif isinstance(teacher, BertEncoder):\n",
    "        teacher_encoding_layers = [layer for layer in next(teacher.children())]\n",
    "        student_encoding_layers = [layer for layer in next(student.children())]\n",
    "        for i in range(len(student_encoding_layers)):\n",
    "            # Camadas pares\n",
    "            #student_encoding_layers[i].load_state_dict(teacher_encoding_layers[2 * i].state_dict())\n",
    "            # Primeiras 6 camadas\n",
    "            #student_encoding_layers[i].load_state_dict(teacher_encoding_layers[i].state_dict())\n",
    "            # Últimas 6 camadas\n",
    "            #student_encoding_layers[i].load_state_dict(teacher_encoding_layers[i+6].state_dict())\n",
    "            # Camadas ímpares\n",
    "            student_encoding_layers[i].load_state_dict(teacher_encoding_layers[2 * i + 1].state_dict())\n",
    "            print(2 * i + 1)\n",
    "    # Else the part is a head or something else, copy the state_dict\n",
    "    else:\n",
    "        student.load_state_dict(teacher.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distill_bert(\n",
    "    teacher_model: BertPreTrainedModel,\n",
    ") -> BertPreTrainedModel:\n",
    "    \"\"\"\n",
    "    Distillates a BERT (teacher_model) like would DistilBERT for a BERT model.\n",
    "    The student model has the same configuration, except for the number of hidden layers, which is // by 2.\n",
    "    The student layers are initialized by copying one out of two layers of the teacher, starting with layer 0.\n",
    "    The head of the teacher is also copied.\n",
    "    \"\"\"\n",
    "    # Get teacher configuration as a dictionary\n",
    "    configuration = teacher_model.config.to_dict()\n",
    "    # Half the number of hidden layers\n",
    "    configuration['num_hidden_layers'] //= 2\n",
    "    # Convert the dictionary to the student configuration\n",
    "    configuration = BertConfig.from_dict(configuration)\n",
    "    # Create uninitialized student model\n",
    "    student_model = type(teacher_model)(configuration)\n",
    "    # Initialize the student's weights\n",
    "    distill_bert_weights(teacher=teacher_model, student=student_model)\n",
    "    # Return the student model\n",
    "    print(student_model.config)\n",
    "    return student_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student = distill_bert(modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import CrossEntropyLoss, CosineEmbeddingLoss\n",
    "\n",
    "def distillation_loss(\n",
    "    teacher_logits : Tensor,\n",
    "    student_logits : Tensor,\n",
    "    labels : Tensor,\n",
    "    temperature : float = 1.0,\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    The distillation loss for distilating a BERT-like model.\n",
    "    The loss takes the (teacher_logits), (student_logits) and (labels) for various losses.\n",
    "    The (temperature) can be given, otherwise it's set to 1 by default.\n",
    "    \"\"\"\n",
    "    # Temperature and sotfmax\n",
    "    student_logits, teacher_logits = (student_logits / temperature).softmax(1), (teacher_logits / temperature).softmax(1)\n",
    "    # Classification loss (problem-specific loss)\n",
    "    loss = CrossEntropyLoss()(student_logits, labels)\n",
    "    # CrossEntropy teacher-student loss\n",
    "    loss = loss + CrossEntropyLoss()(student_logits, teacher_logits)\n",
    "    # Cosine loss\n",
    "    loss = loss + CosineEmbeddingLoss()(teacher_logits, student_logits, torch.ones(teacher_logits.size()[0]))\n",
    "    # Average the loss and return it\n",
    "    loss = loss / 3\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student.save_pretrained(\"/var/projetos/Jupyterhubstorage/victor.silva/HelBERTModel/Modelos/PreTreinamento/distilHelBERT-base-camadas-impares\")\n",
    "tokenizer.save_pretrained(\"/var/projetos/Jupyterhubstorage/victor.silva/HelBERTModel/Modelos/PreTreinamento/distilHelBERT-base-camadas-impares\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student = BertForMaskedLM.from_pretrained(\"/var/projetos/Jupyterhubstorage/victor.silva/HelBERTModel/Modelos/PreTreinamento/distilHelBERT-base-camadas-impares\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"/var/projetos/Jupyterhubstorage/victor.silva/HelBERTModel/Modelos/PreTreinamento/distilHelBERT-base-camadas-impares\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "fill = pipeline(\"fill-mask\", model=student, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill(\"[MASK] da licitacao\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
