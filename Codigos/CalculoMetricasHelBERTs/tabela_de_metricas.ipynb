{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "from pdf2image import convert_from_path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metrics(json_path):\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    model_name = data.get(\"model_name\", \"Unknown Model\")\n",
    "    accuracy = data.get(\"accuracy\", data['cf_report'].get('accuracy'))\n",
    "    f1_weighted = data[\"cf_report\"][\"weighted avg\"].get(\"f1-score\", 0)\n",
    "    f1_macro = data[\"cf_report\"][\"macro avg\"].get(\"f1-score\", 0)\n",
    "    try:\n",
    "        f1_micro = data[\"cf_report\"][\"micro avg\"].get(\"f1-score\", 0)\n",
    "    except KeyError:\n",
    "        f1_micro = 0\n",
    "    metrics = {\n",
    "        \"Model\": model_name,\n",
    "        \"Acurácia (%)\": round(accuracy * 100, 2),\n",
    "        \"F1-weighted (%)\": round(f1_weighted * 100, 2),\n",
    "        \"F1-macro (%)\": round(f1_macro * 100, 2),\n",
    "        \"F1-micro (%)\": round(f1_micro * 100, 2)\n",
    "        \n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_table(json_paths):\n",
    "    all_metrics = []\n",
    "    include_f1_micro = False\n",
    "    \n",
    "    for path in json_paths:\n",
    "        metrics = extract_metrics(path)\n",
    "        all_metrics.append(metrics)\n",
    "        # Verifica se há pelo menos uma ocorrência de F1-micro maior que 0\n",
    "        if metrics[\"F1-micro (%)\"] > 0:\n",
    "            include_f1_micro = True\n",
    "    \n",
    "    # Encontrar os valores máximos de cada métrica\n",
    "    max_f1_weighted = max(m[\"F1-weighted (%)\"] for m in all_metrics)\n",
    "    max_f1_macro = max(m[\"F1-macro (%)\"] for m in all_metrics)\n",
    "    max_accuracy = max(m[\"Acurácia (%)\"] for m in all_metrics)\n",
    "    max_f1_micro = max(m[\"F1-micro (%)\"] for m in all_metrics) if include_f1_micro else None\n",
    "    \n",
    "    table_latex = r\"\"\"\\begin{table}[ht]\n",
    "\\centering\n",
    "\\renewcommand{\\arraystretch}{1.2}\n",
    "\\caption{Desempenho dos modelos para a tarefa de classificação.}\n",
    "\\label{tab:tableDesempenhoModelos}\n",
    "\\vspace{4pt} % Espaço vertical adicionado antes da legenda\n",
    "\\begin{tabular}{l\"\"\"\n",
    "\n",
    "    table_latex += \" c\"  \n",
    "    if include_f1_micro:\n",
    "        table_latex += \" c\"  \n",
    "    table_latex += \" c\"  \n",
    "    table_latex += \" c\"  \n",
    "    \n",
    "    table_latex += \"}\\n\\\\toprule\\n\\\\textbf{Modelo} & \\\\textbf{Acurácia (\\\\%)}\"\n",
    "    \n",
    "    if include_f1_micro:\n",
    "        table_latex += \" & \\\\textbf{F1-micro (\\\\%)}\"\n",
    "    \n",
    "    table_latex += \" & \\\\textbf{F1-macro (\\\\%)} & \\\\textbf{F1-weighted (\\\\%)} \\\\\\\\\\n\\\\midrule\\n\"\n",
    "    \n",
    "    for metrics in all_metrics:\n",
    "        table_latex += f'{metrics[\"Model\"]}'\n",
    "        \n",
    "        accuracy_str = f\"\\\\textbf{{{metrics['Acurácia (%)']}}}\" if metrics[\"Acurácia (%)\"] == max_accuracy else f\"{metrics['Acurácia (%)']}\"\n",
    "        table_latex += f\" & {accuracy_str}\"\n",
    "        \n",
    "        if include_f1_micro:\n",
    "            f1_micro_str = f\"\\\\textbf{{{metrics['F1-micro (%)']}}}\" if metrics[\"F1-micro (%)\"] == max_f1_micro else f\"{metrics['F1-micro (%)']}\"\n",
    "            table_latex += f\" & {f1_micro_str}\"\n",
    "        \n",
    "        f1_macro_str = f\"\\\\textbf{{{metrics['F1-macro (%)']}}}\" if metrics[\"F1-macro (%)\"] == max_f1_macro else f\"{metrics['F1-macro (%)']}\"\n",
    "        table_latex += f\" & {f1_macro_str}\"\n",
    "        \n",
    "        f1_weighted_str = f\"\\\\textbf{{{metrics['F1-weighted (%)']}}}\" if metrics[\"F1-weighted (%)\"] == max_f1_weighted else f\"{metrics['F1-weighted (%)']}\"\n",
    "        table_latex += f\" & {f1_weighted_str}\"\n",
    "        \n",
    "        table_latex += \" \\\\\\\\\\n\"\n",
    "    \n",
    "    table_latex += r\"\"\"\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "    \n",
    "    return table_latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_metricas = \"/var/projetos/Jupyterhubstorage/victor.silva/HelBERTModel/Metricas/Secoes/CrossValidation/NovasMetricas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivos_json = []\n",
    "for arquivo in os.listdir(dir_metricas):\n",
    "    if arquivo.endswith(\".json\"):\n",
    "        arquivos_json.append(os.path.join(dir_metricas, arquivo))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_table = generate_table(arquivos_json)\n",
    "with open(\"table.tex\", \"w\") as file:\n",
    "    file.write(latex_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
