{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install codecarbon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from calflops import calculate_flops\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from codecarbon import EmissionsTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time(model, tokenizer, dataset):\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for sample in tqdm(dataset):\n",
    "            inputs = tokenizer(sample, return_tensors=\"pt\", truncation=True, max_length=512).to('cuda')\n",
    "            outputs = model(**inputs)\n",
    "    end = time.time()\n",
    "    return end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speed_ratio(baseline_time, model, tokenizer, dataset):\n",
    "    time = get_time(model, tokenizer, dataset)\n",
    "    return baseline_time/time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculaFlopsEParams(modelo, tokenizer):\n",
    "    flops, macs, params = calculate_flops(model=modelo, input_shape=(1, 512), transformer_tokenizer=tokenizer)\n",
    "    return flops, macs, params  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_time_inference(model, tokenizer, dataset):\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for sample in tqdm(dataset):\n",
    "            inputs = tokenizer(sample, return_tensors=\"pt\", truncation=True, max_length=512).to('cuda')\n",
    "            outputs = model(**inputs)\n",
    "    end = time.time()\n",
    "    \n",
    "    total_samples = len(dataset)\n",
    "    total_time = end - start\n",
    "    samples_per_second = total_samples / total_time\n",
    "    print(f\"Samples per second: {samples_per_second}\")\n",
    "    return samples_per_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado_metricas = []\n",
    "dataset = load_dataset(\"tcepi/bidCorpus_raw\", \"bidCorpus_gold\")\n",
    "modelos = [\"tcepi/HelBERT-base-uncased-fs\", \"tcepi/distilHelBERT\", \"tcepi/HelBERT-uncased-fs-lsg\"]\n",
    "\n",
    "for model_name in modelos:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token)\n",
    "    modelo = AutoModel.from_pretrained(model_name, use_auth_token=token)\n",
    "    modelo.resize_token_embeddings(len(tokenizer))\n",
    "    tracker = EmissionsTracker()\n",
    "    tracker.start()\n",
    "    time_inference = test_time_inference(modelo.to('cuda'), tokenizer, dataset['train']['text'])\n",
    "    emissions: float =  tracker.stop()\n",
    "    model_size = sum(param.numel() for param in modelo.parameters()) * modelo.dtype.itemsize / (1024**2)\n",
    "    flops, macs, params = calculaFlopsEParams(modelo.to('cpu'), tokenizer)\n",
    "    resultado_metricas.append({\"modelo\": model_name, \"flops\": flops, \"carbon_footprint\": \"{} kg\".format(emissions), \"number_of_params\": params,\n",
    "                               \"model_size\": model_size, \"samples_per_second\": \"{:.2f}/s\".format(time_inference)})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando o time ratio\n",
    "for indice_result in range(len(resultado_metricas)):\n",
    "    if indice_result == 0:\n",
    "        resultado_metricas[indice_result]['speed_ratio'] = \"1.00x\"\n",
    "    else:\n",
    "        resultado_metricas[indice_result]['speed_ratio'] = \"{:.2f}x\".format(float(resultado_metricas[indice_result]['samples_per_second'].split(\"/\")[0]) / float(resultado_metricas[0]['samples_per_second'].split(\"/\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(resultado_metricas, open(\"metricas_modelos.json\", \"w\"), indent=4, ensure_ascii=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
