{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"knkarthick/dialogsum\"\n",
    "dataset = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_indices = [40, 200]\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "for i, index in enumerate(example_indices):\n",
    "    print(dash_line)\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(dash_line)\n",
    "    print(\"INPUT DIALOGUE:\")\n",
    "    print(dataset['test'][index]['dialogue'])\n",
    "    print(dash_line)\n",
    "    print(\"BASELINE SUMMARY:\")  \n",
    "    print(dataset['train'][index]['summary'])\n",
    "    print(dash_line)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google/flan-t5-base'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"What is the capital of India?\"\n",
    "\n",
    "sentence_encoded = tokenizer(sentence, return_tensors='pt')\n",
    "sentence_decoded = tokenizer.decode(sentence_encoded['input_ids'][0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Encoded: {sentence_encoded}\")\n",
    "print(f\"Decoded: {sentence_decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "    \n",
    "    inputs = tokenizer(dialogue, return_tensors='pt')\n",
    "    output = tokenizer.decode(model.generate(inputs['input_ids'], max_new_tokens=50)[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(dash_line)\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(dash_line)\n",
    "    print(\"INPUT PROMPT:\\n{}\".format(dialogue))\n",
    "    print(dash_line)\n",
    "    print(\"BASELINE SUMMARY:\\n{}\".format(summary))\n",
    "    print(dash_line)\n",
    "    print(\"MODEL GENERATED SUMMARY:\\n{}\".format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Summarize the following dialogue:\n",
    "    {dialogue}\n",
    "    \n",
    "    Summary:\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(model.generate(inputs['input_ids'], max_new_tokens=50)[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(dash_line)\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(dash_line)\n",
    "    print(\"INPUT PROMPT:\\n{}\".format(dialogue))\n",
    "    print(dash_line)\n",
    "    print(\"BASELINE SUMMARY:\\n{}\".format(summary))\n",
    "    print(dash_line)\n",
    "    print(\"MODEL GENERATED SUMMARY:\\n{}\".format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Dialogue:\n",
    "    {dialogue}\n",
    "    \n",
    "    What was going on?\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(model.generate(inputs['input_ids'], max_new_tokens=50)[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(dash_line)\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(dash_line)\n",
    "    print(\"INPUT PROMPT:\\n{}\".format(dialogue))\n",
    "    print(dash_line)\n",
    "    print(\"BASELINE SUMMARY:\\n{}\".format(summary))\n",
    "    print(dash_line)\n",
    "    print(\"MODEL GENERATED SUMMARY:\\n{}\".format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(example_indices_full, example_index_to_summarize):\n",
    "    prompt = \"\"\n",
    "    for i, index in enumerate(example_indices_full):\n",
    "        dialogue = dataset['test'][index]['dialogue']\n",
    "        summary = dataset['test'][index]['summary']\n",
    "        \n",
    "        prompt += f\"\"\"\n",
    "        Dialogue:\n",
    "        {dialogue}\n",
    "        \n",
    "        What was going on?\n",
    "        {summary}\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n",
    "        prompt += f\"\"\"\n",
    "        Dialogue:\n",
    "        {dialogue}\n",
    "        \n",
    "        What was going on?\n",
    "        \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_indices_full = [40]\n",
    "example_index_to_summarize = 200\n",
    "\n",
    "one_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(one_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(model.generate(inputs['input_ids'], max_new_tokens=50)[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(\"BASELINE SUMMARY:\\n{}\".format(summary))\n",
    "print(dash_line)\n",
    "print(\"MODEL GENERATED SUMMARY:\\n{}\".format(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-Shot inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_indices_full = [20, 40, 80, 120]\n",
    "example_index_to_summarize = 200\n",
    "\n",
    "few_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(model.generate(inputs['input_ids'], max_new_tokens=50)[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(\"BASELINE SUMMARY:\\n{}\".format(summary))\n",
    "print(dash_line)\n",
    "print(\"MODEL GENERATED SUMMARY:\\n{}\".format(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste dos parâmetros de inferência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, top_k = 3, top_p = 0.1, temperature = 0.8)\n",
    "\n",
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(model.generate(inputs['input_ids'], generation_config=generation_config)[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(\"BASELINE SUMMARY:\\n{}\".format(summary))\n",
    "print(dash_line)\n",
    "print(\"MODEL GENERATED SUMMARY:\\n{}\".format(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
