{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import argilla as rg\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import sys\n",
    "import pickle\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg.init(\n",
    "        api_url='http://procyon.tce.pi.gov.br:6902',\n",
    "        api_key='owner.apikey',\n",
    "        workspace='victor_silva'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ner_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/var/projetos/Jupyterhubstorage/victor.silva/HelBERTModel/Datasets/EntidadesNomeadas/ner_pt/ner_pt.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"pt_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for index in tqdm(dataset.index, desc=\"Rotulando os dados\", colour='yellow'):\n",
    "    #entities = [(ent.label_, ent.start_char, ent.end_char) for ent in doc.ents]\n",
    "    # Crie o registro TokenClassificationRecord\n",
    "    print(eval(dataset['ner_tags'][index]))\n",
    "    record = rg.TokenClassificationRecord(\n",
    "        id=index,\n",
    "        text=' '.join(eval(dataset.loc[index, 'tokens'])),\n",
    "        tokens=eval(dataset.loc[index, 'tokens']),\n",
    "        annotation = eval(dataset['ner_tags'][index]),\n",
    "        status='Validated',\n",
    "    )\n",
    "    records.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entidades = ['TIPO', 'NUMERO_PROCESSO', 'NOME']\n",
    "settings = rg.TokenClassificationSettings(entidades)\n",
    "rg.configure_dataset(\"diariosalepi\", settings, workspace=\"victor_silva\")\n",
    "rg.log(records, \"diariosalepi\", workspace=\"victor_silva\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando e transformando em DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importando as duas bases para juntar no treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetrg = rg.load(\"datasetnerdiarios\", workspace=\"victor_silva\", query=\"status:Validated\")\n",
    "datasetrg1 = rg.load(\"diariosalepi\", workspace=\"victor_silva\", query=\"status:Validated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetCompleto = pd.concat([datasetrg.to_pandas(), datasetrg1.to_pandas()])\n",
    "datasetCompleto.reset_index(drop=True, inplace=True)\n",
    "datasetCompleto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for index in tqdm(datasetCompleto.index, desc=\"Rotulando os dados\", colour='yellow'):\n",
    "    records.append(\n",
    "        rg.TokenClassificationRecord(\n",
    "            id = index,\n",
    "            text=datasetCompleto['text'][index],\n",
    "            tokens=datasetCompleto['tokens'][index],\n",
    "            annotation = datasetCompleto['annotation'][index],\n",
    "            status='Validated'\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entidades = ['TIPO', 'NUMERO_PROCESSO', 'NOME']\n",
    "settings = rg.TokenClassificationSettings(entidades)\n",
    "rg.configure_dataset(\"datasetnerdiariosfinal\", settings, workspace=\"victor_silva\")\n",
    "rg.log(records, \"datasetnerdiariosfinal\", workspace=\"victor_silva\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetrg = rg.load(\"datasetnerdiariosfinal\", workspace=\"victor_silva\", query=\"status:Validated\")\n",
    "dataset = datasetrg.prepare_for_training(framework=\"transformers\", train_size=0.8)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_to_disk(\"/var/projetos/Jupyterhubstorage/victor.silva/HelBERT/Datasets/NamedEntities/dataNerDiarios\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importando somente uma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = load_from_disk(\"../../../Datasets/NamedEntities/dataNerCrossFold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetrg = rg.load(\"datanertreino\", workspace=\"victor_silva\", query=\"status:Validated\")\n",
    "dataset = datasetrg.prepare_for_training(framework=\"transformers\", train_size=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {0: 'O', 1: 'B-LOCAL', 2: 'I-LOCAL', 3: 'B-OBJETO', 4: 'I-OBJETO'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetCompleto = dataset.to_pandas()\n",
    "datasetCompleto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in tqdm(datasetCompleto.index, desc=\"Rotulando os dados\", colour='yellow'):\n",
    "    datasetCompleto['ner_tags'][index] = [mapping[tag] for tag in datasetCompleto['ner_tags'][index]]\n",
    "datasetCompleto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in tqdm(datasetCompleto.index, desc=\"Rotulando os dados\", colour='yellow'):\n",
    "    datasetCompleto['tokens'][index] = \" \".join(datasetCompleto['tokens'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetCompleto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetCompleto.drop(columns=['id'], inplace=True)\n",
    "datasetCompleto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetCompleto.rename(columns={'tokens': 'text', 'ner_tags': 'labels'}, inplace=True)\n",
    "datasetCompleto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetCompleto.to_csv(\"../../../Datasets/NamedEntities/dataNerCrossFold.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_to_disk(\"../../../Datasets/NamedEntities/dataNerCrossFold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER Indicios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv('/var/projetos/Jupyterhubstorage/victor.silva/HelBERTModel/Datasets/Indicios/dataset_bid_notices_weak_sup_habilitacao.csv')\n",
    "dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_indicios = dados[(dados['certidao_protesto'] == 1) | (dados['certificado_boas_praticas'] == 1) | (dados['comprovante_localizacao'] == 1) | (dados['idoneidade_financeira'] == 1) | (dados['integralizado'] == 1) | (dados['licenca_ambiental'] == 1) | (dados['n_min_max_limitacao_atestados'] == 1)]\n",
    "com_indicios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_indicios.drop_duplicates(subset=['text'], inplace=True)\n",
    "com_indicios.dropna(subset=['text'], inplace=True)\n",
    "com_indicios.reset_index(drop=True, inplace=True)\n",
    "com_indicios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(com_indicios['certidao_protesto'].value_counts())\n",
    "print(com_indicios['certificado_boas_praticas'].value_counts())\n",
    "print(com_indicios['comprovante_localizacao'].value_counts())\n",
    "print(com_indicios['idoneidade_financeira'].value_counts())\n",
    "print(com_indicios['integralizado'].value_counts())\n",
    "print(com_indicios['licenca_ambiental'].value_counts())\n",
    "print(com_indicios['n_min_max_limitacao_atestados'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_certidao_protesto = com_indicios[com_indicios['certidao_protesto'] == 1]\n",
    "dados_certificado_boas_praticas = com_indicios[com_indicios['certificado_boas_praticas'] == 1]\n",
    "dados_comprovante_localizacao = com_indicios[com_indicios['comprovante_localizacao'] == 1]\n",
    "dados_idoneidade_financeira = com_indicios[com_indicios['idoneidade_financeira'] == 1]\n",
    "dados_integralizado = com_indicios[com_indicios['integralizado'] == 1]\n",
    "dados_licenca_ambiental = com_indicios[com_indicios['licenca_ambiental'] == 1]\n",
    "dados_n_min_max_limitacao_atestados = com_indicios[com_indicios['n_min_max_limitacao_atestados'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_certidao_protesto = dados_certidao_protesto.sample(n=363).reset_index(drop=True)\n",
    "dados_certificado_boas_praticas = dados_certificado_boas_praticas.sample(n=363).reset_index(drop=True)\n",
    "dados_comprovante_localizacao = dados_comprovante_localizacao.sample(n=363).reset_index(drop=True)\n",
    "dados_idoneidade_financeira = dados_idoneidade_financeira.sample(n=363).reset_index(drop=True)\n",
    "dados_integralizado = dados_integralizado.sample(n=363).reset_index(drop=True)\n",
    "dados_licenca_ambiental = dados_licenca_ambiental.sample(n=363).reset_index(drop=True)\n",
    "dados_n_min_max_limitacao_atestados = dados_n_min_max_limitacao_atestados.sample(n=363).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.concat([dados_certidao_protesto, dados_certificado_boas_praticas, dados_comprovante_localizacao, dados_idoneidade_financeira, dados_integralizado, dados_licenca_ambiental, dados_n_min_max_limitacao_atestados])\n",
    "dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados.reset_index(drop=True, inplace=True)\n",
    "dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executaLimpeza(dataframe: pd.DataFrame, column: str, cased: bool, accents: bool) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Executa a limpeza do DataFrame usando as classes Cleaner e Corretor.\n",
    "    \"\"\"\n",
    "    sentencesDrop = []\n",
    "    for indice in tqdm(dataframe.index, desc=\"Executando a Limpeza\", colour='yellow'):\n",
    "        texto = dataframe[column][indice]\n",
    "        if isinstance(texto, str):\n",
    "            texto = Cleaner().clear(texto)\n",
    "            texto = Corretor(cased, accents).corrige_termos(texto)\n",
    "            dataframe.at[indice, column] = texto\n",
    "        else:\n",
    "            sentencesDrop.append(indice)\n",
    "    dataframe = dataframe.drop(sentencesDrop)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = executaLimpeza(dados, 'text', True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados.dropna(subset=['text'], inplace=True)\n",
    "dados.drop_duplicates(subset=['text'], inplace=True)\n",
    "dados.reset_index(drop=True, inplace=True)\n",
    "dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados.sample(frac=1).reset_index(drop=True)\n",
    "dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados.to_csv('/var/projetos/Jupyterhubstorage/victor.silva/HelBERTModel/Datasets/Indicios/indiciosNER.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv('/var/projetos/Jupyterhubstorage/victor.silva/HelBERTModel/Datasets/Indicios/indiciosNER.csv')\n",
    "dados.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados.to_csv('/var/projetos/Jupyterhubstorage/victor.silva/HelBERTModel/Datasets/Indicios/DatasetsNER/indiciosNERCompleto.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando o dataset com chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import expressoes as exp\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import spacy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"pt_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pickle.load(open('/var/projetos/Jupyterhubstorage/victor.silva/HelBERTModel/Datasets/Indicios/DatasetsNER/indiciosNERCompleto_chunk.pkl', 'rb'))\n",
    "dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEntities(expression: re, doc: str, label: int) -> list:\n",
    "    ner_tags = []\n",
    "    for match in re.finditer(expression, doc.text):\n",
    "        span = doc.char_span(match.start(), match.end())\n",
    "        if span:\n",
    "            ner_tags.append((label, match.start(), match.end()))\n",
    "    return ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for index in tqdm(dados.index, desc=\"Rotulando os dados\", colour='yellow'):\n",
    "    entities = []\n",
    "    text = dados['text'][index]\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    #tokens = text.split()\n",
    "    certidao_protesto = getEntities(exp.certidao_protesto, doc, 'certidao_protesto')\n",
    "    integralizado = getEntities(exp.integralizado, doc, 'integralizado')\n",
    "    idoneidade_financeira = getEntities(exp.idoneidade_financeira, doc, 'idoneidade_financeira')\n",
    "    comprovante_localizacao = getEntities(exp.comprovante_localizacao, doc, 'comprovante_localizacao')\n",
    "    n_min_max_limitacao_atestados = getEntities(exp.n_min_max_limitacao_atestados, doc, 'n_min_max_limitacao_atestados')\n",
    "    certificado_boas_praticas = getEntities(exp.certificado_boas_praticas, doc, 'certificado_boas_praticas')\n",
    "    licenca_ambiental = getEntities(exp.licenca_ambiental, doc, 'licenca_ambiental')\n",
    "    \n",
    "    entities = certidao_protesto + integralizado + idoneidade_financeira + comprovante_localizacao + n_min_max_limitacao_atestados + certificado_boas_praticas + licenca_ambiental\n",
    "\n",
    "    record = rg.TokenClassificationRecord(\n",
    "        id=index,\n",
    "        text=text,\n",
    "        tokens = tokens,\n",
    "        annotation=entities,\n",
    "        status='Validated'\n",
    "    )\n",
    "    records.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg.log(records, \"datasetnerindicios\", workspace=\"victor_silva\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetrg = rg.load(\"datasetnerindicios\", workspace=\"victor_silva\", query=\"status:Validated\")\n",
    "datasetrg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = datasetrg.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasetrg.prepare_for_training(framework=\"transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetrg2 = datasetrg.select(\n",
    "    (\n",
    "        i for i in range(len(dataset)) \n",
    "        if i not in set(2787)\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
