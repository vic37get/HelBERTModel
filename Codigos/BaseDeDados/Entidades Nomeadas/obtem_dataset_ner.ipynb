{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "def read_conll(file_path):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        words = []\n",
    "        tags = []\n",
    "        for line in file:\n",
    "            if line.strip() == \"\":\n",
    "                if words:\n",
    "                    sentences.append(words)\n",
    "                    labels.append(tags)\n",
    "                    words = []\n",
    "                    tags = []\n",
    "            else:\n",
    "                word, tag = line.strip().split()\n",
    "                words.append(word)\n",
    "                tags.append(tag)\n",
    "        if words:\n",
    "            sentences.append(words)\n",
    "            labels.append(tags)\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, ClassLabel, Sequence\n",
    "\n",
    "def convert_to_dataset_dict(df):\n",
    "    unique_tags = set()\n",
    "    for tag in df['ner_tags']:\n",
    "        unique_tags.update(tag)\n",
    "    unique_tags = list(unique_tags)\n",
    "    unique_tags = sorted(unique_tags, key=lambda x: (x[2:], x[:1]))\n",
    "    \n",
    "    datasets = DatasetDict()\n",
    "\n",
    "    # Convertendo colunas para listas\n",
    "    tokens = df['tokens'].tolist()\n",
    "    ner_tags = df['ner_tags'].tolist()\n",
    "\n",
    "    # Criando features de tokens e ner_tags\n",
    "    features = {\n",
    "        'tokens': Sequence(feature={\"dtype\": \"string\"}),\n",
    "        'ner_tags': Sequence(feature=ClassLabel(num_classes=len(unique_tags), names=unique_tags))\n",
    "    }\n",
    "\n",
    "    # Criando dataset com um split único (neste caso, não estamos dividindo em treino/validação/teste)\n",
    "    ner_tags = [list(map(lambda x: unique_tags.index(x), tags)) for tags in ner_tags]\n",
    "    dataset = Dataset.from_dict({'tokens': tokens, 'ner_tags': ner_tags})\n",
    "    dataset = dataset.map(lambda example: {'ner_tags': example['ner_tags']})\n",
    "\n",
    "    datasets['train'] = dataset\n",
    "    datasets.features = features\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DIR_FILES = \"/var/projetos/Jupyterhubstorage/victor.silva/HelBERTModel/Datasets/EntidadesNomeadas/ner_pt/raw\"\n",
    "\n",
    "df_completo = pd.DataFrame()\n",
    "for file in os.listdir(DIR_FILES):\n",
    "    sentences, labels = read_conll(os.path.join(DIR_FILES, file))\n",
    "    data = {'tokens': sentences, 'ner_tags': labels}\n",
    "    df = pd.DataFrame(data)\n",
    "    df_completo = pd.concat([df_completo, df], ignore_index=True)\n",
    "# Pegar só a metade do dataframe completo\n",
    "indice = len(df_completo) // 2\n",
    "\n",
    "df_treino = df_completo.iloc[:indice]\n",
    "df_teste = df_completo.iloc[indice:]\n",
    "\n",
    "df_treino.dropna(inplace=True)\n",
    "df_teste.dropna(inplace=True)\n",
    "\n",
    "df_treino.reset_index(drop=True, inplace=True)\n",
    "df_teste.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_treino_dict = convert_to_dataset_dict(df_treino)\n",
    "df_teste_dict = convert_to_dataset_dict(df_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treino.to_csv(\"/var/projetos/Jupyterhubstorage/victor.silva/HelBERTModel/Datasets/EntidadesNomeadas/ner_pt/ner_pt_treino.csv\", index=False)\n",
    "df_teste.to_csv(\"/var/projetos/Jupyterhubstorage/victor.silva/HelBERTModel/Datasets/EntidadesNomeadas/ner_pt/ner_pt_teste.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "df_treino_dict.save_to_disk(\"/var/projetos/Jupyterhubstorage/victor.silva/HelBERTModel/Datasets/EntidadesNomeadas/ner_pt/ner_pt_treino_dataset\")\n",
    "json.dump(df_treino_dict.features['ner_tags'].feature.names, open(\"/var/projetos/Jupyterhubstorage/victor.silva/HelBERTModel/Datasets/EntidadesNomeadas/ner_pt/ner_pt_treino_dataset/ner_pt_treino_dataset_features.json\", \"w\"))\n",
    "\n",
    "df_teste_dict.save_to_disk(\"/var/projetos/Jupyterhubstorage/victor.silva/HelBERTModel/Datasets/EntidadesNomeadas/ner_pt/ner_pt_teste_dataset\")\n",
    "json.dump(df_teste_dict.features['ner_tags'].feature.names, open(\"/var/projetos/Jupyterhubstorage/victor.silva/HelBERTModel/Datasets/EntidadesNomeadas/ner_pt/ner_pt_teste_dataset/ner_pt_teste_dataset_features.json\", \"w\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
