{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install tqdm\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from expressoes import lista_habilitacao\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterDocuments(n):\n",
    "    for _ in range(n):\n",
    "        yield _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_split(text, iter, length=200, overlap=50, max_chunks=35):\n",
    "    l_total = []\n",
    "    id = []\n",
    "    segment_label = []\n",
    "    l_parcial = []\n",
    "    n_words = len(text.split()) \n",
    "    n = n_words//(length-overlap)+1 # n is number of splits\n",
    "    if n_words % (length-overlap) == 0:\n",
    "        n = n-1\n",
    "    if n ==0:\n",
    "        n = 1\n",
    "    n = min(n, max_chunks)\n",
    "    id_document = next(iter)\n",
    "    for w in range(n):\n",
    "        if w == 0:\n",
    "            l_parcial = text.split()[:length]\n",
    "        else:\n",
    "            l_parcial = text.split()[w*(length-overlap):w*(length-overlap) + length]\n",
    "        l = \" \".join(l_parcial)\n",
    "        if w==n-1:\n",
    "            if len(l_parcial) < 0.75*length and n!=1: # mantém o final se tiver mais de 75% do tamanho de um pedaço(length)\n",
    "                continue\n",
    "        def check_habilitacao(l):\n",
    "            chunks = []\n",
    "            sorted_habilitacao = sorted(lista_habilitacao.items(), key=lambda x: x[0])\n",
    "            for key, value in sorted_habilitacao:\n",
    "                if re.search(value, l):\n",
    "                    chunks.append(1)\n",
    "                else:\n",
    "                    chunks.append(0)\n",
    "            return chunks\n",
    "        l_total.append(l)\n",
    "        segment_label.append(check_habilitacao(l))\n",
    "        id.append(id_document)\n",
    "    return l_total, id, segment_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm as tqdm\n",
    "def chunking(novodf):\n",
    "    tqdm.tqdm.pandas()\n",
    "    iter = iterDocuments(novodf.shape[0])\n",
    "    result = novodf['text'].progress_apply(lambda text: get_text_split(text,iter))\n",
    "    ids_documents = [_ for i in result for _ in i[1]]\n",
    "    chunks = [_ for i in result for _ in i[0]]\n",
    "    segment_label = [_ for i in result for _ in i[2]]\n",
    "    bar = tqdm.tqdm(total=len(ids_documents))\n",
    "    labels = novodf.loc[:, novodf.columns[1:]].values.tolist()\n",
    "    labels_map = {i:_ for _,i in enumerate(novodf.columns[1:])}\n",
    "    # Mapear para ordem alfabética\n",
    "    \n",
    "    multilabel = []\n",
    "    for chunk,ids,seg_label in zip(chunks,ids_documents,segment_label):\n",
    "        data = dict()\n",
    "        data['text'] = chunk\n",
    "        data['id_document'] = ids\n",
    "        data['document_label'] = labels[ids]\n",
    "        data['segment_label'] = seg_label\n",
    "        data['labels_map'] = labels_map\n",
    "        multilabel.append(data)\n",
    "        bar.update(1)\n",
    "    return multilabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#treino=pd.read_csv('dataset_bid_notices_weak_hab_clean_treino.csv')\n",
    "#teste=pd.read_csv('dataset_bid_notices_weak_hab_clean_teste.csv')\n",
    "#validacao=pd.read_csv('dataset_bid_notices_weak_hab_clean_validacao.csv')\n",
    "sintetico = pd.read_csv('/var/projetos/Jupyterhubstorage/victor.silva/HelBERT/Datasets/Indicios/dataset_syntetic.csv')\n",
    "gold = pd.read_csv('/var/projetos/Jupyterhubstorage/victor.silva/HelBERT/Datasets/Indicios/dataset_bid_notices_gold.csv')\n",
    "#print(treino.shape)\n",
    "#train=chunking(treino)\n",
    "#print(teste.shape)\n",
    "#test=chunking(teste)\n",
    "#print(validacao.shape)\n",
    "#validation=chunking(validacao)\n",
    "print(sintetico.shape)\n",
    "sintetic=chunking(sintetico)\n",
    "print(gold.shape)\n",
    "gold=chunking(gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(train).to_pickle('dataset_bid_notices_weak_hab_clean_treino_chunk_more.pkl')\n",
    "#pd.DataFrame(test).to_pickle('dataset_bid_notices_weak_hab_clean_teste_chunk_more.pkl')\n",
    "#pd.DataFrame(validation).to_pickle('dataset_bid_notices_weak_hab_clean_validacao_chunk_more.csv')\n",
    "pd.DataFrame(sintetic).to_pickle('/var/projetos/Jupyterhubstorage/victor.silva/HelBERT/Datasets/Indicios/dataset_syntetic_chunk.pkl')\n",
    "pd.DataFrame(gold).to_pickle('/var/projetos/Jupyterhubstorage/victor.silva/HelBERT/Datasets/Indicios/dataset_bid_notices_gold_chunk.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('./train-BERT-indicios-weak-2/dataset_bid_notices_weak_hab_clean_treino_chunk.csv',index=False)\n",
    "test.to_csv('./train-BERT-indicios-weak-2/dataset_bid_notices_weak_hab_clean_teste_chunk.csv',index=False)\n",
    "validation.to_csv('./train-BERT-indicios-weak-2/dataset_bid_notices_weak_hab_clean_validacao_chunk.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
