{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install deep-translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "\n",
    "def back_translate(texts, language_src, language_dst):\n",
    "  \"\"\"Implements back translation\"\"\"\n",
    "  # Translate from source to target language\n",
    "  translated = GoogleTranslator(source='auto', target=language_dst).translate(texts)\n",
    "\n",
    "  # Translate from target language back to source language\n",
    "  back_translated = GoogleTranslator(source='auto', target=language_src).translate(translated)\n",
    "\n",
    "  return back_translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_weak_sel_cleaned= pd.read_csv('/var/projetos/Jupyterhubstorage/victor.silva/HelBERTModel/Datasets/Indicios/DatasetsWeak/DataAugmentation/data_for_augmentation_n_min_max_idoneidade_financeira.csv')\n",
    "df_weak_sel_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm.auto as tqdm\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "df_translated=pd.DataFrame(columns=df_weak_sel_cleaned.columns)\n",
    "for i, row in tqdm.tqdm(df_weak_sel_cleaned.iterrows(), total=df_weak_sel_cleaned.shape[0]):\n",
    "    df_translated.loc[i]=row\n",
    "    if len(df_translated.loc[i,'text'])<5000:\n",
    "        result=back_translate(df_translated.loc[i,'text'],'pt','ja')\n",
    "    else:\n",
    "        result=\"\"\n",
    "        sentences = re.split(r'\\.\\s+', df_translated.loc[i,'text'])\n",
    "        for sentence in tqdm.tqdm(sentences,total=len(sentences)):\n",
    "            if len(result)==0:\n",
    "                result=back_translate(sentence,'pt','ja')\n",
    "            else:\n",
    "                result=result+\". \"+back_translate(sentence,'pt','ja')\n",
    "    result = re.sub(r'\\.+', '.', result)\n",
    "    result = unidecode(result)\n",
    "    result = result.lower()\n",
    "    df_translated.loc[i,'text']=result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weak_sel_cleaned.loc[0,'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_translated.loc[0,'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_translated.to_csv('/var/projetos/Jupyterhubstorage/victor.silva/HelBERTModel/Datasets/Indicios/DatasetsWeak/DataAugmentation/data_back_translated_n_min_max_idoneidade_financeira.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Helper function to download data for a language\n",
    "def download(model_name):\n",
    "  tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "  model = MarianMTModel.from_pretrained(model_name).to('cpu')\n",
    "  return tokenizer, model\n",
    "\n",
    "# download model for English -> Romance\n",
    "src_lang_tokenizer, src_lang_model = download('Helsinki-NLP/opus-mt-en-ROMANCE')\n",
    "# download model for Romance -> English\n",
    "tmp_lang_tokenizer, tmp_lang_model = download('Helsinki-NLP/opus-mt-ROMANCE-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(texts, model, tokenizer, language):\n",
    "  \"\"\"Translate texts into a target language\"\"\"\n",
    "  # Format the text as expected by the model\n",
    "  formatter_fn = lambda txt: f\"{txt}\" if language == \"en\" else f\">>{language}<< {txt}\"\n",
    "  original_texts = [formatter_fn(txt) for txt in texts]\n",
    "  # Tokenize (text to tokens)\n",
    "  input_ids = tokenizer(original_texts, return_tensors=\"pt\",padding=True,truncation=True).input_ids.to('cpu')\n",
    "  # Translate\n",
    "  translated = model.generate(input_ids)\n",
    "\n",
    "  # Decode (tokens to text)\n",
    "  translated_texts = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "\n",
    "  return translated_texts\n",
    "\n",
    "def back_translate(texts, language_src, language_dst):\n",
    "  \"\"\"Implements back translation\"\"\"\n",
    "  # Translate from source to target language\n",
    "  translated = translate(texts, tmp_lang_model, tmp_lang_tokenizer, language_dst)\n",
    "\n",
    "  # Translate from target language back to source language\n",
    "  back_translated = translate(translated, src_lang_model, src_lang_tokenizer, language_src)\n",
    "\n",
    "  return back_translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm.auto as tqdm\n",
    "import re\n",
    "df_translated=pd.DataFrame(columns=df_weak_sel_cleaned.columns)\n",
    "for i, row in tqdm.tqdm(df_weak_sel_cleaned.iterrows(), total=df_weak_sel_cleaned.shape[0]):\n",
    "    df_translated.loc[i]=row\n",
    "    sentences = re.split(r'\\.\\s+', df_translated.loc[i,'text'])\n",
    "    result=[]\n",
    "    for sentence in tqdm.tqdm(sentences,total=len(sentences)):\n",
    "        result.append(back_translate(sentence,'pt_BR','en'))\n",
    "    df_translated.loc[i,'text']=\". \".join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def insert_red_flag(amostra,texto):\n",
    "    # Split the text into sentences\n",
    "    sentences_list = re.split('\\. |; |: |, |\\*|\\n', texto)\n",
    "\n",
    "    # Insert at a random position\n",
    "    random_index = random.randint(0, len(sentences_list))\n",
    "    sentences_list.insert(random_index, amostra)\n",
    "\n",
    "    # Rejoin the list of sentences\n",
    "    return '. '.join(sentences_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def find_sentence_with_substring(text, regex):    \n",
    "    \n",
    "    sentences = re.split('\\. |; |, |\\*|\\n', text)\n",
    "    sents = []\n",
    "    for sentence in sentences:\n",
    "        if re.search(regex, sentence):\n",
    "            sents.append(sentence)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metade_dataset=int(df_weak_sel_cleaned.shape[0]/2)\n",
    "cols_indicios=df_weak_sel_cleaned.columns.values.tolist()[1:]\n",
    "quantos_aumentar={}\n",
    "for col in cols_indicios:\n",
    "    quantos_aumentar[col]=metade_dataset-df_weak_sel_cleaned.value_counts(col).values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantos_aumentar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_certidao_protesto = re.compile(r'((certid[aã]o).{0,100}(protesto))',flags=re.IGNORECASE|re.S)\n",
    "exp_integralizado = re.compile(r'((capital.{0,20}integralizado)|(patrim[oô]nio.{0,20}integralizado))', flags=re.IGNORECASE|re.S)\n",
    "exp_idoneidade_financeira = re.compile('((atesta[do]*)|(certid[ãa]o)|(declara[çc]*[ãa]*[o]*))(.{0,70}((i[ni]*doneidade)).{0,20}((financeira)|(banc[áa]ria)))', flags=re.IGNORECASE|re.S)\n",
    "exp_n_min_max_limitacao_atestados = re.compile(r'((((dois|duas)|(tr[êe]s)|(quatro)|(cinco)).{0,10}((atestado[s]?)|(certid[aãoões]*))).{0,20}((capacidade t[eé]cnica)|(qualifica[cç][aã]o t[eé]cnica)))', flags=re.IGNORECASE|re.S)\n",
    "exp_certificado_boas_praticas = re.compile(r'(((certificado[s]?)|(atestado[s]?)|(certid[aã]o)).{0,20}(boa[s]?.{0,5}pr[aá]tica[s]?))', flags=re.IGNORECASE|re.S)\n",
    "exp_licenca_ambiental = re.compile(r'((licen[cç]a).{0,20}(ambiental))', flags=re.IGNORECASE|re.S)\n",
    "exp_comprovante_localizacao = re.compile('((alvar[aá])|(comprov[mnteçcãao]*)).{0,20}((localiza[çc][ãa]o)|(funcionamento))',flags=re.IGNORECASE|re.S)\n",
    "expressoes={'certidao_protesto':exp_certidao_protesto,'integralizado':exp_integralizado,'idoneidade_financeira':exp_idoneidade_financeira,\n",
    "            'n_min_max_limitacao_atestados':exp_n_min_max_limitacao_atestados,'certificado_boas_praticas':exp_certificado_boas_praticas,\n",
    "            'licenca_ambiental':exp_licenca_ambiental,'comprovante_localizacao':exp_comprovante_localizacao}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_list={'certidao_protesto':[],'integralizado':[],\n",
    "                'idoneidade_financeira':[],'n_min_max_limitacao_atestados':[],\n",
    "                'certificado_boas_praticas':[],'licenca_ambiental':[],'comprovante_localizacao':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,row in df_weak_sel_cleaned.iterrows():\n",
    "    for col in cols_indicios:\n",
    "        if row[col]==1:\n",
    "            sentences_list[col]=list(set(sentences_list[col]+find_sentence_with_substring(row['text'], expressoes[col])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols_indicios:\n",
    "    print(col,len(sentences_translated[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_path = \"sentences_translated.pkl\"\n",
    "with open(file_path, \"rb\") as file:\n",
    "    sentences_translated = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences_translated={}\n",
    "#for col in cols_indicios:\n",
    "#    sentences_translated[col]=back_translate(sentences_list[col],\"pt_BR\",\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols_indicios:\n",
    "    falta=quantos_aumentar[col]\n",
    "    df_syntetic_insert=df_weak_sel_cleaned[df_weak_sel_cleaned[col]==0].sample(falta)\n",
    "    index_list = df_syntetic_insert.index.tolist()\n",
    "    while falta>0:\n",
    "        random.shuffle(index_list)\n",
    "        ind=index_list.pop()\n",
    "        texto=df_syntetic_insert.loc[ind,'text']\n",
    "        df_weak_sel_cleaned.loc[ind,'text']=insert_red_flag(sentences_translated[col][random.randint(0,len(sentences_translated[col])-1)],texto)\n",
    "        df_weak_sel_cleaned.loc[ind,col]=1\n",
    "        falta-=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols_indicios:\n",
    "    print(col,df_weak_sel_cleaned.value_counts(col).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weak_sel_cleaned.to_csv('./train-BERT-indicios-weak/dataset_weak_augmented_insert_red_flags_translated.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
